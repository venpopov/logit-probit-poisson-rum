{
  "hash": "1c15fed38495d1ac954f9c3b72005614",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice\nauthors:\n  - name: Vencislav Popov\n    affiliation: Department of Psychology, University of Zurich\n    corresponding: true\nbibliography: references.bib\nnumber-sections: true\n---\n\n# Abstract\n\nThis study unifies the two canonical discrete choice specifications--Multinomial Logit and Multinomial Probit--within a single generative framework. We introduce a **Poisson Count Race**, wherein $K$ alternatives generate events according to independent Poisson processes. A choice is determined when an alternative reaches a specific cumulative count threshold, $\\theta$. We demonstrate that at the threshold $\\theta=1$, the model yields the Multinomial Logit (Luce choice rule). By normalizing the utility noise to maintain a constant variance--a process termed temperature identification--we show that as $\\theta \\to \\infty$, the model converges to the Multinomial Probit. This formulation provides a parametric bridge in which $\\theta$ governs the shape of the error distribution and a separate parameter, $\\beta$, modulates the temperature. Crucially, this bridge arises from a single stochastic accumulation mechanism rather than from post hoc variance matching between distinct models.\n\n\n# Introduction\n\nDiscrete choice models based on Random Utility Theory (RUT) form a central pillar of mathematical psychology, econometrics, and cognitive science. Across these fields, it is common to assume that each alternative in a choice set elicits a latent scalar quantity--often interpreted as strength, utility, or evidence--and that observed choices arise from a comparison of these latent quantities under stochastic variability. Despite the diversity of substantive applications, two modeling traditions dominate this landscape: logit-based models, derived from Luce's choice axiom and extreme-value theory, and probit-based models, derived from Gaussian signal detection theory.\n\nThe Multinomial Logit (MNL) model arises when stochastic variability is modeled via independent Gumbel (Type I Extreme Value) disturbances added to deterministic utilities. This specification is tightly linked to Luce's ratio-of-strengths axiom (Luce, 1959) and admits a well-known generative interpretation: if each alternative generates stochastic events according to an independent exponential clock, the probability that an alternative is chosen first is proportional to its rate. This exponential race interpretation has played a foundational role in mathematical psychology and economics, and it underlies a broad class of models including the Plackett--Luce ranking model and the \"Gumbel--max trick\" widely used in machine learning (McFadden, 1974; Yellott, 1977).\n\nThe Multinomial Probit (MNP) model, by contrast, assumes Gaussian noise on latent utilities and traces its lineage to Thurstone's theory of comparative judgment (Thurstone, 1927) and later developments in psychometrics and signal detection theory (Bock & Jones, 1968). Probit models are often motivated by appeal to aggregation, pooling, or measurement noise, and they offer greater flexibility in representing correlated disturbances across alternatives. Unlike logit, however, probit does not admit a simple closed-form choice rule, nor does it enjoy an equally canonical process-level interpretation.\n\nAlthough logit and probit are often treated as interchangeable in practice, their conceptual relationship remains surprisingly underdeveloped. It is well known that logistic and Gaussian cumulative distribution functions closely approximate one another under variance matching in binary choice tasks, and this observation is frequently cited to justify pragmatic model choice. Yet this numerical similarity is rarely given a deeper theoretical explanation. Instead, logit and probit are typically presented as competing assumptions about the distribution of unobserved utility noise, reflecting different historical traditions rather than different instantiations of a common mechanism.\n\nRecent work has renewed attention to this issue. Robinson et al. (2023) revisited the tension between Luce-based choice models and signal detection theory, applying a critical test of parameter invariance across changes in the number of alternatives ($m$). Their empirical analysis demonstrated that Gaussian signal detection parameters ($d'$) remained stable as the choice set size increased, whereas softmax parameters ($\\beta$) exhibited systematic variance. While this diagnostic work highlights the superior generalization of Gaussian assumptions in certain memory tasks, it leaves open the generative question: is there a single mechanism that can account for both regimes?\n\nHowever, this work compares fixed model classes rather than deriving them from a common generative mechanism.\n\nThe present work addresses this by shifting perspective. Rather than beginning with static assumptions about utility noise, we begin with a stochastic accumulation process and ask what class of random utility models it induces.\n\n## A stochastic accumulation perspective\n\nWe introduce a **Poisson count race**, a simple generative model in which each alternative produces stochastic events over time according to an independent Poisson process. A decision is made when one alternative reaches a fixed cumulative count threshold, denoted by $\\theta$. The threshold $\\theta$ thus controls how much stochastic evidence must be accumulated before commitment.\n\nPsychologically, $\\theta$ can be interpreted as a commitment criterion or decision caution parameter: higher thresholds require more accumulated events before choice.\n\nThis construction generalizes the classical exponential race in a minimal way. When $\\theta = 1$, the model reduces exactly to the familiar exponential race, recovering Luce's choice rule and the Multinomial Logit model without approximation. For $\\theta > 1$, choice depends not on the first event, but on the time required to accumulate multiple events, introducing a new degree of freedom into the stochastic mechanism.\n\nCrucially, the Poisson count race admits an exact Random Utility representation. The waiting time for an alternative to reach $\\theta$ events follows a Gamma (Erlang) distribution, and comparing these waiting times is equivalent to comparing latent utilities composed of a deterministic component and a stochastic component with a log-Gamma distribution. Varying the threshold $\\theta$ therefore induces a one-parameter family of random utility models with a shared systematic utility but systematically varying noise distributions.\n\nAlthough the Poisson count race is an accumulation process, the present work does not aim to model response times or trial-by-trial dynamics. Instead, accumulation is used here as a generative device for inducing a family of random utility models with interpretable noise structure.\n\n## Noise scale, noise shape, and identification\n\nA central difficulty in comparing discrete choice models with different error distributions is the identification of scale. In standard practice, the scale of utility noise is absorbed into the coefficients, making it difficult to compare models with different distributional assumptions on equal footing. This issue is particularly salient when contrasting logit and probit models, whose error distributions differ not only in shape but also in variance and tail behavior.\n\nTo address this, we introduce a **temperature identification** that standardizes the stochastic component of utility to have fixed variance across all values of $\\theta$. This separates two conceptually distinct aspects of randomness in choice:\n\n1. **Noise scale**, controlled by a temperature parameter $\\beta$, which governs the overall magnitude of stochasticity relative to systematic utility.\n\n2. **Noise shape**, controlled by the accumulation threshold $\\theta$, which determines the distributional form of the stochastic component.\n\nUnder this identification, increasing $\\theta$ does not trivially eliminate noise by driving the model toward deterministic choice. Instead, it isolates the effect of changing the shape of stochasticity while holding its magnitude fixed.\n\n## Logit and probit as endpoint regimes\n\nWithin the temperature-identified Poisson count race, two classical models emerge as limiting cases of a single generative mechanism. When $\\theta = 1$, the stochastic utility component is exactly Gumbel distributed, yielding the **Multinomial Logit** model without approximation. As $\\theta \\to \\infty$, the standardized log-Gamma noise converges in distribution to a Gaussian by the Central Limit Theorem, and the resulting choice probabilities converge to those of the **Multinomial Probit** model.\n\nFrom this perspective, the familiar distinction between logit and probit reflects differences in the stopping rule governing an evidence accumulation process, rather than fundamentally distinct assumptions about decision noise. Extreme-value behavior and Gaussian behavior arise naturally as endpoint regimes of the same stochastic system.\n\n## Contribution and overview\n\nThe contribution of this paper is not to introduce a new discrete choice specification, but to provide a unifying generative framework that clarifies the relationship between two foundational models. Specifically, we show that:\n\n1. Multinomial Logit arises exactly as the single-event threshold case of a Poisson race.\n\n2. Multinomial Probit emerges as a non-degenerate asymptotic limit under temperature identification.\n\n3. Intermediate accumulation thresholds correspond to log-Gamma random utility models with a clear process interpretation.\n\nThe remainder of the paper formalizes this framework. Section 2 gives the formal definition of generalized random utility models. Section 3 introduces the Poisson count race and derives its exact random utility representation. Section 4 establishes the logit boundary at $\\theta = 1$. Section 5 introduces temperature identification and analyzes the probit limit as $\\theta \\to \\infty$. Section 6 presents simulations illustrating the interpolation between regimes, and Section 7 discusses implications for discrete choice modeling and evidence accumulation theories.\n\n\n# Formal statement of random utility models\n\nConsider a decision-maker facing a set of $K \\ge 2$ mutually exclusive alternatives. The utility associated with alternative $i$ is decomposed into a systematic component, $v_i$, and a stochastic component, $\\epsilon_i$:\n\n$$U_{i} = v_{i} + \\epsilon_{i}, \\quad i=1, \\dots, K$$\n\nThe decision-maker selects the alternative that maximizes utility:\n\n$$C = \\arg \\max_{i} U_{i}$$\n\nClassically, the distributional assumptions regarding $\\epsilon_i$ dictate the structural form of the model:\n\n1. **Gumbel Errors:** If the $\\epsilon_i$ terms are independent and identically distributed (i.i.d.) according to a Type I Extreme Value distribution, the choice probabilities follow the **Multinomial Logit (MNL)** or Softmax form.\n\n2. **Gaussian Errors:** If the $\\epsilon_i$ terms follow a Multivariate Normal distribution, the choice probabilities are described by the **Multinomial Probit (MNP)** model.\n\nWhile the Logit model offers analytical tractability, the Probit model permits more flexible covariance structures. This paper derives a mechanism that naturally interpolates between these two regimes through a stochastic race process.\n\n# The Generative Model: A Poisson Count Race\n\nLet the accumulation of evidence or preference for each alternative $i$ be modeled by independent Poisson processes, denoted $N_i(t)$, with rate parameters $\\lambda_i > 0$.\n\nWe define a **Count Race** characterized by an integer threshold $\\theta \\ge 1$. The process terminates when any single process accumulates $\\theta$ events.\n\n**Definition 1 (Stopping Time).** The stopping time for the system is defined as:\n\n$$\\tau_{\\theta} = \\inf \\{t \\ge 0 : \\max_{i} N_i(t) = \\theta \\}$$\n\n**Definition 2 (Choice).** The chosen alternative is the specific process that triggers the stopping time:\n\n$$C = \\arg \\max_{i} N_i(\\tau_{\\theta})$$\n\n## Transformation to Waiting Times\n\nTo map this stochastic process to a utility framework, consider $T_i^{(\\theta)}$, the waiting time until the $i$-th process records its $\\theta$-th event:\n\n$$T_i^{(\\theta)} := \\inf \\{t : N_i(t) = \\theta \\}$$\n\nFor a Poisson process with rate $\\lambda_i$, the waiting time to the $\\theta$-th jump follows a Gamma (Erlang) distribution:\n\n$$T_i^{(\\theta)} \\sim \\text{Gamma}(\\text{shape}=\\theta, \\text{rate}=\\lambda_i)$$\n\nThe condition that alternative $i$ \"wins\" the race (i.e., reaches $\\theta$ events first) is equivalent to observing the minimum waiting time:\n\n$$C = \\arg \\min_{i} T_i^{(\\theta)}$$\n\n## The Random Utility Representation\n\nThe Gamma random variables may be standardized. Let $G_i \\sim \\text{Gamma}(\\theta, 1)$ be i.i.d. random variables. Utilizing the scaling property of the Gamma distribution, the waiting times can be expressed as:\n\n$$T_i^{(\\theta)} \\stackrel{d}{=} \\frac{G_i}{\\lambda_i}$$\n\nConsequently, the choice problem is formulated as:\n\n$$C = \\arg \\min_{i} \\left( \\frac{G_i}{\\lambda_i} \\right)$$\n\nApplying the natural logarithm, a monotonic transformation, reverses the optimization direction from minimization to maximization:\n\n$$\\begin{aligned} C &= \\arg \\min_{i} (\\log G_i - \\log \\lambda_i) \\\\ &= \\arg \\max_{i} (\\log \\lambda_i - \\log G_i) \\end{aligned}$$\n\nThis establishes an exact RUM structure:\n\n$$U_i^{(\\theta)} = v_i + \\epsilon_i^{(\\theta)}$$\n\nwhere:\n\n- **Systematic Utility:** $v_i = \\log \\lambda_i$  \n- **Stochastic Error:** $\\epsilon_i^{(\\theta)} = -\\log G_i$, with $G_i \\sim \\text{Gamma}(\\theta, 1)$.\n\nThus, the Poisson count race is isomorphic to a Random Utility Model characterized by Log-Gamma noise.\n\n# The Logit Boundary ($\\theta = 1$)\n\nIn the specific instance where the threshold is a single event ($\\theta = 1$), the waiting time distribution simplifies:\n\n$$G_i \\sim \\text{Gamma}(1, 1) \\equiv \\text{Exponential}(1)$$\n\nA fundamental property of the Gumbel distribution is its relationship to the Exponential distribution:\n\n$$X \\sim \\text{Exp}(1) \\implies -\\log(X) \\sim \\text{Gumbel}(\\text{Type I EV})$$\n\nTherefore, when $\\theta=1$, the noise terms $\\epsilon_i^{(1)}$ are i.i.d. Gumbel. This recovers the exact Multinomial Logit formula:\n\n$$\\Pr(C=i) = \\frac{\\exp(v_i)}{\\sum_{j=1}^K \\exp(v_j)} = \\frac{\\lambda_i}{\\sum_{j=1}^K \\lambda_j}$$\n\n**Result 1:** The Poisson race with threshold $\\theta=1$ corresponds exactly to the Luce Choice Rule (Softmax). This is a classic, well-known derivation.\n\n\n# Temperature Identification\n\nFor thresholds $\\theta > 1$, the error distribution deviates from the Gumbel form. Furthermore, as $\\theta$ increases, the variance of the error term diminishes. Specifically, for $\\epsilon^{(\\theta)} = -\\log G$ where $G \\sim \\text{Gamma}(\\theta, 1)$:\n\n$$\\begin{aligned} \\mathbb{E}[\\epsilon^{(\\theta)}] &= -\\psi(\\theta) \\\\ \\text{Var}(\\epsilon^{(\\theta)}) &= \\psi_1(\\theta) \\end{aligned}$$\n\nwhere $\\psi(\\cdot)$ denotes the digamma function and $\\psi_1(\\cdot)$ the trigamma function.\n\nAs $\\theta \\to \\infty$, $\\psi_1(\\theta) \\approx 1/\\theta \\to 0$. In the absence of standardization, the model becomes deterministic as the stochastic noise vanishes. To facilitate comparison of error \"shapes\" across varying $\\theta$ values, a consistent scale must be enforced. This process is referred to as **Temperature Identification**. Unlike conventional scale normalization, this identification preserves non-degenerate choice behavior as $\\theta$ increases, allowing the large-$\\theta$ limit to be meaningfully interpreted.\n\nWe define the standardized noise term $Z_i^{(\\theta)}$ such that it possesses zero mean and unit variance for all $\\theta$:\n\n$$Z_i^{(\\theta)} := \\frac{\\epsilon_i^{(\\theta)} - \\mu_\\theta}{\\sigma_\\theta} = \\frac{-\\log G_i + \\psi(\\theta)}{\\sqrt{\\psi_1(\\theta)}}$$\n\nWe propose the **Temperature-Identified Family** of utility models:\n\n$$U_i^{(\\theta)} = v_i + \\beta Z_i^{(\\theta)}$$\n\nHere:\n\n- $v_i$ represents the systematic utility.\n- $\\theta$ governs the **shape** of the noise distribution (tail behavior).\n- $\\beta$ represents the **temperature** (the scale of noise relative to utility).\n\n# The Probit Limit ($\\theta \\to \\infty$)\n\nThis section investigates the asymptotic behavior of the temperature-identified model as the count threshold increases. We rely on the asymptotic normality of the log-transformed Gamma distribution.\n\nIntuitively, as the count threshold increases, the log-Gamma noise reflects the aggregation of many small stochastic increments, and Gaussian behavior emerges by the Central Limit Theorem.\n\nAs $\\theta \\to \\infty$:\n\n$$Z_i^{(\\theta)} \\xrightarrow{d} \\mathcal{N}(0, 1)$$\n\nConsequently, the random utilities converge in distribution:\n\n$$(v_i + \\beta Z_i^{(\\theta)})_{i=1}^K \\xrightarrow{d} (v_i + \\beta Z_i)_{i=1}^K$$\n\nwhere $Z_i \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0, 1)$.\n\nGiven that the error terms converge to additive Gaussian noise, the choice probabilities converge to those of the Multinomial Probit model.\n\n**Result 2:** As $\\theta \\to \\infty$, the Temperature-Identified Poisson race converges to a Probit model with noise scale $\\beta$.\n\n# Summary\n\nThe derivation presented herein establishes the Poisson Count Race as a flexible generative mechanism for discrete choice. By identifying the model via temperature standardization, we obtain a single-parameter family (indexed by $\\theta$) that continuously bridges the two classical extremes of choice modeling:\n\n1. $\\theta = 1$**:** Exact **Logit** (Gumbel errors).\n\n2. $1 < \\theta < \\infty$**:** **Log-Gamma** RUM (an intermediate regime).\n\n3. $\\theta \\to \\infty$**:** **Probit** (Gaussian errors).\n\nThis result suggests that the distinction between Logit and Probit may be viewed not as a fundamental difference in error philosophy, but rather as a difference in the stopping rule governing an underlying stochastic accumulation process.\n\n# Simulation Study: Binary Choice\n\nTo illustrate how the Poisson count race family interpolates between Logit and Probit, we conduct a simulation study in the binary choice setting ($K=2$). This setting admits closed-form choice probabilities and allows direct visual comparison with both classical models.\n\nLet $\\lambda_1$ and $\\lambda_2$ denote the Poisson rates of the two alternatives, and define the log-rate ratio $x = \\log(\\lambda_1/\\lambda_2)$. The probability that alternative 1 wins the race can be expressed in closed form using the regularized incomplete Beta function:\n\n$$\\Pr(C = 1 \\mid x, \\theta) = I_{\\sigma(x)}(\\theta, \\theta)$$\n\nwhere $I_p(a, b)$ is the regularized incomplete Beta function and $\\sigma(x) = (1 + e^{-x})^{-1}$. For $\\theta = 1$, this reduces exactly to $\\sigma(x)$, the logistic function.\n\nWhen choice probabilities are plotted directly as a function of $x$ for increasing $\\theta$, the choice function becomes increasingly steep and converges to a step function at $x = 0$, reflecting deterministic selection of the alternative with the larger rate. This confirms that without temperature identification, increasing the count threshold simply reduces stochasticity rather than inducing Gaussian behavior.\n\nTo compare noise *shape* independently of noise *scale*, we adopt the temperature identification introduced in Section 5. For binary choice, the variance of the utility noise difference is $\\text{Var}(\\epsilon_1^{(\\theta)} - \\epsilon_2^{(\\theta)}) = 2\\psi_1(\\theta)$. We therefore define the standardized signal $s = x / \\sqrt{2\\psi_1(\\theta)}$. On this variance-matched axis, the Logit reference ($\\theta = 1$) uses $\\text{sd}_{\\text{diff}} = \\pi/\\sqrt{3}$ (the standard deviation of the difference of two independent Gumbel variates), and the Probit reference is simply $\\Phi(s)$. Under this normalization, the $\\theta = 1$ Poisson race coincides exactly with the variance-matched logit curve, while increasing $\\theta$ yields choice functions that converge uniformly to the probit curve.\n\nBecause logit and probit are themselves numerically close under variance matching, the differences between models are small in absolute magnitude but systematic. To make these differences visible, @fig-residuals plots residuals relative to the variance-matched Logit model. At $\\theta = 1$, the residual is identically zero (exact Logit). As $\\theta$ increases, the residuals grow smoothly and converge toward the Probit$-$Logit difference curve, with the maximum absolute deviation from probit decaying rapidly in $\\theta$. This confirms that the temperature-identified Poisson count race defines a continuous, parameterized family of choice rules that interpolates smoothly between logit-like and probit-like behavior.\n\n\n::: {#cell-fig-residuals .cell}\n\n```{.r .cell-code .hidden}\nsource(\"R/race_functions.R\")\n\nlibrary(ggplot2)\n\ns <- seq(-4, 4, length.out = 1200)\nthetas <- c(1, 2, 3, 5, 10, 20, 50, 200)\n\n# Build data frame\ndf <- data.frame(\n  s = s,\n  residual = probit_residual(s),\n  model = \"Probit\"\n)\n\nfor (th in thetas) {\n  df <- rbind(df, data.frame(\n    s = s,\n    residual = race_residual(s, th),\n    model = paste0(\"θ = \", th)\n  ))\n}\n\n# Order factor levels so Probit appears first, then thetas in order\ndf$model <- factor(df$model,\n  levels = c(\"Probit\", paste0(\"θ = \", thetas))\n)\n\nggplot(df, aes(x = s, y = residual, color = model, linewidth = model)) +\n  geom_line() +\n  geom_hline(yintercept = 0, linewidth = 0.5) +\n  scale_linewidth_manual(\n    values = c(\"Probit\" = 1.2, setNames(rep(0.6, length(thetas)),\n                                         paste0(\"θ = \", thetas)))\n  ) +\n  labs(\n    x = expression(\"Rescaled signal \" ~ s == log(lambda[1]/lambda[2]) / sd(epsilon[1] - epsilon[2])),\n    y = expression(\"Residual: \" ~ Pr(choice == 1) - Logit[ref](s)),\n    color = \"Model\",\n    linewidth = \"Model\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![Residuals of Poisson count race choice probabilities relative to the Logit reference, plotted on a variance-matched axis. Each curve corresponds to a different accumulation threshold θ. At θ = 1 the model is exactly Logit (zero residual). As θ increases, the curves converge toward the Probit − Logit difference (black curve), confirming the theoretical bridge between the two models.](index_files/figure-html/fig-residuals-1.png){#fig-residuals width=672}\n:::\n:::\n\n\n# Discussion\n\nThe present work develops a generative framework in which two foundational discrete choice models—Multinomial Logit and Multinomial Probit—arise as endpoint regimes of a single stochastic accumulation process. By introducing a Poisson count race and a temperature identification that separates noise scale from noise shape, we provide a principled account of how extreme-value and Gaussian choice behavior can emerge from different stopping rules applied to the same underlying mechanism.\n\nThe goal of this paper is not to advocate replacing existing models, but to clarify their relationship. Logit and probit are often treated as competing assumptions about the distribution of unobserved utility noise. Our results suggest a different interpretation: these models correspond to distinct regimes of evidence accumulation, characterized by how much stochastic evidence is required before commitment.\n\n## Relation to existing choice models\n\nFrom a mathematical standpoint, all components of the present framework are classical. Exponential races yield Luce’s choice rule, Gamma waiting times arise from accumulated Poisson events, and asymptotic normality follows from the Central Limit Theorem. The contribution lies in assembling these elements into a single, coherent generative family and identifying the conditions under which its limiting behavior remains non-degenerate.\n\nThis synthesis clarifies why logit and probit models often behave similarly in practice, particularly under variance matching, while also explaining why they diverge systematically in multi-alternative settings. Rather than viewing these differences as artifacts of arbitrary distributional assumptions, the Poisson count race shows how they arise naturally from differences in stopping rules.\n\nImportantly, the present model should not be conflated with full sequential sampling models such as the Diffusion Decision Model or Linear Ballistic Accumulator. Those models are designed to jointly account for response times and accuracy and typically involve continuous accumulation with explicit drift and boundary parameters. The Poisson count race, by contrast, is deliberately minimal: it uses accumulation as a generative device to induce a family of random utility models, without making claims about within-trial dynamics or response time distributions. In this sense, it occupies an intermediate position between static random utility formulations and full dynamical decision models.\n\n## Temperature identification and model comparison\n\nA central technical step in the present work is the introduction of a temperature identification that standardizes the variance of the stochastic utility component across values of the accumulation threshold. Without this identification, increasing the threshold trivially drives the model toward deterministic choice, obscuring the effect of changing noise shape.\n\nBy fixing the scale of stochasticity and allowing only its shape to vary, the temperature-identified Poisson count race makes it possible to meaningfully compare models with different error distributions. This perspective reframes familiar variance-matching arguments in a generative context: rather than adjusting scale parameters post hoc to align predictions, scale normalization becomes an intrinsic part of the model definition.\n\nUnder this identification, the large-threshold limit yields a genuine Multinomial Probit model, not merely an approximation. This provides a process-level interpretation of probit that parallels the classical exponential race interpretation of logit and helps explain why Gaussian assumptions often exhibit greater parameter stability across changes in task structure.\n\n## Intermediate regimes and log-Gamma utility noise\n\nBetween the logit and probit endpoints lies a continuum of models characterized by log-Gamma utility noise. These intermediate regimes are not intended as new “default” choice models, but they may be useful in settings where the strict independence properties of logit are too restrictive, while the computational complexity of probit is undesirable.\n\nFrom a theoretical perspective, the existence of these intermediate regimes underscores that logit and probit are not isolated modeling choices, but special cases of a broader family. From a practical perspective, they suggest that deviations from logit or probit behavior may sometimes reflect differences in accumulation thresholds rather than fundamentally different noise sources.\n\n## Implications for empirical modeling\n\nRecent empirical work has emphasized differences in parameter invariance and generalization between logit-based and signal-detection-based models across task structures. The present framework complements this line of research by offering a theoretical account of why such differences may arise. In particular, models with larger effective accumulation thresholds naturally exhibit Gaussian-like behavior, which may confer greater stability across changes in the number of alternatives or decision format.\n\nAt the same time, the present results caution against interpreting superior empirical performance of one model class as evidence for a particular noise distribution in isolation. Differences between logit and probit may reflect differences in decision criteria or commitment thresholds rather than differences in representational noise per se.\n\n## Limitations and extensions\n\nThe Poisson count race is intentionally simple. It assumes independent accumulation processes and focuses exclusively on choice probabilities, abstracting away from response times and within-trial dynamics. Extensions that relax these assumptions—by allowing correlated accumulators, time-varying rates, or joint modeling of choice and response time—are natural directions for future work.\n\nAnother limitation is that the present analysis treats the accumulation threshold as fixed across trials and alternatives. Allowing threshold variability or adaptive stopping rules could further enrich the family of induced choice models and connect more directly to theories of decision caution and speed–accuracy trade-offs.\n\nFinally, while the present paper focuses on the relationship between logit and probit, the framework naturally invites comparison with other random utility specifications. Exploring whether additional classical models arise as limiting regimes under alternative accumulation rules may provide further insight into the structure of discrete choice behavior.\n\n## Concluding remarks\n\nBy grounding discrete choice models in a common stochastic accumulation process, the Poisson count race reframes a long-standing modeling distinction in a new light. Logit and probit emerge not as competing assumptions about utility noise, but as endpoint regimes corresponding to different stopping rules applied to the same underlying mechanism.\n\nThis unification does not diminish the practical differences between these models, but it clarifies their conceptual relationship and provides a principled basis for comparison. More broadly, it illustrates how process-level reasoning can illuminate the structure of static choice models and suggests that some longstanding modeling dichotomies may reflect differences in perspective rather than fundamental incompatibilities.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}