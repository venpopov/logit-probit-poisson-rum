{
  "hash": "8c5aaf173aff7c95b3740a8eb7d1690c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multinomial Simulation Study: Poisson Count Race\"\nformat: \n  html:\n    toc: true\n    code-fold: true\n    fig-width: 8\n    fig-height: 6\nexecute:\n  warning: false\n  message: false\n---\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n#| label: setup\nlibrary(ggplot2)\nlibrary(patchwork)\n\ntheme_set(theme_minimal(base_size = 13))\n\n# ── Core simulation engine ──────────────────────────────────────────────────\n#' Simulate choice probabilities from the temperature-identified Poisson count race\n#'\n#' @param v   Numeric vector of systematic utilities (length K)\n#' @param theta  Accumulation threshold (positive integer)\n#' @param beta   Temperature (noise scale)\n#' @param n_sim  Number of Monte Carlo draws\n#' @return Named numeric vector of estimated choice probabilities\nrace_choice_probs <- function(v, theta, beta = 1, n_sim = 1e6) {\n  K <- length(v)\n  # Draw Gamma(theta, 1) for each alternative and each simulation\n  G <- matrix(rgamma(n_sim * K, shape = theta, rate = 1), nrow = n_sim, ncol = K)\n  # Standardised log-Gamma noise\n  mu_theta <- digamma(theta)\n  sd_theta <- sqrt(trigamma(theta))\n  Z <- (-log(G) + mu_theta) / sd_theta\n  # Temperature-identified utilities\n  U <- sweep(Z * beta, 2, v, \"+\")\n  # Choice = argmax\n  choices <- max.col(U, ties.method = \"random\")\n  tabulate(choices, nbins = K) / n_sim\n}\n\n#' Multinomial Logit (softmax) choice probabilities (exact)\n#'\n#' Under temperature identification with fixed beta, the Gumbel scale parameter\n#' is b = beta * sqrt(6) / pi, so the softmax inverse temperature is\n#' pi / (beta * sqrt(6)).\n#' @param v   Numeric vector of systematic utilities\n#' @param beta  Temperature\n#' @return Numeric vector of choice probabilities\nmnl_probs <- function(v, beta = 1) {\n  sigma_gumbel <- pi / sqrt(6)   # SD of Gumbel(0,1)\n  scaled <- v * sigma_gumbel / beta  # = v * pi / (beta * sqrt(6))\n  ev <- exp(scaled - max(scaled))\n  ev / sum(ev)\n}\n\n#' Multinomial Probit choice probabilities (Monte Carlo)\n#'\n#' Independent equal-variance Gaussian errors with scale beta.\n#' @param v   Numeric vector of systematic utilities\n#' @param beta  Temperature\n#' @param n_sim Number of Monte Carlo draws\n#' @return Numeric vector of estimated choice probabilities\nmnp_probs <- function(v, beta = 1, n_sim = 1e6) {\n  K <- length(v)\n  Z <- matrix(rnorm(n_sim * K), nrow = n_sim, ncol = K)\n  U <- sweep(Z * beta, 2, v, \"+\")\n  choices <- max.col(U, ties.method = \"random\")\n  tabulate(choices, nbins = K) / n_sim\n}\n\n#' Total variation distance between two probability vectors\ntv_dist <- function(p, q) 0.5 * sum(abs(p - q))\n```\n:::\n\n\n# Introduction\n\nThe binary simulation in the main text demonstrates that the temperature-identified Poisson count race interpolates smoothly between Logit ($\\theta = 1$) and Probit ($\\theta \\to \\infty$) in the two-alternative case. Here we extend this analysis to the multinomial setting ($K > 2$), where the differences between logit and probit become richer and more consequential.\n\nIn the binary case, logit and probit choice functions differ only in the shape of the psychometric curve—a subtle quantitative distinction. With three or more alternatives, additional qualitative differences emerge. Most prominently, the Multinomial Logit model satisfies the *Independence of Irrelevant Alternatives* (IIA) property: the ratio of choice probabilities for any two alternatives is independent of the remaining alternatives in the choice set. The Multinomial Probit model, even with independent errors, does not share this property. The Poisson count race therefore provides a window into how IIA-like behavior gradually weakens as the noise distribution transitions from Gumbel to Gaussian.\n\nWe organise the multinomial simulations around five questions:\n\n1. **Convergence**: How quickly do Poisson count race choice probabilities converge to the Probit reference as $\\theta$ increases, and does the rate of convergence depend on $K$?\n2. **Probability vectors**: How does the full distribution over alternatives change as $\\theta$ varies from 1 to large values?\n3. **Set-size scaling**: How does the probability of choosing a target alternative scale with the number of competitors, and how does this scaling differ between logit, probit, and intermediate regimes?\n4. **Independence of Irrelevant Alternatives**: How does the IIA property—exact under logit—erode as $\\theta$ increases toward the probit regime?\n5. **Parameter invariance**: When misspecified logit or probit models are fit to Poisson count race data, which model yields parameters that are invariant to $K$?\n\nAll simulations use Monte Carlo sampling with $10^6$ replications per condition unless otherwise noted.\n\n# Study 1: Convergence to Probit\n\nWe first examine how the total variation (TV) distance between the Poisson count race choice probabilities and the Logit / Probit references changes as a function of $\\theta$, for different numbers of alternatives $K$.\n\nFor each $K$, we use linearly spaced utilities $v_i = (K - i)/(K - 1)$ for $i = 1, \\ldots, K$, ensuring that the best and worst alternatives always have utilities 1 and 0 regardless of $K$. The temperature is fixed at $\\beta = 1$.\n\n\n::: {#cell-fig-tv-distance .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-tv-distance\n#| fig-cap: \"Total variation distance between Poisson count race choice probabilities and the Logit (blue) and Probit (red) references, as a function of threshold θ. Each panel corresponds to a different number of alternatives K. As θ increases, the race model moves away from logit and toward probit across all values of K. The convergence to probit is rapid: by θ ≈ 20–50, the TV distance from probit is negligible.\"\n#| fig-height: 5\n#| fig-width: 10\n\nset.seed(42)\nn_sim <- 1e6\n\nKs <- c(3, 5, 8)\nthetas <- c(1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100, 200)\n\nresults <- data.frame()\n\nfor (K in Ks) {\n  v <- seq(1, 0, length.out = K)  # linearly spaced utilities\n  \n  # Reference models\n  p_logit <- mnl_probs(v, beta = 1)\n  p_probit <- mnp_probs(v, beta = 1, n_sim = 2e6)\n  \n  for (th in thetas) {\n    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n    \n    results <- rbind(results, data.frame(\n      K = K,\n      theta = th,\n      tv_logit = tv_dist(p_race, p_logit),\n      tv_probit = tv_dist(p_race, p_probit)\n    ))\n  }\n}\n\n# Reshape for plotting\ndf_tv <- rbind(\n  data.frame(K = results$K, theta = results$theta, \n             TV = results$tv_logit, Reference = \"vs. Logit\"),\n  data.frame(K = results$K, theta = results$theta, \n             TV = results$tv_probit, Reference = \"vs. Probit\")\n)\ndf_tv$K_label <- paste0(\"K = \", df_tv$K)\ndf_tv$K_label <- factor(df_tv$K_label, levels = paste0(\"K = \", Ks))\n\nggplot(df_tv, aes(x = theta, y = TV, color = Reference)) +\n  geom_line(linewidth = 0.8) +\n  geom_point(size = 1.5) +\n  scale_x_log10() +\n  facet_wrap(~K_label) +\n  scale_color_manual(values = c(\"vs. Logit\" = \"steelblue\", \"vs. Probit\" = \"firebrick\")) +\n  labs(\n    x = expression(\"Threshold \" * theta),\n    y = \"Total variation distance\",\n    color = \"Reference\"\n  ) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Total variation distance between Poisson count race choice probabilities and the Logit (blue) and Probit (red) references, as a function of threshold θ. Each panel corresponds to a different number of alternatives K. As θ increases, the race model moves away from logit and toward probit across all values of K. The convergence to probit is rapid: by θ ≈ 20–50, the TV distance from probit is negligible.](multinomial_simulations_files/figure-html/fig-tv-distance-1.png){#fig-tv-distance width=960}\n:::\n:::\n\n\n\n# Study 2: Choice Probability Vectors\n\nTo visualise how the full distribution over alternatives evolves with $\\theta$, we fix $K = 5$ with utilities $v = (2.0,\\; 1.5,\\; 1.0,\\; 0.5,\\; 0.0)$ and plot the choice probability for each alternative across a range of thresholds.\n\n\n::: {#cell-fig-prob-vectors .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-prob-vectors\n#| fig-cap: \"Choice probabilities for each of five alternatives as a function of threshold θ, under the temperature-identified Poisson count race with β = 1. Horizontal dashed lines mark the Logit reference (θ = 1); horizontal dotted lines mark the Probit reference (θ → ∞). As θ increases, the race probabilities transition smoothly from logit to probit values. The probit model concentrates slightly more probability on the best alternative and less on the worst, relative to logit, reflecting the thinner tails of the Gaussian distribution.\"\n#| fig-height: 5\n#| fig-width: 8\n\nset.seed(123)\nK <- 5\nv <- seq(2, 0, length.out = K)\nthetas_fine <- c(1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100, 200)\nn_sim <- 2e6\n\n# References\np_logit <- mnl_probs(v, beta = 1)\np_probit <- mnp_probs(v, beta = 1, n_sim = 5e6)\n\n# Race probabilities across theta\ndf_probs <- data.frame()\nfor (th in thetas_fine) {\n  p <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n  for (i in 1:K) {\n    df_probs <- rbind(df_probs, data.frame(\n      theta = th,\n      alternative = paste0(\"Alt \", i, \" (v=\", v[i], \")\"),\n      prob = p[i]\n    ))\n  }\n}\n\n# Reference data\ndf_ref <- data.frame()\nfor (i in 1:K) {\n  df_ref <- rbind(df_ref, data.frame(\n    alternative = paste0(\"Alt \", i, \" (v=\", v[i], \")\"),\n    logit = p_logit[i],\n    probit = p_probit[i]\n  ))\n}\n\ndf_probs$alternative <- factor(df_probs$alternative, \n                                levels = paste0(\"Alt \", 1:K, \" (v=\", v, \")\"))\ndf_ref$alternative <- factor(df_ref$alternative,\n                              levels = paste0(\"Alt \", 1:K, \" (v=\", v, \")\"))\n\nggplot(df_probs, aes(x = theta, y = prob, color = alternative)) +\n  geom_line(linewidth = 0.9) +\n  geom_point(size = 1.2) +\n  geom_hline(data = df_ref, aes(yintercept = logit, color = alternative), \n             linetype = \"dashed\", linewidth = 0.5, alpha = 0.7) +\n  geom_hline(data = df_ref, aes(yintercept = probit, color = alternative), \n             linetype = \"dotted\", linewidth = 0.5, alpha = 0.7) +\n  scale_x_log10() +\n  labs(\n    x = expression(\"Threshold \" * theta),\n    y = \"Choice probability\",\n    color = \"Alternative\"\n  ) +\n  theme(legend.position = \"right\") +\n  annotate(\"text\", x = 250, y = max(p_logit) + 0.01, label = \"— Logit    ··· Probit\", \n           size = 3, hjust = 1)\n```\n\n::: {.cell-output-display}\n![Choice probabilities for each of five alternatives as a function of threshold θ, under the temperature-identified Poisson count race with β = 1. Horizontal dashed lines mark the Logit reference (θ = 1); horizontal dotted lines mark the Probit reference (θ → ∞). As θ increases, the race probabilities transition smoothly from logit to probit values. The probit model concentrates slightly more probability on the best alternative and less on the worst, relative to logit, reflecting the thinner tails of the Gaussian distribution.](multinomial_simulations_files/figure-html/fig-prob-vectors-1.png){#fig-prob-vectors width=768}\n:::\n:::\n\n\n\n# Study 3: Set-Size Scaling\n\nA critical diagnostic for discriminating between logit and probit models is the effect of adding alternatives to the choice set (Robinson et al., 2023). Under MNL, the probability of choosing a target alternative with fixed utility is strictly determined by the ratio of its strength to the total strength. Under MNP, the scaling with set size differs because the probability of \"winning\" the maximum comparison depends on the shape of the noise distribution.\n\nWe fix a target alternative with utility $v_\\text{target} = 1$ and add $K - 1$ equal competitors, each with utility $v_\\text{comp} = 0$. As $K$ grows, we track the probability of choosing the target.\n\nUnder MNL, the choice probability is $P(\\text{target}) = e^a / (e^a + K - 1)$, where $a = v_t \\cdot \\pi / (\\beta \\sqrt{6})$ is the effective scaled utility.\n\nUnder MNP: $P(\\text{target}) = \\int \\phi(z) \\,\\Phi(v_t/\\beta + z)^{K-1}\\, dz$ (by symmetry of the $K - 1$ equal competitors). This integral reveals that probit's thinner tails give the target a larger advantage over many competitors than logit's heavier tails.\n\n\n::: {#cell-fig-set-size .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-set-size\n#| fig-cap: \"Probability of choosing a target alternative (v = 1) against K − 1 equal competitors (v = 0) as a function of set size K, for selected values of θ. MNL (dashed blue) and MNP (dashed red) references are shown. As θ increases, the race model transitions from logit-like to probit-like set-size scaling. The probit model assigns systematically higher probability to the target than logit when K is large, reflecting the thinner tails of Gaussian noise.\"\n#| fig-height: 5\n#| fig-width: 8\n\nset.seed(456)\nn_sim <- 2e6\n\nKs <- c(2, 3, 4, 5, 6, 8, 10, 15, 20)\nthetas_set <- c(2, 5, 10, 50, 200)\n\nv_target <- 1\nv_comp <- 0\n\ndf_setsize <- data.frame()\n\nfor (K in Ks) {\n  v <- c(v_target, rep(v_comp, K - 1))\n  \n  # References\n  p_logit_target <- mnl_probs(v, beta = 1)[1]\n  p_probit_target <- mnp_probs(v, beta = 1, n_sim = n_sim)[1]\n  \n  df_setsize <- rbind(df_setsize, data.frame(\n    K = K, theta = NA, prob = p_logit_target, model = \"Logit (MNL)\"\n  ))\n  df_setsize <- rbind(df_setsize, data.frame(\n    K = K, theta = NA, prob = p_probit_target, model = \"Probit (MNP)\"\n  ))\n  \n  for (th in thetas_set) {\n    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n    df_setsize <- rbind(df_setsize, data.frame(\n      K = K, theta = th, prob = p_race[1], model = paste0(\"θ = \", th)\n    ))\n  }\n}\n\n# Separate reference and race data\ndf_ref_lines <- df_setsize[df_setsize$model %in% c(\"Logit (MNL)\", \"Probit (MNP)\"), ]\ndf_race_lines <- df_setsize[!df_setsize$model %in% c(\"Logit (MNL)\", \"Probit (MNP)\"), ]\n\ndf_race_lines$model <- factor(df_race_lines$model, \n                               levels = paste0(\"θ = \", thetas_set))\n\nggplot() +\n  geom_line(data = df_ref_lines, aes(x = K, y = prob, color = model), \n            linetype = \"dashed\", linewidth = 1) +\n  geom_point(data = df_ref_lines, aes(x = K, y = prob, color = model), size = 2) +\n  geom_line(data = df_race_lines, aes(x = K, y = prob, group = model, color = model), \n            linewidth = 0.7) +\n  geom_point(data = df_race_lines, aes(x = K, y = prob, color = model), size = 1.3) +\n  scale_color_manual(\n    values = c(\n      \"Logit (MNL)\" = \"steelblue\",\n      \"Probit (MNP)\" = \"firebrick\",\n      setNames(scales::hue_pal()(length(thetas_set)), paste0(\"θ = \", thetas_set))\n    )\n  ) +\n  labs(\n    x = \"Number of alternatives K\",\n    y = expression(\"P(target chosen)\"),\n    color = \"Model\"\n  ) +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![Probability of choosing a target alternative (v = 1) against K − 1 equal competitors (v = 0) as a function of set size K, for selected values of θ. MNL (dashed blue) and MNP (dashed red) references are shown. As θ increases, the race model transitions from logit-like to probit-like set-size scaling. The probit model assigns systematically higher probability to the target than logit when K is large, reflecting the thinner tails of Gaussian noise.](multinomial_simulations_files/figure-html/fig-set-size-1.png){#fig-set-size width=768}\n:::\n:::\n\n\n\n# Study 4: Independence of Irrelevant Alternatives\n\nThe IIA property is a hallmark of the Multinomial Logit model: the ratio of choice probabilities for any two alternatives is invariant to the composition of the choice set. Formally, for alternatives $i$ and $j$:\n\n$$\\frac{P(i \\mid \\mathcal{C})}{P(j \\mid \\mathcal{C})} = \\frac{e^{v_i}}{e^{v_j}} \\quad \\text{for all choice sets } \\mathcal{C} \\ni i, j$$\n\nThis property does not hold for the Multinomial Probit model, even when errors are independent and identically distributed. The Poisson count race therefore provides a mechanism through which IIA holds exactly at $\\theta = 1$ and is progressively violated as $\\theta$ increases.\n\nTo quantify this, we consider three alternatives with utilities $v = (2, 1, 0)$. We compute the ratio $P(1)/P(2)$ under two conditions:\n\n- **Full set**: all three alternatives present $\\{1, 2, 3\\}$\n- **Reduced set**: only alternatives $\\{1, 2\\}$ present\n\nUnder IIA, these ratios should be identical. We track the percentage change in the ratio as $\\theta$ varies.\n\n\n::: {#cell-fig-iia .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-iia\n#| fig-cap: \"Test of Independence of Irrelevant Alternatives (IIA). Left: the ratio P(Alt 1)/P(Alt 2) computed in the full set {1, 2, 3} (solid) and the reduced set {1, 2} (dashed), as a function of θ. Under IIA (θ = 1), the two ratios are equal. As θ increases, they diverge. Right: percentage change in the ratio when alternative 3 is removed, quantifying the IIA violation. The violation increases with θ, confirming that IIA is a property of the Gumbel noise shape that erodes as the noise distribution shifts toward Gaussian.\"\n#| fig-height: 5\n#| fig-width: 10\n\nset.seed(789)\nn_sim <- 5e6\n\nv_full <- c(2, 1, 0)\nv_reduced <- c(2, 1)\n\nthetas_iia <- c(1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100, 200)\n\ndf_iia <- data.frame()\n\nfor (th in thetas_iia) {\n  # Full set {1, 2, 3}\n  p_full <- race_choice_probs(v_full, theta = th, beta = 1, n_sim = n_sim)\n  ratio_full <- p_full[1] / p_full[2]\n  \n  # Reduced set {1, 2}\n  p_reduced <- race_choice_probs(v_reduced, theta = th, beta = 1, n_sim = n_sim)\n  ratio_reduced <- p_reduced[1] / p_reduced[2]\n  \n  df_iia <- rbind(df_iia, data.frame(\n    theta = th,\n    ratio_full = ratio_full,\n    ratio_reduced = ratio_reduced,\n    pct_change = 100 * (ratio_reduced - ratio_full) / ratio_full\n  ))\n}\n\n# Also compute reference values\np_logit_full <- mnl_probs(v_full)[1] / mnl_probs(v_full)[2]\np_logit_reduced <- mnl_probs(v_reduced)[1] / mnl_probs(v_reduced)[2]\n\np_probit_full <- mnp_probs(v_full, n_sim = 5e6)\nr_probit_full <- p_probit_full[1] / p_probit_full[2]\np_probit_reduced <- mnp_probs(v_reduced, n_sim = 5e6)\nr_probit_reduced <- p_probit_reduced[1] / p_probit_reduced[2]\nprobit_pct <- 100 * (r_probit_reduced - r_probit_full) / r_probit_full\n\n# Left panel: ratios\ndf_ratios <- rbind(\n  data.frame(theta = df_iia$theta, ratio = df_iia$ratio_full, \n             set = \"Full {1, 2, 3}\"),\n  data.frame(theta = df_iia$theta, ratio = df_iia$ratio_reduced, \n             set = \"Reduced {1, 2}\")\n)\n\np1 <- ggplot(df_ratios, aes(x = theta, y = ratio, color = set, linetype = set)) +\n  geom_line(linewidth = 0.8) +\n  geom_point(size = 1.5) +\n  scale_x_log10() +\n  scale_linetype_manual(values = c(\"Full {1, 2, 3}\" = \"solid\", \"Reduced {1, 2}\" = \"dashed\")) +\n  scale_color_manual(values = c(\"Full {1, 2, 3}\" = \"steelblue\", \"Reduced {1, 2}\" = \"darkorange\")) +\n  labs(\n    x = expression(\"Threshold \" * theta),\n    y = \"P(Alt 1) / P(Alt 2)\",\n    color = \"Choice set\",\n    linetype = \"Choice set\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n# Right panel: IIA violation\np2 <- ggplot(df_iia, aes(x = theta, y = pct_change)) +\n  geom_line(linewidth = 0.8, color = \"grey30\") +\n  geom_point(size = 1.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"steelblue\", linewidth = 0.6) +\n  geom_hline(yintercept = probit_pct, linetype = \"dashed\", color = \"firebrick\", linewidth = 0.6) +\n  annotate(\"text\", x = 1.2, y = 0.5, label = \"Logit (IIA)\", color = \"steelblue\", \n           hjust = 0, size = 3.5) +\n  annotate(\"text\", x = 1.2, y = probit_pct - 0.5, label = \"Probit limit\", color = \"firebrick\", \n           hjust = 0, size = 3.5) +\n  scale_x_log10() +\n  labs(\n    x = expression(\"Threshold \" * theta),\n    y = \"% change in P(1)/P(2)\\nwhen Alt 3 removed\"\n  )\n\np1 + p2 + plot_layout(widths = c(1, 1))\n```\n\n::: {.cell-output-display}\n![Test of Independence of Irrelevant Alternatives (IIA). Left: the ratio P(Alt 1)/P(Alt 2) computed in the full set {1, 2, 3} (solid) and the reduced set {1, 2} (dashed), as a function of θ. Under IIA (θ = 1), the two ratios are equal. As θ increases, they diverge. Right: percentage change in the ratio when alternative 3 is removed, quantifying the IIA violation. The violation increases with θ, confirming that IIA is a property of the Gumbel noise shape that erodes as the noise distribution shifts toward Gaussian.](multinomial_simulations_files/figure-html/fig-iia-1.png){#fig-iia width=960}\n:::\n:::\n\n\n\n# Study 5: Parameter Invariance Across Set Size\n\nA key empirical diagnostic for distinguishing between logit and probit is parameter invariance across changes in set size $K$ (Robinson et al., 2023). If choice data are generated by a logit model, the softmax inverse temperature $\\beta_{\\text{logit}}$ recovered from fitting a logit specification should remain constant as $K$ increases. Conversely, if the data follow a probit model, the Gaussian noise scale $\\beta_{\\text{probit}}$ should be invariant to $K$.\n\nWe test this directly. For each value of $\\theta$ and each set size $K$ (with a target at $v = 1$ vs.\\ $K-1$ equal competitors at $v = 0$), we compute the \"true\" choice probability $P(\\text{target})$ from the race model and then recover the best-fitting logit and probit temperature parameters by inversion.\n\n\n::: {#cell-fig-parameter-recovery .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-parameter-recovery\n#| fig-cap: \"Recovered temperature parameters under logit (left) and probit (right) model assumptions, plotted as a function of set size K. Each line corresponds to a different accumulation threshold θ. At θ = 1 (exact logit), the recovered logit parameter is invariant to K, while the recovered probit parameter drifts. At large θ (approaching probit), the pattern reverses: the probit parameter is stable while the logit parameter varies. This directly parallels the empirical diagnostic of Robinson et al. (2023).\"\n#| fig-height: 5\n#| fig-width: 10\n\nset.seed(303)\nn_sim <- 3e6\n\nKs_param <- c(2, 3, 4, 5, 6, 8, 10, 15, 20)\nthetas_param <- c(1, 3, 10, 50, 200)\nv_target <- 1\nsigma_G <- pi / sqrt(6)\n\n# Helper: probit P(target) for symmetric case (1 target vs K-1 equal competitors)\nprobit_target_prob <- function(v_t, beta, K) {\n  integrand <- function(z) dnorm(z) * pnorm(v_t / beta + z)^(K - 1)\n  integrate(integrand, -10, 10, rel.tol = 1e-10)$value\n}\n\n# Recover logit beta from observed P(target)\nrecover_logit_beta <- function(p_target, v_t, K) {\n  if (p_target <= 1/K || p_target >= 1) return(NA_real_)\n  a <- log(p_target * (K - 1) / (1 - p_target))\n  if (a <= 0) return(NA_real_)\n  v_t * sigma_G / a\n}\n\n# Recover probit beta from observed P(target) via root-finding\nrecover_probit_beta <- function(p_target, v_t, K) {\n  if (p_target <= 1/K || p_target >= 1) return(NA_real_)\n  f <- function(log_beta) probit_target_prob(v_t, exp(log_beta), K) - p_target\n  tryCatch({\n    root <- uniroot(f, interval = c(log(0.01), log(50)), tol = 1e-8)\n    exp(root$root)\n  }, error = function(e) NA_real_)\n}\n\ndf_param <- data.frame()\n\nfor (th in thetas_param) {\n  for (K in Ks_param) {\n    v <- c(v_target, rep(0, K - 1))\n    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)[1]\n    \n    beta_logit <- recover_logit_beta(p_race, v_target, K)\n    beta_probit <- recover_probit_beta(p_race, v_target, K)\n    \n    df_param <- rbind(df_param, data.frame(\n      K = K, theta = th, \n      beta_logit = beta_logit, \n      beta_probit = beta_probit,\n      model_label = paste0(\"θ = \", th)\n    ))\n  }\n}\n\ndf_param$model_label <- factor(df_param$model_label, \n                                levels = paste0(\"θ = \", thetas_param))\n\n# Reshape for faceting\ndf_param_long <- rbind(\n  data.frame(K = df_param$K, theta_label = df_param$model_label,\n             beta = df_param$beta_logit, recovery = \"Logit recovery (β_L)\"),\n  data.frame(K = df_param$K, theta_label = df_param$model_label,\n             beta = df_param$beta_probit, recovery = \"Probit recovery (β_P)\")\n)\n\nggplot(df_param_long, aes(x = K, y = beta, color = theta_label)) +\n  geom_line(linewidth = 0.8) +\n  geom_point(size = 1.5) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"grey50\", linewidth = 0.5) +\n  facet_wrap(~recovery, scales = \"free_y\") +\n  labs(\n    x = \"Number of alternatives K\",\n    y = expression(\"Recovered \" * beta),\n    color = \"Threshold\"\n  ) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Recovered temperature parameters under logit (left) and probit (right) model assumptions, plotted as a function of set size K. Each line corresponds to a different accumulation threshold θ. At θ = 1 (exact logit), the recovered logit parameter is invariant to K, while the recovered probit parameter drifts. At large θ (approaching probit), the pattern reverses: the probit parameter is stable while the logit parameter varies. This directly parallels the empirical diagnostic of Robinson et al. (2023).](multinomial_simulations_files/figure-html/fig-parameter-recovery-1.png){#fig-parameter-recovery width=960}\n:::\n:::\n\n\n# Study 6: Distributional Shape — Noise Skewness and Kurtosis\n\nThe log-Gamma noise distribution transitions from highly skewed (Gumbel, $\\theta = 1$) to symmetric (Gaussian, $\\theta \\to \\infty$). This transition in distributional shape underlies all the choice-level phenomena documented above. To make this explicit, we plot the standardised noise density for several values of $\\theta$ alongside the standard normal reference.\n\n\n::: {#cell-fig-noise-densities .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-noise-densities\n#| fig-cap: \"Standardised noise densities Z^(θ) for selected values of θ. At θ = 1 (Gumbel), the distribution is right-skewed. As θ increases, the density converges to the standard normal (black dashed). The right tail — which governs the probability of 'upset' choices in multi-alternative settings — becomes thinner with increasing θ, explaining why probit concentrates more probability on the best alternative.\"\n#| fig-height: 5\n#| fig-width: 8\n\nset.seed(101)\nn_draw <- 1e6\n\nthetas_dens <- c(1, 2, 5, 10, 50)\n\ndf_dens <- data.frame()\n\nfor (th in thetas_dens) {\n  G <- rgamma(n_draw, shape = th, rate = 1)\n  eps <- -log(G)\n  Z <- (eps + digamma(th)) / sqrt(trigamma(th))  # note: +digamma because E[eps] = -digamma\n  # Use density estimation\n  d <- density(Z, from = -5, to = 5, n = 512)\n  df_dens <- rbind(df_dens, data.frame(\n    x = d$x, y = d$y, \n    model = paste0(\"θ = \", th)\n  ))\n}\n\n# Normal reference\nx_norm <- seq(-5, 5, length.out = 512)\ndf_dens <- rbind(df_dens, data.frame(\n  x = x_norm, y = dnorm(x_norm), model = \"N(0,1)\"\n))\n\ndf_dens$model <- factor(df_dens$model, \n                         levels = c(paste0(\"θ = \", thetas_dens), \"N(0,1)\"))\n\nggplot(df_dens, aes(x = x, y = y, color = model, linetype = model, linewidth = model)) +\n  geom_line() +\n  scale_linetype_manual(\n    values = c(setNames(rep(\"solid\", length(thetas_dens)), paste0(\"θ = \", thetas_dens)),\n               \"N(0,1)\" = \"dashed\")\n  ) +\n  scale_linewidth_manual(\n    values = c(setNames(rep(0.7, length(thetas_dens)), paste0(\"θ = \", thetas_dens)),\n               \"N(0,1)\" = 1.0)\n  ) +\n  labs(\n    x = expression(\"Standardised noise \" * Z^(theta)),\n    y = \"Density\",\n    color = \"Model\", linetype = \"Model\", linewidth = \"Model\"\n  ) +\n  coord_cartesian(xlim = c(-4.5, 4.5)) +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![Standardised noise densities Z^(θ) for selected values of θ. At θ = 1 (Gumbel), the distribution is right-skewed. As θ increases, the density converges to the standard normal (black dashed). The right tail — which governs the probability of 'upset' choices in multi-alternative settings — becomes thinner with increasing θ, explaining why probit concentrates more probability on the best alternative.](multinomial_simulations_files/figure-html/fig-noise-densities-1.png){#fig-noise-densities width=768}\n:::\n:::\n\n\n\n::: {#cell-fig-skew-kurtosis .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-skew-kurtosis\n#| fig-cap: \"Skewness and excess kurtosis of the standardised log-Gamma noise as a function of θ. Both moments converge to zero (the Gaussian values) as θ increases, with skewness decaying as O(θ^{-1/2}) and excess kurtosis as O(θ^{-1}). These moment trajectories fully characterise the transition from Gumbel to Gaussian noise shape.\"\n#| fig-height: 4\n#| fig-width: 10\n\n# Exact skewness and kurtosis of -log(Gamma(theta,1)), standardised\n# Skewness = psi_2(theta) / psi_1(theta)^(3/2) where psi_2 is the tetragamma\n# Excess kurtosis = psi_3(theta) / psi_1(theta)^2 where psi_3 is the pentagamma\n# Note: psigamma(x, deriv=n) gives the n-th derivative of log Gamma\n# psi_1 = trigamma, psi_2 = psigamma(,2), psi_3 = psigamma(,3)\n\nthetas_moments <- seq(1, 200, by = 1)\n\nskewness <- -psigamma(thetas_moments, deriv = 2) / trigamma(thetas_moments)^(3/2)\n# Note: -log(G) has skewness = -psi_2 / psi_1^{3/2} (the negative sign because of -log)\n# Actually: let's be careful. For X = -log(G), E[(X-mu)^3] = -psi_2(theta)\n# and Var(X) = psi_1(theta), so skewness = -psi_2(theta) / psi_1(theta)^{3/2}\n\n# Excess kurtosis: E[(X-mu)^4]/Var^2 - 3 = psi_3(theta)/psi_1(theta)^2\nex_kurtosis <- psigamma(thetas_moments, deriv = 3) / trigamma(thetas_moments)^2\n\ndf_moments <- rbind(\n  data.frame(theta = thetas_moments, value = skewness, moment = \"Skewness\"),\n  data.frame(theta = thetas_moments, value = ex_kurtosis, moment = \"Excess kurtosis\")\n)\n\nggplot(df_moments, aes(x = theta, y = value)) +\n  geom_line(linewidth = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"firebrick\") +\n  facet_wrap(~moment, scales = \"free_y\") +\n  labs(\n    x = expression(\"Threshold \" * theta),\n    y = \"Value\"\n  )\n```\n\n::: {.cell-output-display}\n![Skewness and excess kurtosis of the standardised log-Gamma noise as a function of θ. Both moments converge to zero (the Gaussian values) as θ increases, with skewness decaying as O(θ^{-1/2}) and excess kurtosis as O(θ^{-1}). These moment trajectories fully characterise the transition from Gumbel to Gaussian noise shape.](multinomial_simulations_files/figure-html/fig-skew-kurtosis-1.png){#fig-skew-kurtosis width=960}\n:::\n:::\n\n\n\n# Study 7: Robustness Across Utility Structures\n\nThe preceding studies used specific utility vectors. To assess robustness, we examine whether the convergence pattern holds across different utility configurations that are common in psychological experiments.\n\n\n::: {#cell-fig-robustness .cell}\n\n```{.r .cell-code .hidden}\n#| label: fig-robustness\n#| fig-cap: \"Total variation distance from Probit as a function of θ for K = 5 alternatives under four qualitatively different utility structures. Uniform: all utilities equal (v = 0). Dominant: one strong alternative (v₁ = 3, others 0). Linear: linearly spaced utilities. Clustered: two groups of similar alternatives. Convergence to probit is robust across configurations, though the asymptotic TV distance (reflecting Monte Carlo noise) varies with K and utility spread.\"\n#| fig-height: 5\n#| fig-width: 8\n\nset.seed(202)\nn_sim <- 2e6\nK <- 5\n\nconfigs <- list(\n  \"Uniform\" = rep(0, K),\n  \"Dominant\" = c(3, rep(0, K - 1)),\n  \"Linear\" = seq(2, 0, length.out = K),\n  \"Clustered\" = c(2.0, 1.9, 0.1, 0.0, 0.0)\n)\n\nthetas_robust <- c(1, 2, 3, 5, 10, 20, 50, 100, 200)\n\ndf_robust <- data.frame()\n\nfor (cfg_name in names(configs)) {\n  v <- configs[[cfg_name]]\n  p_probit <- mnp_probs(v, beta = 1, n_sim = 5e6)\n  \n  for (th in thetas_robust) {\n    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n    df_robust <- rbind(df_robust, data.frame(\n      theta = th,\n      tv_probit = tv_dist(p_race, p_probit),\n      config = cfg_name\n    ))\n  }\n}\n\ndf_robust$config <- factor(df_robust$config, levels = names(configs))\n\nggplot(df_robust, aes(x = theta, y = tv_probit, color = config)) +\n  geom_line(linewidth = 0.8) +\n  geom_point(size = 1.5) +\n  scale_x_log10() +\n  labs(\n    x = expression(\"Threshold \" * theta),\n    y = \"TV distance from Probit\",\n    color = \"Utility structure\"\n  ) +\n  theme(legend.position = \"right\")\n```\n\n::: {.cell-output-display}\n![Total variation distance from Probit as a function of θ for K = 5 alternatives under four qualitatively different utility structures. Uniform: all utilities equal (v = 0). Dominant: one strong alternative (v₁ = 3, others 0). Linear: linearly spaced utilities. Clustered: two groups of similar alternatives. Convergence to probit is robust across configurations, though the asymptotic TV distance (reflecting Monte Carlo noise) varies with K and utility spread.](multinomial_simulations_files/figure-html/fig-robustness-1.png){#fig-robustness width=768}\n:::\n:::\n\n\n\n# Summary\n\nThese multinomial simulations confirm and extend the binary-case results:\n\n1. **Convergence is rapid and universal**: Across different values of $K$ and different utility structures, the Poisson count race converges to the Multinomial Probit reference within $\\theta \\approx 20$–$50$ in total variation distance.\n\n2. **Probability redistribution**: As $\\theta$ increases, the probit model concentrates more probability on the best alternative and less on inferior alternatives, reflecting the thinner tails of Gaussian noise relative to Gumbel.\n\n3. **Set-size scaling**: The logit and probit models predict systematically different scaling of target choice probability with the number of competitors. The Poisson count race interpolates between these two patterns, connecting to the empirical findings of Robinson et al. (2023).\n\n4. **IIA erosion**: The Independence of Irrelevant Alternatives property, which holds exactly at $\\theta = 1$, is progressively violated as $\\theta$ increases. This provides a process-level account of why IIA holds for logit but not probit: it is a consequence of the Gumbel noise shape, and alternative noise shapes—induced by higher accumulation thresholds—do not preserve it.\n\n5. **Parameter invariance**: When choice data generated by the Poisson count race are fit under a logit assumption, the recovered inverse temperature drifts with set size $K$ for all $\\theta > 1$. Conversely, when fit under a probit assumption, the recovered noise scale remains stable for large $\\theta$ but drifts when $\\theta$ is small. This cross-over in parameter invariance provides a process-level account of the empirical findings of Robinson et al. (2023): parameter stability across $K$ is diagnostic of whether the effective noise distribution is closer to Gumbel or Gaussian.\n\n6. **Noise shape transition**: The underlying mechanism is a smooth transition in the shape of the standardised noise distribution, from the skewed Gumbel ($\\theta = 1$) to the symmetric Gaussian ($\\theta \\to \\infty$). The skewness and kurtosis decay at known rates, providing analytic control over the approximation quality.\n",
    "supporting": [
      "multinomial_simulations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}