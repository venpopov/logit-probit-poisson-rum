<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="keywords" content="discrete choice, random utility, multinomial logit, multinomial probit, Poisson process, evidence accumulation">

<title>A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-19b75d10122142dd3a7fa784dfdce8d2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice">
<meta name="citation_abstract" content="This study unifies the two canonical discrete choice specifications--Multinomial Logit and Multinomial Probit--within a single generative framework. I introduce a **Poisson Count Race**, wherein $K$ alternatives generate events according to independent Poisson processes. A choice is determined when an alternative reaches a cumulative count threshold $\theta$. At the threshold $\theta=1$, the model yields the Multinomial Logit (Luce choice rule). By normalizing the utility noise to maintain a constant variance--a process termed temperature identification--I show that as $\theta \to \infty$, the model converges to the Multinomial Probit. This formulation provides a parametric bridge in which $\theta$ governs the shape of the error distribution and a separate parameter, $\beta$, modulates the temperature, while the systematic utilities $v_i$ are shared across all regimes. Crucially, this bridge arises from a single stochastic accumulation mechanism: both models belong to the log-Gamma random utility family, with $\theta$ governing the transition between Gumbel and Gaussian noise shapes. The unification is distributional — logit and probit are members of the same parametric family — rather than dynamical, as the variance-preserving comparison across $\theta$ values compares different accumulation systems at matched discriminability.
">
<meta name="citation_keywords" content="discrete choice,random utility,multinomial logit,multinomial probit,Poisson process,evidence accumulation">
<meta name="citation_author" content="Vencislav Popov">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Individual choice behavior;,citation_abstract=This research monograph is devoted to a theoretical (mathematical) analysis of one of the major themes of interest to psychologists: choice. The analysis begins by stating a general axiom that may hold among the probabilities of choice from related sets of alternatives. This is shown to imply the existence of a ratio scale that is then used to analyze a number of traditional problems. The 1st subject treated is psychophysics, and covers areas involving time- and space-order effects, Fechner’s equal jnd problem, power law in psychophysics and its relation to discrimination data, psychophysical interaction between 2 independent physical variables and possible correlates with Stevens’ distinction between prothetic and metathetic continua, Thurston’s low of comparative judgment, signal detectability theory, and ranking of stimuli. The next major theme studied is utility theory. Unusual results are obtained which suggest an experiment to test the theory. Topics in learning are analyzed in a concluding chapter which uses the stochastic theories of learning as the basic approach, with the exception that distributions of response strengths are assumed to be transformed rather than response probabilities. 3 classes of learning operators emerge, both linear and nonlinear. (PsycINFO Database Record (c) 2016 APA, all rights reserved);,citation_author=R. Duncan Luce;,citation_publication_date=1959;,citation_cover_date=1959;,citation_year=1959;,citation_series_title=Individual choice behavior;">
<meta name="citation_reference" content="citation_title=Conditional logit analysis of qualitative choice behavior;,citation_author=Daniel McFadden;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_inbook_title=Frontiers in econometrics;,citation_series_title=Frontiers in econometrics. - New York [u.a.] : Academic Press, ISBN 0-12-776150-0. - 1974, p. 105-142;">
<meta name="citation_reference" content="citation_title=The relationship between Luce’s Choice Axiom, Thurstone’s Theory of Comparative Judgment, and the double exponential distribution;,citation_abstract=Holman and Marley have shown that Thurstone’s Case V model becomes equivalent to the Choice Axiom if its discriminal processes are assumed to be independent double exponential random variables instead of normal ones. It is shown here that for pair comparisons, this representation is not unique; other discriminal process distributions (specifiable only in terms of their characteristic functions) also yield a model equivalent to the Choice Axiom. However, none of these models is equivalent to the Choice Axiom for triple comparisons: There the double exponential representation is unique. It is also shown that within the framework of Thurstone’s theory, the double exponential distribution, and hence the Choice Axiom, is implied by a weaker assumption, called “invariance under uniform expansions of the choice set.”;,citation_author=John Yellott;,citation_publication_date=1977-04;,citation_cover_date=1977-04;,citation_year=1977;,citation_issue=2;,citation_doi=10.1016/0022-2496(77)90026-8;,citation_issn=0022-2496;,citation_volume=15;,citation_language=en-US;,citation_journal_title=Journal of Mathematical Psychology;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=A law of comparative judgment.;,citation_abstract=A new psychological law, called the law of comparative judgment, is presented with some of its special applications in the measurement of psychological values. This law is applicable not only to the comparison of physical stimulus intensities but also to qualitative comparative judgments, such as those of excellence of specimens in an educational scale. It should be possible also to verify it on comparative judgments which involve simultaneous and successive contrast. The law is stated as follows:[Equation omitted]in which S&amp;amp;amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; and S&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; are the psychological scale values of the two compared stimuli; x&amp;lt;sub&amp;gt;12&amp;lt;/sub&amp;gt; is the sigma value corresponding to the proportion of judgments p&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; &amp;gt; p&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt;. $\varsigma\&amp;$lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; is the discriminal dispersion of stimulus R&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; and $\varsigma\&amp;$lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; is the dispersion of stimulus R&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt;. r is the correlation between the discriminal deviations of R&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt; and R&amp;lt;sub&amp;gt;2&amp;lt;/sub&amp;gt; in the same judgment. This law is basic for work on Weber’s and Fechner’s laws, applies to the judgments of a single observer who compares a series of stimuli by the method of paired comparisons when no &amp;quot;equal&amp;quot; judgments are allowed, and is a rational equation for the method of constant stimuli. The law is then applied to five cases each of which involves different assumptions and different degrees of simplification of the law for practical use. The weighting of the observation equations is discussed because the observation equations obtained with the five cases are not of the same reliability and hence should not be equally weighted. (PsycINFO Database Record (c) 2016 APA, all rights reserved);,citation_author=L. L. Thurstone;,citation_publication_date=1927-07;,citation_cover_date=1927-07;,citation_year=1927;,citation_issue=4;,citation_doi=10.1037/h0070288;,citation_issn=1939-1471;,citation_volume=34;,citation_language=en-US;,citation_journal_title=Psychological Review;,citation_publisher=Psychological Review Company;">
<meta name="citation_reference" content="citation_title=Binary-Choice Constraints and Random Utility Indicators (1960);,citation_abstract=This paper contains suggestions for combining various available types of information on economic choice. There are in the main three types.;,citation_author=Jacob Marschak;,citation_editor=Jacob Marschak;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_doi=10.1007/978-94-010-9276-0_9;,citation_isbn=978-94-010-9276-0;,citation_language=en-US;,citation_inbook_title=Economic Information, Decision, and Prediction: Selected Essays: Volume I Part I Economics of Decision;">
<meta name="citation_reference" content="citation_title=A Conditional Probit Model for Qualitative Choice: Discrete Decisions Recognizing Interdependence and Heterogeneous Preferences;,citation_abstract=To date, the most widely used method for empirical analysis of multiple alternative qualitative choices has been an extension of binary logit analysis called conditional logit analysis. Although this method is extremely attractive because of its computational simplicity, it is burdened with a property termed the &amp;amp;amp;quot;independence of irrelevant alternatives&amp;quot; that is quite unrealistic in many choice situations. We have proposed in this paper a computationally feasible method of estimation not constrained by the independence restriction and which allows for a much richer range of human behavior than does the conditional logit model. An important characteristic of the model is provision for correlation among the random components of utility and, as a by-product, the explicit allowance for variation in tastes across individuals for the attributes of alternatives. We have demonstrated the model and compared it with the logit one by analyzing the travel mode choice decisions of commuters to the central business district of Washington, D.C. Substantial differences are found in predictions based on the two models. The example allows three alternatives. Extension to four or five is quite feasible.;,citation_author=Jerry A. Hausman;,citation_author=David A. Wise;,citation_publication_date=1978;,citation_cover_date=1978;,citation_year=1978;,citation_fulltext_html_url=https://www.jstor.org/stable/1913909;,citation_issue=2;,citation_doi=10.2307/1913909;,citation_issn=0012-9682;,citation_volume=46;,citation_journal_title=Econometrica;,citation_publisher=[Wiley, Econometric Society];">
<meta name="citation_reference" content="citation_title=How do people build up visual memory representations from sensory evidence? Revisiting two classic models of choice;,citation_abstract=In many decision tasks, we have a set of alternative choices and are faced with the problem of how to use our latent beliefs and preferences about each alternative to make a single choice. Cognitive and decision models typically presume that beliefs and preferences are distilled to a scalar latent strength for each alternative, but it is also critical to model how people use these latent strengths to choose a single alternative. Most models follow one of two traditions to establish this link. Modern psychophysics and memory researchers make use of signal detection theory, assuming that latent strengths are perturbed by noise, and the highest resulting signal is selected. By contrast, many modern decision theoretic modeling and machine learning approaches use the softmax function (which is based on Luce’s choice axiom; Luce, 1959) to give some weight to non-maximal-strength alternatives. Despite the prominence of these two theories of choice, current approaches rarely address the connection between them, and the choice of one or the other appears more motivated by the tradition in the relevant literature than by theoretical or empirical reasons to prefer one theory to the other. The goal of the current work is to revisit this topic by elucidating which of these two models provides a better characterization of latent processes in m-alternative decision tasks, with a particular focus on memory tasks. In a set of visual memory experiments, we show that, within the same experimental design, the softmax parameter $\beta$ varies across m-alternatives, whereas the parameter d$\prime$ of the signal-detection model is stable. Together, our findings indicate that replacing softmax with signal-detection link models would yield more generalizable predictions across changes in task structure. More ambitiously, the invariance of signal detection model parameters across different tasks suggests that the parametric assumptions of these models may be more than just a mathematical convenience, but reflect something real about human decision-making.;,citation_author=Maria M. Robinson;,citation_author=Isabella C. DeStefano;,citation_author=Edward Vul;,citation_author=Timothy F. Brady;,citation_publication_date=2023-12;,citation_cover_date=2023-12;,citation_year=2023;,citation_doi=10.1016/j.jmp.2023.102805;,citation_issn=0022-2496;,citation_volume=117;,citation_language=en-US;,citation_journal_title=Journal of Mathematical Psychology;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=A theory of memory retrieval.;,citation_abstract=Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe–memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe–memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study–test, and continuous) and to speed–accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme. (PsycINFO Database Record (c) 2016 APA, all rights reserved);,citation_author=Roger Ratcliff;,citation_publication_date=1978-03;,citation_cover_date=1978-03;,citation_year=1978;,citation_issue=2;,citation_doi=10.1037/0033-295X.85.2.59;,citation_issn=1939-1471;,citation_volume=85;,citation_language=en-US;,citation_journal_title=Psychological Review;,citation_publisher=American Psychological Association;">
<meta name="citation_reference" content="citation_title=A Ballistic Model of Choice Response Time.;,citation_abstract=Almost all models of response time (RT) use a stochastic accumulation process. To account for the benchmark RT phenomena, researchers have found it necessary to include between-trial variability in the starting point and/or the rate of accumulation, both in linear (R. Ratcliff &amp;amp;amp;amp; J. N. Rouder, 1998) and nonlinear (M. Usher &amp;amp; J. L. McClelland, 2001) models. The authors show that a ballistic (deterministic within-trial) model using a simplified version of M. Usher and J. L. McClelland’s (2001) nonlinear accumulation process with between-trial variability in accumulation rate and starting point is capable of accounting for the benchmark behavioral phenomena. The authors successfully fit their model to R. Ratcliff and J. N. Rouder’s (1998) data, which exhibit many of the benchmark phenomena. (PsycINFO Database Record (c) 2016 APA, all rights reserved);,citation_author=Scott Brown;,citation_author=Andrew Heathcote;,citation_publication_date=2005-01;,citation_cover_date=2005-01;,citation_year=2005;,citation_issue=1;,citation_doi=10.1037/0033-295X.112.1.117;,citation_issn=1939-1471;,citation_volume=112;,citation_language=en-US;,citation_journal_title=Psychological Review;,citation_publisher=American Psychological Association;">
</head>

<body class="quarto-light"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Vencislav Popov </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Department of Psychology, University of Zurich
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This study unifies the two canonical discrete choice specifications–Multinomial Logit and Multinomial Probit–within a single generative framework. I introduce a <strong>Poisson Count Race</strong>, wherein <span class="math inline">\(K\)</span> alternatives generate events according to independent Poisson processes. A choice is determined when an alternative reaches a cumulative count threshold <span class="math inline">\(\theta\)</span>. At the threshold <span class="math inline">\(\theta=1\)</span>, the model yields the Multinomial Logit (Luce choice rule). By normalizing the utility noise to maintain a constant variance–a process termed temperature identification–I show that as <span class="math inline">\(\theta \to \infty\)</span>, the model converges to the Multinomial Probit. This formulation provides a parametric bridge in which <span class="math inline">\(\theta\)</span> governs the shape of the error distribution and a separate parameter, <span class="math inline">\(\beta\)</span>, modulates the temperature, while the systematic utilities <span class="math inline">\(v_i\)</span> are shared across all regimes. Crucially, this bridge arises from a single stochastic accumulation mechanism: both models belong to the log-Gamma random utility family, with <span class="math inline">\(\theta\)</span> governing the transition between Gumbel and Gaussian noise shapes. The unification is distributional — logit and probit are members of the same parametric family — rather than dynamical, as the variance-preserving comparison across <span class="math inline">\(\theta\)</span> values compares different accumulation systems at matched discriminability.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>discrete choice, random utility, multinomial logit, multinomial probit, Poisson process, evidence accumulation</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#model-overview" id="toc-model-overview" class="nav-link" data-scroll-target="#model-overview"><span class="header-section-number">1.1</span> Model overview</a></li>
  </ul></li>
  <li><a href="#the-generative-model-a-poisson-count-race" id="toc-the-generative-model-a-poisson-count-race" class="nav-link" data-scroll-target="#the-generative-model-a-poisson-count-race"><span class="header-section-number">2</span> The Generative Model: A Poisson Count Race</a>
  <ul class="collapse">
  <li><a href="#transformation-to-waiting-times" id="toc-transformation-to-waiting-times" class="nav-link" data-scroll-target="#transformation-to-waiting-times"><span class="header-section-number">2.1</span> Transformation to Waiting Times</a></li>
  <li><a href="#the-random-utility-representation" id="toc-the-random-utility-representation" class="nav-link" data-scroll-target="#the-random-utility-representation"><span class="header-section-number">2.2</span> The Random Utility Representation</a></li>
  </ul></li>
  <li><a href="#the-logit-boundary-theta-1" id="toc-the-logit-boundary-theta-1" class="nav-link" data-scroll-target="#the-logit-boundary-theta-1"><span class="header-section-number">3</span> The Logit Boundary (<span class="math inline">\(\theta = 1\)</span>)</a></li>
  <li><a href="#temperature-identification" id="toc-temperature-identification" class="nav-link" data-scroll-target="#temperature-identification"><span class="header-section-number">4</span> Temperature Identification</a></li>
  <li><a href="#the-probit-limit-theta-to-infty" id="toc-the-probit-limit-theta-to-infty" class="nav-link" data-scroll-target="#the-probit-limit-theta-to-infty"><span class="header-section-number">5</span> The Probit Limit (<span class="math inline">\(\theta \to \infty\)</span>)</a></li>
  <li><a href="#simulation-studies-binary-choice" id="toc-simulation-studies-binary-choice" class="nav-link" data-scroll-target="#simulation-studies-binary-choice"><span class="header-section-number">6</span> Simulation Studies: Binary Choice</a></li>
  <li><a href="#simulation-studies-multinomial-k-alternative-choice" id="toc-simulation-studies-multinomial-k-alternative-choice" class="nav-link" data-scroll-target="#simulation-studies-multinomial-k-alternative-choice"><span class="header-section-number">7</span> Simulation Studies: Multinomial K-alternative Choice</a>
  <ul class="collapse">
  <li><a href="#study-1-convergence-to-probit" id="toc-study-1-convergence-to-probit" class="nav-link" data-scroll-target="#study-1-convergence-to-probit"><span class="header-section-number">7.1</span> Study 1: Convergence to Probit</a></li>
  <li><a href="#study-2-choice-probability-vectors" id="toc-study-2-choice-probability-vectors" class="nav-link" data-scroll-target="#study-2-choice-probability-vectors"><span class="header-section-number">7.2</span> Study 2: Choice Probability Vectors</a></li>
  <li><a href="#study-3-set-size-scaling" id="toc-study-3-set-size-scaling" class="nav-link" data-scroll-target="#study-3-set-size-scaling"><span class="header-section-number">7.3</span> Study 3: Set-Size Scaling</a></li>
  <li><a href="#study-4-independence-of-irrelevant-alternatives" id="toc-study-4-independence-of-irrelevant-alternatives" class="nav-link" data-scroll-target="#study-4-independence-of-irrelevant-alternatives"><span class="header-section-number">7.4</span> Study 4: Independence of Irrelevant Alternatives</a></li>
  <li><a href="#study-5-parameter-invariance-across-set-size" id="toc-study-5-parameter-invariance-across-set-size" class="nav-link" data-scroll-target="#study-5-parameter-invariance-across-set-size"><span class="header-section-number">7.5</span> Study 5: Parameter Invariance Across Set Size</a></li>
  <li><a href="#study-6-distributional-shape-noise-skewness-and-kurtosis" id="toc-study-6-distributional-shape-noise-skewness-and-kurtosis" class="nav-link" data-scroll-target="#study-6-distributional-shape-noise-skewness-and-kurtosis"><span class="header-section-number">7.6</span> Study 6: Distributional Shape — Noise Skewness and Kurtosis</a></li>
  <li><a href="#study-7-robustness-across-utility-structures" id="toc-study-7-robustness-across-utility-structures" class="nav-link" data-scroll-target="#study-7-robustness-across-utility-structures"><span class="header-section-number">7.7</span> Study 7: Robustness Across Utility Structures</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">7.8</span> Summary</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">8</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#relation-to-existing-models" id="toc-relation-to-existing-models" class="nav-link" data-scroll-target="#relation-to-existing-models"><span class="header-section-number">8.1</span> Relation to existing models</a></li>
  <li><a href="#implications-for-empirical-modeling" id="toc-implications-for-empirical-modeling" class="nav-link" data-scroll-target="#implications-for-empirical-modeling"><span class="header-section-number">8.2</span> Implications for empirical modeling</a></li>
  <li><a href="#limitations-and-extensions" id="toc-limitations-and-extensions" class="nav-link" data-scroll-target="#limitations-and-extensions"><span class="header-section-number">8.3</span> Limitations and extensions</a></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks"><span class="header-section-number">8.4</span> Concluding remarks</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="index-preview.html"><i class="bi bi-journal-code"></i>Article Notebook</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Discrete choice models based on Random Utility (RU) theory <span class="citation" data-cites="mcfaddenConditionalLogitAnalysis1974">(<a href="#ref-mcfaddenConditionalLogitAnalysis1974" role="doc-biblioref">McFadden 1974</a>)</span> form a central pillar of mathematical psychology, econometrics, and cognitive science. In these models, each alternative in a choice set elicits a <em>latent</em> scalar quantity - often interpreted as strength, utility, or evidence - and the observed choices arise from a comparison of these latent quantities under stochastic variability. The general setup is surprisingly simple. Consider a decision-maker facing a set of <span class="math inline">\(K \ge 2\)</span> mutually exclusive alternatives. The utility associated with alternative <span class="math inline">\(i\)</span> can be decomposed into a systematic component, <span class="math inline">\(v_i\)</span>, and a stochastic component, <span class="math inline">\(\epsilon_i\)</span>:</p>
<p><span class="math display">\[
U_{i} = v_{i} + \epsilon_{i}, \quad i=1, \dots, K
\]</span></p>
<p>The decision-maker then selects the alternative with maximum utility:</p>
<p><span class="math display">\[
C = \arg \max_{i \in K} U_{i}
\]</span></p>
<p>Within this general framework, different assumptions about the distribution of errors, <span class="math inline">\(\epsilon_i\)</span>, determine the structure of the choice model and its predictions. Two important classes dominate the field: logit-based models, derived from Luce’s choice axiom and extreme-value theory <span class="citation" data-cites="luceIndividualChoiceBehavior1959 yellottRelationshipLucesChoice1977 mcfaddenConditionalLogitAnalysis1974">(<a href="#ref-luceIndividualChoiceBehavior1959" role="doc-biblioref">Luce 1959</a>; <a href="#ref-yellottRelationshipLucesChoice1977" role="doc-biblioref">Yellott 1977</a>; <a href="#ref-mcfaddenConditionalLogitAnalysis1974" role="doc-biblioref">McFadden 1974</a>)</span>, and probit-based models, derived from Thurstone’s Theory of comparative judgements and gaussian Signal Detection Theory <span class="citation" data-cites="robinsonHowPeopleBuild2023 thurstoneLawComparativeJudgment1927 hausmanConditionalProbitModel1978a">(<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">Robinson et al. 2023</a>; <a href="#ref-thurstoneLawComparativeJudgment1927" role="doc-biblioref">Thurstone 1927</a>; <a href="#ref-hausmanConditionalProbitModel1978a" role="doc-biblioref">Hausman and Wise 1978</a>)</span>. These two model families correspond to two types of error distributions:</p>
<ol type="1">
<li><p><strong>Gumbel Errors:</strong> If the <span class="math inline">\(\epsilon_i\)</span> terms are independent and identically distributed (i.i.d.) according to a Type I Extreme Value distribution, the choice probabilities follow the <strong>Multinomial Logit (MNL)</strong> or Softmax form.</p></li>
<li><p><strong>Gaussian Errors:</strong> If the <span class="math inline">\(\epsilon_i\)</span> terms follow a Multivariate Normal distribution (potentially allowing for correlated errors across alternatives) the choice probabilities are described by the <strong>Multinomial Probit (MNP)</strong> model.</p></li>
</ol>
<p>The random utility framework thus unifies axiomatic choice models <span class="citation" data-cites="luceIndividualChoiceBehavior1959">(<a href="#ref-luceIndividualChoiceBehavior1959" role="doc-biblioref">Luce 1959</a>)</span> and measurement detection-based models <span class="citation" data-cites="thurstoneLawComparativeJudgment1927">(<a href="#ref-thurstoneLawComparativeJudgment1927" role="doc-biblioref">Thurstone 1927</a>)</span> under a single functional form. However, the unification currently stops there - the different error distributions reflect different assumptions about the generating process. As <span class="citation" data-cites="robinsonHowPeopleBuild2023">Robinson et al. (<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">2023</a>)</span> recently put it, the two models “describe different ways of translating sensory evidence into decision variables.”.</p>
<p>The central question addressed in this paper is this: Can the two canonical random-utility discrete-choice specifications — multinomial logit and multinomial probit — be derived from a shared generative mechanism? Can we find a deeper unifying principle? The surprising answer is yes - both models can be derived as limit cases of a single stochastic evidence accumulation mechanism. The key distinction between the models is not the form of the utility noise itself, but the stopping rule governing how stochastic evidence is accumulated before a choice is made.”</p>
<section id="model-overview" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="model-overview"><span class="header-section-number">1.1</span> Model overview</h2>
<p>I build on a <strong>Poisson count race</strong> (e.g., Pike, 1973; Townsend &amp; Ashby, 1983; Smith &amp; Van Zandt, 2000), wherein each alternative generates stochastic events via an independent Poisson process. A decision is made when one alternative reaches a cumulative count threshold <span class="math inline">\(\theta\)</span>. The threshold <span class="math inline">\(\theta\)</span> thus controls how much stochastic evidence must be accumulated before commitment.</p>
<p>To compare noise <em>shape</em> across <span class="math inline">\(\theta\)</span> independently of noise <em>scale</em>, I introduce a <strong>temperature identification</strong> that standardizes the stochastic utility component to unit variance for all <span class="math inline">\(\theta\)</span>, with a separate parameter <span class="math inline">\(\beta\)</span> controlling the overall magnitude of stochasticity. With this normalization, we can establish three main results:</p>
<ol type="1">
<li>At <span class="math inline">\(\theta = 1\)</span>, this reduces exactly to the Multinomial Logit</li>
<li>For any <span class="math inline">\(\theta \ge 1\)</span>, the Poisson count race is isomorphic to a random utility model with log-Gamma noise, interpolating between Gumbel and Gaussian error distributions;</li>
<li>As <span class="math inline">\(\theta \to \infty\)</span> under temperature identification, the model converges to the Multinomial Probit.</li>
</ol>
<p>From this perspective, logit and probit can be understood as members of a single parametric family of evidence accumulation models that differ in the accumulation stopping rule. Extreme-value noise and Gaussian noise arise as the two endpoint regimes of log-Gamma noise, which is the natural error distribution of Poisson count races. It is important to note, however, that this unification is algebraic and distributional rather than dynamical: the temperature identification that enables comparison across <span class="math inline">\(\theta\)</span> values compares different accumulation systems at matched discriminability, not a single system under threshold variation (see Section 5 for details).</p>
<p>While Poisson counter models have a rich history in mathematical psychology—particularly as accounts of response time distributions under time-varying evidence rates (e.g., Pike, 1973; Townsend &amp; Ashby, 1983; Smith &amp; Van Zandt, 2000) - they have rarely been invoked to address the theoretical relationship between <em>static</em> discrete choice models. Smith and Van Zandt (2000) showed that Poisson races yield choice probabilities governed by the incomplete Beta function, but their analysis focused on relatively small integer thresholds <span class="math inline">\(\theta \approx 5\text{–}10\)</span> chosen to capture reaction-time skewness.</p>
<p>The present work takes a different perspective. Rather than treating the threshold as a fixed descriptive parameter, we examine the asymptotic behavior of the Poisson count race as <span class="math inline">\(\theta \to \infty\)</span> under variance-preserving (temperature) identification. This shift in emphasis reveals that the Poisson race is not merely a model of latency, but a generative mechanism that continuously interpolates between the two canonical pillars of discrete choice: Multinomial Logit <span class="math inline">\(\theta = 1\)</span> and Multinomial Probit <span class="math inline">\(\theta \to \infty\)</span>.</p>
<p>The remainder of the paper formalizes this framework, presents simulations illustrating the interpolation between regimes, and discusses implications for discrete choice modeling.</p>
</section>
</section>
<section id="the-generative-model-a-poisson-count-race" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Generative Model: A Poisson Count Race</h1>
<p>Let the accumulation of evidence or preference for each alternative <span class="math inline">\(i\)</span> be modeled by independent Poisson processes, denoted <span class="math inline">\(N_i(t)\)</span>, with rate parameters <span class="math inline">\(\lambda_i &gt; 0\)</span>. Here <span class="math inline">\(N_i(t)\)</span> represents cumulative count for alternative <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>.</p>
<p>Then, define a <strong>Count Race</strong> characterized by an integer threshold <span class="math inline">\(\theta \ge 1\)</span>. The process terminates as soon as any single alternative accumulates <span class="math inline">\(\theta\)</span> events.</p>
<p><strong>Definition 1 (Stopping Time).</strong> The stopping time for the system is the first time any process hits the threshold:</p>
<p><span class="math display">\[
\tau_{\theta} = \inf \{t \ge 0 : \max_{i} N_i(t) = \theta \}
\]</span></p>
<p><strong>Definition 2 (Choice).</strong> The chosen alternative is the specific process that triggers the stopping time:</p>
<p><span class="math display">\[
C = \arg \max_{i} N_i(\tau_{\theta})
\]</span></p>
<section id="transformation-to-waiting-times" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="transformation-to-waiting-times"><span class="header-section-number">2.1</span> Transformation to Waiting Times</h2>
<p>To map this stochastic process to a random utility framework, consider <span class="math inline">\(T_i^{(\theta)}\)</span>, the waiting time until the <span class="math inline">\(i\)</span>-th process records its <span class="math inline">\(\theta\)</span>-th event:</p>
<p><span class="math display">\[
T_i^{(\theta)} := \inf \{t : N_i(t) = \theta \}
\]</span></p>
<p>For a Poisson process with rate <span class="math inline">\(\lambda_i\)</span>, the waiting time to the <span class="math inline">\(\theta\)</span>-th jump follows a Gamma (Erlang) distribution with shape <span class="math inline">\(\theta\)</span> and rate <span class="math inline">\(\lambda_i\)</span>:</p>
<p><span class="math display">\[
T_i^{(\theta)} \sim \text{Gamma}(\text{shape}=\theta, \text{rate}=\lambda_i)
\]</span></p>
<p>The condition that alternative <span class="math inline">\(i\)</span> wins the race is equivalent to observing the minimum waiting time:</p>
<p><span class="math display">\[
C = \arg \min_{i} T_i^{(\theta)}
\]</span></p>
</section>
<section id="the-random-utility-representation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="the-random-utility-representation"><span class="header-section-number">2.2</span> The Random Utility Representation</h2>
<p>Since the Poisson processes are independent, the waiting times <span class="math inline">\(T_1^{(\theta)}, \ldots, T_K^{(\theta)}\)</span> are mutually independent. Utilizing the scaling property of the Gamma distribution, we can express each waiting time as:</p>
<p><span class="math display">\[
T_i^{(\theta)} \stackrel{d}{=} \frac{G_i}{\lambda_i}
\]</span></p>
<p>where <span class="math inline">\(G_1, \ldots, G_K \stackrel{\text{i.i.d.}}{\sim} \text{Gamma}(\theta, 1)\)</span> are standard Gamma random variables. The choice problem then becomes:</p>
<p><span class="math display">\[
C = \arg \min_{i} \left( \frac{G_i}{\lambda_i} \right)
\]</span></p>
<p>Applying the natural logarithm, a monotonic transformation, reverses the optimization direction from minimization to maximization:</p>
<p><span class="math display">\[
\begin{aligned} C &amp;= \arg \min_{i} (\log G_i - \log \lambda_i) \\ &amp;= \arg \max_{i} (\log \lambda_i - \log G_i) \end{aligned}
\]</span></p>
<p>This establishes an exact Random Utility Model (RUM) structure:</p>
<p><span class="math display">\[
U_i^{(\theta)} = v_i + \epsilon_i^{(\theta)}
\]</span></p>
<p>where:</p>
<ul>
<li><strong>Systematic Utility:</strong> <span class="math inline">\(v_i = \log \lambda_i\)</span><br>
</li>
<li><strong>Stochastic Error:</strong> <span class="math inline">\(\epsilon_i^{(\theta)} = -\log G_i\)</span>, with <span class="math inline">\(G_i \sim \text{Gamma}(\theta, 1)\)</span>.</li>
</ul>
<p>Thus, the Poisson count race is isomorphic to a Random Utility Model characterized by Log-Gamma noise.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Result 1 (Log-Gamma Random Utility Family)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For any integer threshold <span class="math inline">\(\theta \ge 1\)</span>, the Poisson count race induces a random utility model <span class="math inline">\(U_i^{(\theta)} = \log \lambda_i - \log G_i\)</span> with i.i.d. log-Gamma(<span class="math inline">\(\theta\)</span>) noise. This family interpolates between Gumbel noise (<span class="math inline">\(\theta = 1\)</span>) and, under temperature identification, Gaussian noise (<span class="math inline">\(\theta \to \infty\)</span>).</p>
</div>
</div>
</section>
</section>
<section id="the-logit-boundary-theta-1" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> The Logit Boundary (<span class="math inline">\(\theta = 1\)</span>)</h1>
<p>In the specific instance where the threshold is a single event (<span class="math inline">\(\theta = 1\)</span>), the waiting time distribution simplifies to the Exponential distribution:</p>
<p><span class="math display">\[
G_i \sim \text{Gamma}(1, 1) \equiv \text{Exponential}(1)
\]</span></p>
<p>A fundamental property of Extreme Value Theory is the relationship between the Exponential and Gumbel distributions:</p>
<p><span class="math display">\[
X \sim \text{Exp}(1) \implies -\log(X) \sim \text{Gumbel}(\text{Type I EV})
\]</span></p>
<p>Therefore, when <span class="math inline">\(\theta=1\)</span>, the noise terms <span class="math inline">\(\epsilon_i^{(1)}\)</span> are i.i.d. standard Gumbel. This recovers the exact Multinomial Logit formula:</p>
<p><span class="math display">\[
\Pr(C=i) = \frac{\exp(v_i)}{\sum_{j=1}^K \exp(v_j)} = \frac{\lambda_i}{\sum_{j=1}^K \lambda_j}
\]</span></p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Result 2 (Logit Boundary)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The Poisson count race with threshold <span class="math inline">\(\theta=1\)</span> recovers the Multinomial Logit model exactly. The accumulation of a single unit of evidence corresponds to the Luce Choice Rule (Softmax). This is a classic, well-known derivation.</p>
</div>
</div>
</section>
<section id="temperature-identification" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Temperature Identification</h1>
<p>For thresholds <span class="math inline">\(\theta &gt; 1\)</span>, the error distribution deviates from the Gumbel form. More critically, as <span class="math inline">\(\theta\)</span> increases, the variance of the error term diminishes. Specifically, for <span class="math inline">\(\epsilon^{(\theta)} = -\log G\)</span> where <span class="math inline">\(G \sim \text{Gamma}(\theta, 1)\)</span>, the moments are:</p>
<p><span class="math display">\[
\begin{aligned} \mathbb{E}[\epsilon^{(\theta)}] &amp;= -\psi(\theta) \\ \text{Var}(\epsilon^{(\theta)}) &amp;= \psi_1(\theta) \end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\psi(\cdot)\)</span> is the digamma function and <span class="math inline">\(\psi_1(\cdot)\)</span> is the trigamma function.</p>
<p>As <span class="math inline">\(\theta \to \infty\)</span>, the variance <span class="math inline">\(\psi_1(\theta) \approx 1/\theta \to 0\)</span>. Without intervention, the model would converge to a deterministic choice rule (argmax of systematic utilities) simply because the noise vanishes. To facilitate a meaningful comparison of error shapes across varying <span class="math inline">\(\theta\)</span>, we must enforce a consistent scale.</p>
<p>We term this process Temperature Identification and define the standardized noise term <span class="math inline">\(Z_i^{(\theta)}\)</span> to have zero mean and unit variance for all <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
Z_i^{(\theta)} := \frac{\epsilon_i^{(\theta)} - \mu_\theta}{\sigma_\theta} = \frac{-\log G_i + \psi(\theta)}{\sqrt{\psi_1(\theta)}}
\]</span></p>
<p>This leads to a <strong>Temperature-Identified Family</strong> of utility models:</p>
<p><span class="math display">\[
U_i^{(\theta)} = v_i + \beta Z_i^{(\theta)}
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(v_i\)</span> is the systematic utility (evidence rate).</li>
<li><span class="math inline">\(\theta\)</span> governs the shape of the noise (from skewed Gumbel to symmetric Gaussian).</li>
<li><span class="math inline">\(\beta\)</span> governs the temperature (the magnitude of noise relative to utility).</li>
</ul>
<p>An important caveat accompanies this construction. In a random utility model, rescaling all utilities by a common positive constant does not change choice probabilities. Therefore, the temperature-identified model <span class="math inline">\(U_i^{(\theta)} = v_i + \beta Z_i^{(\theta)}\)</span> is observationally equivalent to a model with systematic utilities <span class="math inline">\(v_i / \sigma_\theta\)</span> and <em>unstandardized</em> log-Gamma noise <span class="math inline">\(\epsilon_i^{(\theta)}\)</span>, where <span class="math inline">\(\sigma_\theta = \sqrt{\psi_1(\theta)}\)</span>. This means that the temperature identification does not describe the behavior of a single accumulation system under threshold variation. For a fixed set of Poisson rates <span class="math inline">\(\lambda_i\)</span>, increasing <span class="math inline">\(\theta\)</span> both reshapes the noise and reduces its variance; the variance reduction alone drives choice toward determinism regardless of shape. The temperature identification removes this confound by comparing across systems with different effective rate structures — specifically, the effective rates would need to scale as <span class="math inline">\(\lambda_i^{1/\sigma_\theta}\)</span> to maintain constant discriminability. The unification established here is therefore <em>distributional</em> — logit and probit belong to the same parametric family of log-Gamma random utility models — rather than <em>dynamical</em>. One cannot convert a logit-like decision process into a probit-like one merely by raising the decision threshold within a single system.</p>
</section>
<section id="the-probit-limit-theta-to-infty" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> The Probit Limit (<span class="math inline">\(\theta \to \infty\)</span>)</h1>
<p>This section establishes the asymptotic behavior of the temperature-identified model.</p>
<p>Recall that <span class="math inline">\(G_i \sim \text{Gamma}(\theta, 1)\)</span>. For integer <span class="math inline">\(\theta\)</span>, <span class="math inline">\(G_i\)</span> can be represented as the sum of <span class="math inline">\(\theta\)</span> independent exponential variables: <span class="math inline">\(G_i = \sum_{j=1}^{\theta} E_{ij}\)</span>. By the Central Limit Theorem, the standardized variable converges to a standard normal distribution:</p>
<p><span class="math display">\[
\frac{G_i - \theta}{\sqrt{\theta}} \xrightarrow{d} \mathcal{N}(0, 1)
\]</span></p>
<p>We are interested in the distribution of the log-transformed variable, <span class="math inline">\(\epsilon_i^{(\theta)} = -\log G_i\)</span>. By applying the Delta Method with the transformation <span class="math inline">\(g(x) = -\log x\)</span>, the asymptotic distribution of <span class="math inline">\(-\log G_i\)</span> is normal with variance <span class="math inline">\([g'(\theta)]^2 \cdot \text{Var}(G_i) = (-1/\theta)^2 \cdot \theta = 1/\theta\)</span>.</p>
<p>Standardizing this result matches our temperature identification scaling. Since <span class="math inline">\(\psi_1(\theta) \sim 1/\theta\)</span> for large <span class="math inline">\(\theta\)</span>, the standardized term converges to the standard normal:</p>
<p><span class="math display">\[
Z_i^{(\theta)} \xrightarrow{d} \mathcal{N}(0, 1)
\]</span></p>
<p>Consequently, the vector of random utilities converges in distribution:</p>
<p><span class="math display">\[
(v_i + \beta Z_i^{(\theta)})_{i=1}^K \xrightarrow{d} (v_i + \beta Z_i)_{i=1}^K
\]</span></p>
<p>where <span class="math inline">\(Z_i \stackrel{i.i.d.}{\sim} \mathcal{N}(0, 1)\)</span>.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Result 3 (Probit Limit)
</div>
</div>
<div class="callout-body-container callout-body">
<p>As <span class="math inline">\(\theta \to \infty\)</span>, the Temperature-Identified Poisson race converges to the Independent Multinomial Probit model. Within the temperature-identified family, Logit corresponds to the <span class="math inline">\(\theta=1\)</span> member and Probit to the <span class="math inline">\(\theta \to \infty\)</span> limit. This characterizes the two models as occupying different positions within a single parametric family of log-Gamma random utility models, indexed by the accumulation threshold.</p>
</div>
</div>
</section>
<section id="simulation-studies-binary-choice" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Simulation Studies: Binary Choice</h1>
<p>To illustrate how the Poisson count race family interpolates between Logit and Probit, I conducted a simulation study in the binary choice setting (<span class="math inline">\(K=2\)</span>). This setting admits closed-form choice probabilities and allows direct visual comparison with both classical models.</p>
<p>Let <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> denote the Poisson rates of the two alternatives, and define the log-rate ratio <span class="math inline">\(x = \log(\lambda_1/\lambda_2)\)</span>. The probability that alternative 1 wins the race can be expressed in closed form using the regularized incomplete Beta function:</p>
<p><span class="math display">\[
\Pr(C = 1 \mid x, \theta) = I_{\sigma(x)}(\theta, \theta)
\]</span></p>
<p>where <span class="math inline">\(I_p(a, b)\)</span> is the regularized incomplete Beta function and <span class="math inline">\(\sigma(x) = (1 + e^{-x})^{-1}\)</span>. To see this, note that alternative 1 wins the race if and only if <span class="math inline">\(T_1^{(\theta)} &lt; T_2^{(\theta)}\)</span>, where <span class="math inline">\(T_i^{(\theta)} \sim \text{Gamma}(\theta, \lambda_i)\)</span>. Equivalently, defining <span class="math inline">\(W = T_1^{(\theta)} / (T_1^{(\theta)} + T_2^{(\theta)})\)</span>, we have <span class="math inline">\(W \sim \text{Beta}(\theta, \theta)\)</span> when evaluated at <span class="math inline">\(p = \lambda_1 / (\lambda_1 + \lambda_2) = \sigma(x)\)</span>, giving <span class="math inline">\(\Pr(T_1 &lt; T_2) = I_{\sigma(x)}(\theta, \theta)\)</span>. For <span class="math inline">\(\theta = 1\)</span>, this reduces exactly to <span class="math inline">\(\sigma(x)\)</span>, the logistic function.</p>
<p>When choice probabilities are plotted directly as a function of <span class="math inline">\(x\)</span> for increasing <span class="math inline">\(\theta\)</span>, the choice function becomes increasingly steep and converges to a step function at <span class="math inline">\(x = 0\)</span>, reflecting deterministic selection of the alternative with the larger rate. This confirms that without temperature identification, increasing the count threshold simply reduces stochasticity rather than inducing Gaussian behavior.</p>
<p>To compare noise <em>shape</em> independently of noise <em>scale</em>, I adopted the temperature identification introduced in Section 5. For binary choice, the variance of the utility noise difference is <span class="math inline">\(\text{Var}(\epsilon_1^{(\theta)} - \epsilon_2^{(\theta)}) = 2\psi_1(\theta)\)</span>. I therefore define the standardized signal <span class="math inline">\(s = x / \sqrt{2\psi_1(\theta)}\)</span>. On this variance-matched axis, the Logit reference (<span class="math inline">\(\theta = 1\)</span>) uses <span class="math inline">\(\text{sd}_{\text{diff}} = \pi/\sqrt{3}\)</span> (the standard deviation of the difference of two independent Gumbel variates), and the Probit reference is simply <span class="math inline">\(\Phi(s)\)</span>.</p>
<p>For the multinomial simulations, the same principle applies. The logit reference uses an effective inverse temperature of <span class="math inline">\(\pi / (\beta\sqrt{6})\)</span>, which ensures the Gumbel noise has standard deviation matching <span class="math inline">\(\beta\)</span> under the unit-variance convention. The probit reference uses noise scale <span class="math inline">\(\beta\)</span> directly. All simulations use <span class="math inline">\(\beta = 1\)</span> unless otherwise noted. Under this normalization, the <span class="math inline">\(\theta = 1\)</span> Poisson race coincides exactly with the variance-matched logit curve, while increasing <span class="math inline">\(\theta\)</span> yields choice functions that converge uniformly to the probit curve.</p>
<p>Because logit and probit are themselves numerically close under variance matching, the differences between models are small in absolute magnitude but systematic. To make these differences visible, <a href="#fig-residuals" class="quarto-xref">Figure&nbsp;1</a> plots residuals relative to the variance-matched Logit model. At <span class="math inline">\(\theta = 1\)</span>, the residual is identically zero (exact Logit). As <span class="math inline">\(\theta\)</span> increases, the residuals grow smoothly and converge toward the Probit<span class="math inline">\(-\)</span>Logit difference curve, with the maximum absolute deviation from probit decaying rapidly in <span class="math inline">\(\theta\)</span>. This confirms that the temperature-identified Poisson count race defines a continuous, parameterized family of choice rules that interpolates smoothly between logit-like and probit-like behavior.</p>
<div id="cell-fig-residuals" class="cell">
<div class="cell-output-display">
<div id="fig-residuals" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-residuals-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Residuals of Poisson count race choice probabilities relative to the Logit reference, plotted on a variance-matched axis. Each curve corresponds to a different accumulation threshold θ. At θ = 1 the model is exactly Logit (zero residual). As θ increases, the curves converge toward the Probit − Logit difference (black curve), confirming the theoretical bridge between the two models."><img src="index_files/figure-html/fig-residuals-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Residuals of Poisson count race choice probabilities relative to the Logit reference, plotted on a variance-matched axis. Each curve corresponds to a different accumulation threshold θ. At θ = 1 the model is exactly Logit (zero residual). As θ increases, the curves converge toward the Probit − Logit difference (black curve), confirming the theoretical bridge between the two models.
</figcaption>
</figure>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-1" href="index-preview.html#cell-fig-residuals">Source: Article Notebook</a></div>
</section>
<section id="simulation-studies-multinomial-k-alternative-choice" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Simulation Studies: Multinomial K-alternative Choice</h1>
<p>The binary simulation demonstrates that the temperature-identified Poisson count race interpolates smoothly between Logit (<span class="math inline">\(\theta = 1\)</span>) and Probit (<span class="math inline">\(\theta \to \infty\)</span>) in the two-alternative case. Here I extend this analysis to the multinomial setting (<span class="math inline">\(K &gt; 2\)</span>), where the differences between logit and probit become richer and more consequential.</p>
<p>In the binary case, logit and probit choice functions differ only in the shape of the psychometric curve—a subtle quantitative distinction. With three or more alternatives, additional qualitative differences emerge. Most prominently, the Multinomial Logit model satisfies the <em>Independence of Irrelevant Alternatives</em> (IIA) property: the ratio of choice probabilities for any two alternatives is independent of the remaining alternatives in the choice set <span class="citation" data-cites="luceIndividualChoiceBehavior1959">(<a href="#ref-luceIndividualChoiceBehavior1959" role="doc-biblioref">Luce 1959</a>)</span>. The Multinomial Probit model, even with independent errors, does not share this property <span class="citation" data-cites="hausmanConditionalProbitModel1978a">(<a href="#ref-hausmanConditionalProbitModel1978a" role="doc-biblioref">Hausman and Wise 1978</a>)</span>. The Poisson count race therefore provides a window into how IIA-like behavior gradually weakens as the noise distribution transitions from Gumbel to Gaussian.</p>
<p>The multinomial simulations are organised around five questions:</p>
<ol type="1">
<li><strong>Convergence</strong>: How quickly do Poisson count race choice probabilities converge to the Probit reference as <span class="math inline">\(\theta\)</span> increases, and does the rate of convergence depend on <span class="math inline">\(K\)</span>?</li>
<li><strong>Probability vectors</strong>: How does the full distribution over alternatives change as <span class="math inline">\(\theta\)</span> varies from 1 to large values?</li>
<li><strong>Set-size scaling</strong>: How does the probability of choosing a target alternative scale with the number of competitors, and how does this scaling differ between logit, probit, and intermediate regimes?</li>
<li><strong>Independence of Irrelevant Alternatives</strong>: How does the IIA property—exact under logit—erode as <span class="math inline">\(\theta\)</span> increases toward the probit regime?</li>
<li><strong>Parameter invariance</strong>: When misspecified logit or probit models are fit to Poisson count race data, which model yields parameters that are invariant to <span class="math inline">\(K\)</span>?</li>
</ol>
<p>All simulations use Monte Carlo sampling with <span class="math inline">\(10^6\)</span> to <span class="math inline">\(10^7\)</span> replications per condition unless otherwise noted.</p>
<section id="study-1-convergence-to-probit" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="study-1-convergence-to-probit"><span class="header-section-number">7.1</span> Study 1: Convergence to Probit</h2>
<p>I first examine how the total variation (TV) distance between the Poisson count race choice probabilities and the Logit / Probit references changes as a function of <span class="math inline">\(\theta\)</span>, for different numbers of alternatives <span class="math inline">\(K\)</span>.</p>
<p>For each <span class="math inline">\(K\)</span>, I use linearly spaced utilities <span class="math inline">\(v_i = (K - i)/(K - 1)\)</span> for <span class="math inline">\(i = 1, \ldots, K\)</span>, ensuring that the best and worst alternatives always have utilities 1 and 0 regardless of <span class="math inline">\(K\)</span>. The temperature is fixed at <span class="math inline">\(\beta = 1\)</span>.</p>
<div id="cell-fig-tv-distance" class="cell">
<div class="cell-output-display">
<div id="fig-tv-distance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tv-distance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-tv-distance-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Total variation distance between Poisson count race choice probabilities and the Logit (blue) and Probit (red) references, as a function of threshold θ, for different numbers of alternatives K."><img src="index_files/figure-html/fig-tv-distance-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tv-distance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Total variation distance between Poisson count race choice probabilities and the Logit (blue) and Probit (red) references, as a function of threshold θ, for different numbers of alternatives K.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-2-choice-probability-vectors" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="study-2-choice-probability-vectors"><span class="header-section-number">7.2</span> Study 2: Choice Probability Vectors</h2>
<p>To visualise how the full distribution over alternatives evolves with <span class="math inline">\(\theta\)</span>, I fix <span class="math inline">\(K = 5\)</span> with utilities <span class="math inline">\(v = (2.0,\; 1.5,\; 1.0,\; 0.5,\; 0.0)\)</span> and plot the choice probability for each alternative across a range of thresholds.</p>
<div id="cell-fig-prob-vectors" class="cell">
<div class="cell-output-display">
<div id="fig-prob-vectors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prob-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-prob-vectors-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Choice probabilities for each of five alternatives as a function of threshold θ. Dashed lines: Logit reference; dotted lines: Probit reference."><img src="index_files/figure-html/fig-prob-vectors-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prob-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Choice probabilities for each of five alternatives as a function of threshold θ. Dashed lines: Logit reference; dotted lines: Probit reference.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-3-set-size-scaling" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="study-3-set-size-scaling"><span class="header-section-number">7.3</span> Study 3: Set-Size Scaling</h2>
<p>A critical diagnostic for discriminating between logit and probit models is the effect of adding alternatives to the choice set <span class="citation" data-cites="robinsonHowPeopleBuild2023">(<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">Robinson et al. 2023</a>)</span>. Under MNL, the probability of choosing a target alternative with fixed utility is strictly determined by the ratio of its strength to the total strength. Under MNP, the scaling with set size differs because the probability of “winning” the maximum comparison depends on the shape of the noise distribution.</p>
<p>I fix a target alternative with utility <span class="math inline">\(v_\text{target} = 1\)</span> and add <span class="math inline">\(K - 1\)</span> equal competitors, each with utility <span class="math inline">\(v_\text{comp} = 0\)</span>. As <span class="math inline">\(K\)</span> grows, I track the probability of choosing the target.</p>
<p>Under MNL, the choice probability is <span class="math inline">\(P(\text{target}) = e^a / (e^a + K - 1)\)</span>, where <span class="math inline">\(a = v_t \cdot \pi / (\beta \sqrt{6})\)</span> is the effective scaled utility.</p>
<p>Under MNP: <span class="math inline">\(P(\text{target}) = \int \phi(z) \,\Phi(v_t/\beta + z)^{K-1}\, dz\)</span> (by symmetry of the <span class="math inline">\(K - 1\)</span> equal competitors). This integral reveals that probit’s thinner tails give the target a larger advantage over many competitors than logit’s heavier tails.</p>
<div id="cell-fig-set-size" class="cell">
<div class="cell-output-display">
<div id="fig-set-size" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-set-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-set-size-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Probability of choosing a target alternative (v = 1) against K − 1 equal competitors (v = 0) as a function of set size K. MNL (dashed blue) and MNP (dashed red) references are shown."><img src="index_files/figure-html/fig-set-size-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-set-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Probability of choosing a target alternative (v = 1) against K − 1 equal competitors (v = 0) as a function of set size K. MNL (dashed blue) and MNP (dashed red) references are shown.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-4-independence-of-irrelevant-alternatives" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="study-4-independence-of-irrelevant-alternatives"><span class="header-section-number">7.4</span> Study 4: Independence of Irrelevant Alternatives</h2>
<p>The IIA property is a hallmark of the Multinomial Logit model: the ratio of choice probabilities for any two alternatives is invariant to the composition of the choice set. Formally, for alternatives <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[\frac{P(i \mid \mathcal{C})}{P(j \mid \mathcal{C})} = \frac{e^{v_i}}{e^{v_j}} \quad \text{for all choice sets } \mathcal{C} \ni i, j\]</span></p>
<p>This property does not hold for the Multinomial Probit model, even when errors are independent and identically distributed. The Poisson count race therefore provides a mechanism through which IIA holds exactly at <span class="math inline">\(\theta = 1\)</span> and is progressively violated as <span class="math inline">\(\theta\)</span> increases.</p>
<p>To quantify this, I consider three alternatives with utilities <span class="math inline">\(v = (2, 1, 0)\)</span>. I compute the ratio <span class="math inline">\(P(1)/P(2)\)</span> under two conditions:</p>
<ul>
<li><strong>Full set</strong>: all three alternatives present <span class="math inline">\(\{1, 2, 3\}\)</span></li>
<li><strong>Reduced set</strong>: only alternatives <span class="math inline">\(\{1, 2\}\)</span> present</li>
</ul>
<p>Under IIA, these ratios should be identical. I track the percentage change in the ratio as <span class="math inline">\(\theta\)</span> varies.</p>
<div id="cell-fig-iia" class="cell">
<div class="cell-output-display">
<div id="fig-iia" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-iia-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Test of IIA. Left: the ratio P(Alt 1)/P(Alt 2) in the full set {1, 2, 3} (solid) and reduced set {1, 2} (dashed). Right: percentage change in the ratio when alternative 3 is removed."><img src="index_files/figure-html/fig-iia-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Test of IIA. Left: the ratio P(Alt 1)/P(Alt 2) in the full set {1, 2, 3} (solid) and reduced set {1, 2} (dashed). Right: percentage change in the ratio when alternative 3 is removed.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-5-parameter-invariance-across-set-size" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="study-5-parameter-invariance-across-set-size"><span class="header-section-number">7.5</span> Study 5: Parameter Invariance Across Set Size</h2>
<p>A key empirical diagnostic for distinguishing between logit and probit is parameter invariance across changes in set size <span class="math inline">\(K\)</span> <span class="citation" data-cites="robinsonHowPeopleBuild2023">(<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">Robinson et al. 2023</a>)</span>. If choice data are generated by a logit model, the softmax inverse temperature <span class="math inline">\(\beta_{\text{logit}}\)</span> recovered from fitting a logit specification should remain constant as <span class="math inline">\(K\)</span> increases. Conversely, if the data follow a probit model, the Gaussian noise scale <span class="math inline">\(\beta_{\text{probit}}\)</span> should be invariant to <span class="math inline">\(K\)</span>.</p>
<p>I test this directly. For each value of <span class="math inline">\(\theta\)</span> and each set size <span class="math inline">\(K\)</span> (with a target at <span class="math inline">\(v = 1\)</span> vs.&nbsp;<span class="math inline">\(K-1\)</span> equal competitors at <span class="math inline">\(v = 0\)</span>), I compute the “true” choice probability <span class="math inline">\(P(\text{target})\)</span> from the race model and then recover the best-fitting logit and probit temperature parameters by inversion.</p>
<div id="cell-fig-parameter-recovery" class="cell">
<div class="cell-output-display">
<div id="fig-parameter-recovery" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parameter-recovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-parameter-recovery-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Recovered temperature parameters under logit (left) and probit (right) model assumptions as a function of set size K and accumulation threshold θ."><img src="index_files/figure-html/fig-parameter-recovery-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parameter-recovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Recovered temperature parameters under logit (left) and probit (right) model assumptions as a function of set size K and accumulation threshold θ.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-6-distributional-shape-noise-skewness-and-kurtosis" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="study-6-distributional-shape-noise-skewness-and-kurtosis"><span class="header-section-number">7.6</span> Study 6: Distributional Shape — Noise Skewness and Kurtosis</h2>
<p>The log-Gamma noise distribution transitions from highly skewed (Gumbel, <span class="math inline">\(\theta = 1\)</span>) to symmetric (Gaussian, <span class="math inline">\(\theta \to \infty\)</span>). This transition in distributional shape underlies all the choice-level phenomena documented above. To make this explicit, I plot the standardised noise density for several values of <span class="math inline">\(\theta\)</span> alongside the standard normal reference.</p>
<div id="cell-fig-noise-densities" class="cell">
<div class="cell-output-display">
<div id="fig-noise-densities" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noise-densities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-noise-densities-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Standardised noise densities for selected values of θ. As θ increases, the density converges to the standard normal (black dashed)."><img src="index_files/figure-html/fig-noise-densities-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noise-densities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Standardised noise densities for selected values of θ. As θ increases, the density converges to the standard normal (black dashed).
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-skew-kurtosis" class="cell">
<div class="cell-output-display">
<div id="fig-skew-kurtosis" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-skew-kurtosis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-skew-kurtosis-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Skewness and excess kurtosis of the standardised log-Gamma noise as a function of θ. Both moments converge to zero (the Gaussian values) as θ increases, with skewness decaying as O(θ^{-1/2}) and excess kurtosis as O(θ^{-1}). These moment trajectories fully characterise the transition from Gumbel to Gaussian noise shape."><img src="index_files/figure-html/fig-skew-kurtosis-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-skew-kurtosis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Skewness and excess kurtosis of the standardised log-Gamma noise as a function of θ. Both moments converge to zero (the Gaussian values) as θ increases, with skewness decaying as O(θ^{-1/2}) and excess kurtosis as O(θ^{-1}). These moment trajectories fully characterise the transition from Gumbel to Gaussian noise shape.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-7-robustness-across-utility-structures" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="study-7-robustness-across-utility-structures"><span class="header-section-number">7.7</span> Study 7: Robustness Across Utility Structures</h2>
<p>The preceding studies used specific utility vectors. To assess robustness, I examine whether the convergence pattern holds across different utility configurations that are common in psychological experiments.</p>
<div id="cell-fig-robustness" class="cell">
<div class="cell-output-display">
<div id="fig-robustness" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robustness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-robustness-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Total variation distance from Probit as a function of θ for K = 5 alternatives under four utility structures: uniform, dominant, linear, and clustered."><img src="index_files/figure-html/fig-robustness-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robustness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Total variation distance from Probit as a function of θ for K = 5 alternatives under four utility structures: uniform, dominant, linear, and clustered.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.8</span> Summary</h2>
<p>These multinomial simulations confirm and extend the binary-case results:</p>
<ol type="1">
<li><p><strong>Convergence is gradual and universal</strong>: Across different values of <span class="math inline">\(K\)</span> and different utility structures, the Poisson count race converges to the Multinomial Probit reference within <span class="math inline">\(\theta \approx 50\)</span>–<span class="math inline">\(100\)</span> in total variation distance.</p></li>
<li><p><strong>Probability redistribution</strong>: As <span class="math inline">\(\theta\)</span> increases, the probit model concentrates more probability on the best alternative and less on inferior alternatives, reflecting the thinner tails of Gaussian noise relative to Gumbel.</p></li>
<li><p><strong>Set-size scaling</strong>: The logit and probit models predict systematically different scaling of target choice probability with the number of competitors. The Poisson count race interpolates between these two patterns, connecting to the empirical findings of <span class="citation" data-cites="robinsonHowPeopleBuild2023">(<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">Robinson et al. 2023</a>)</span>.</p></li>
<li><p><strong>IIA erosion</strong>: The Independence of Irrelevant Alternatives property, which holds exactly at <span class="math inline">\(\theta = 1\)</span>, is progressively violated as <span class="math inline">\(\theta\)</span> increases. This provides a process-level account of why IIA holds for logit but not probit: it is a consequence of the Gumbel noise shape, and alternative noise shapes—induced by higher accumulation thresholds—do not preserve it.</p></li>
<li><p><strong>Parameter invariance</strong>: When choice data generated by the Poisson count race are fit under a logit assumption, the recovered inverse temperature drifts with set size <span class="math inline">\(K\)</span> for all <span class="math inline">\(\theta &gt; 1\)</span>. Conversely, when fit under a probit assumption, the recovered noise scale remains stable for large <span class="math inline">\(\theta\)</span> but drifts when <span class="math inline">\(\theta\)</span> is small. This cross-over in parameter invariance provides a process-level account of the empirical findings of <span class="citation" data-cites="robinsonHowPeopleBuild2023">(<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">Robinson et al. 2023</a>)</span>: parameter stability across <span class="math inline">\(K\)</span> is diagnostic of whether the effective noise distribution is closer to Gumbel or Gaussian.</p></li>
<li><p><strong>Noise shape transition</strong>: The underlying mechanism is a smooth transition in the shape of the standardised noise distribution, from the skewed Gumbel (<span class="math inline">\(\theta = 1\)</span>) to the symmetric Gaussian (<span class="math inline">\(\theta \to \infty\)</span>). The skewness and kurtosis decay at known rates, providing analytic control over the approximation quality.</p></li>
</ol>
</section>
</section>
<section id="discussion" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Discussion</h1>
<p>The present work develops a generative framework in which Multinomial Logit and Multinomial Probit arise as endpoint regimes of a single parametric family of stochastic accumulation models. By introducing a Poisson count race and a temperature identification that separates noise scale from noise shape, this paper clarifies how extreme-value and Gaussian choice behavior emerge as members of the log-Gamma random utility family, indexed by the accumulation threshold <span class="math inline">\(\theta\)</span>. As discussed in Section 5, this bridge is distributional rather than dynamical: the temperature identification compares different accumulation systems at matched discriminability, rather than describing the behavior of a single system under threshold manipulation. The goal is not to advocate replacing existing models, but to clarify their relationship: logit and probit represent different positions within a continuum of log-Gamma noise shapes, with the accumulation threshold governing the transition between them.</p>
<p>The multinomial simulations demonstrate that this unification is not merely a theoretical curiosity. Thresholded accumulation induces systematic, graded violations of IIA that converge toward the dependence structure characteristic of multinomial probit. This dependence emerges endogenously from the accumulation and stopping rule, rather than being imposed by construction. The parameter invariance results further connect the framework to recent empirical findings <span class="citation" data-cites="robinsonHowPeopleBuild2023">(<a href="#ref-robinsonHowPeopleBuild2023" role="doc-biblioref">Robinson et al. 2023</a>)</span>, providing a process-level account of why Gaussian-based parameters exhibit greater stability across changes in set size.</p>
<section id="relation-to-existing-models" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="relation-to-existing-models"><span class="header-section-number">8.1</span> Relation to existing models</h2>
<p>From a mathematical standpoint, all components of the present framework are classical: exponential races yield Luce’s choice rule, Gamma waiting times arise from accumulated Poisson events, and asymptotic normality follows from the Central Limit Theorem. The contribution lies in assembling these elements into a single generative family and identifying the conditions under which its limiting behavior remains non-degenerate.</p>
<p>The present model should not be conflated with full sequential sampling models such as the Diffusion Decision Model <span class="citation" data-cites="ratcliffTheoryMemoryRetrieval1978">(<a href="#ref-ratcliffTheoryMemoryRetrieval1978" role="doc-biblioref">Ratcliff 1978</a>)</span> or Linear Ballistic Accumulator <span class="citation" data-cites="brownBallisticModelChoice2005">(<a href="#ref-brownBallisticModelChoice2005" role="doc-biblioref">Brown and Heathcote 2005</a>)</span>. Those models jointly account for response times and accuracy via continuous accumulation with explicit drift and boundary parameters. The Poisson count race is deliberately minimal: it uses accumulation as a generative device to induce a family of random utility models, without making claims about within-trial dynamics or response time distributions.</p>
<p>Between the logit and probit endpoints lies a continuum of log-Gamma random utility models. These intermediate regimes are not intended as new default specifications, but they underscore that logit and probit are special cases of a broader family. Deviations from logit or probit behavior may sometimes reflect differences in accumulation thresholds rather than fundamentally different noise sources.</p>
</section>
<section id="implications-for-empirical-modeling" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="implications-for-empirical-modeling"><span class="header-section-number">8.2</span> Implications for empirical modeling</h2>
<p>The present framework offers a theoretical account of why logit-based and probit-based models may differ in parameter invariance across task structures. Models with larger effective accumulation thresholds naturally exhibit Gaussian-like behavior, which may confer greater stability across changes in the number of alternatives. At the same time, the results caution against interpreting superior empirical performance of one model class as evidence for a particular noise distribution in isolation: differences between logit and probit may reflect differences in decision criteria or commitment thresholds rather than differences in representational noise per se.</p>
</section>
<section id="limitations-and-extensions" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="limitations-and-extensions"><span class="header-section-number">8.3</span> Limitations and extensions</h2>
<p>The Poisson count race is intentionally simple. It assumes independent accumulation processes and focuses exclusively on choice probabilities, abstracting away from response times and within-trial dynamics. Extensions that allow correlated accumulators, time-varying rates, or joint modeling of choice and response time are natural directions for future work.</p>
<p>The present analysis treats the accumulation threshold as fixed across trials and alternatives. Allowing threshold variability or adaptive stopping rules could further enrich the family of induced choice models and connect more directly to theories of decision caution and speed–accuracy trade-offs.</p>
<p>The model positions the Poisson count race as a parametric family indexed by <span class="math inline">\((\theta, \beta)\)</span>, but a formal identification analysis is beyond the present scope. In principle, <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\beta\)</span> play distinct roles—shape versus scale of the noise distribution—and the shape of the psychometric function or the pattern of IIA violations could serve to identify <span class="math inline">\(\theta\)</span> from choice data. Whether these parameters are jointly identifiable from aggregate choice frequencies alone, and under what experimental designs, remains an open question for future investigation.</p>
<p>Finally, the framework naturally invites comparison with other random utility specifications. Exploring whether additional classical models arise as limiting regimes under alternative accumulation rules may provide further insight into the structure of discrete choice behavior.</p>
</section>
<section id="concluding-remarks" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="concluding-remarks"><span class="header-section-number">8.4</span> Concluding remarks</h2>
<p>By grounding discrete choice models in a common stochastic accumulation process, the Poisson count race reframes a long-standing modeling distinction. Logit and probit emerge not as competing assumptions about utility noise, but as members of a single parametric family — the log-Gamma random utility models — indexed by the accumulation threshold <span class="math inline">\(\theta\)</span>. This unification is algebraic and distributional: it reveals that the two canonical specifications occupy endpoint positions within a continuous family of noise shapes, rather than representing fundamentally distinct generating mechanisms. At the same time, the unification does not imply that a single decision system can transition between regimes by adjusting its threshold alone, since the temperature identification that enables comparison across <span class="math inline">\(\theta\)</span> values entails different effective rate structures. The framework thus clarifies the conceptual relationship between logit and probit and provides a principled basis for comparison, illustrating how process-level reasoning can illuminate the structure of static choice models.</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-brownBallisticModelChoice2005" class="csl-entry" role="listitem">
Brown, Scott, and Andrew Heathcote. 2005. <span>“A <span>Ballistic Model</span> of <span>Choice Response Time</span>.”</span> <em>Psychological Review</em> 112 (1): 117–28. <a href="https://doi.org/10.1037/0033-295X.112.1.117">https://doi.org/10.1037/0033-295X.112.1.117</a>.
</div>
<div id="ref-hausmanConditionalProbitModel1978a" class="csl-entry" role="listitem">
Hausman, Jerry A., and David A. Wise. 1978. <span>“A <span>Conditional Probit Model</span> for <span>Qualitative Choice</span>: <span>Discrete Decisions Recognizing Interdependence</span> and <span>Heterogeneous Preferences</span>.”</span> <em>Econometrica</em> 46 (2): 403–26. <a href="https://doi.org/10.2307/1913909">https://doi.org/10.2307/1913909</a>.
</div>
<div id="ref-luceIndividualChoiceBehavior1959" class="csl-entry" role="listitem">
Luce, R. Duncan. 1959. <em>Individual Choice Behavior</em>. Individual Choice Behavior. Oxford, England: John Wiley.
</div>
<div id="ref-mcfaddenConditionalLogitAnalysis1974" class="csl-entry" role="listitem">
McFadden, Daniel. 1974. <span>“Conditional Logit Analysis of Qualitative Choice Behavior.”</span> In <em>Frontiers in Econometrics</em>, 105. Frontiers in Econometrics. - <span>New York</span> [u.a.] : <span>Academic Press</span>, <span>ISBN</span> 0-12-776150-0. - 1974, p. 105-142.
</div>
<div id="ref-ratcliffTheoryMemoryRetrieval1978" class="csl-entry" role="listitem">
Ratcliff, Roger. 1978. <span>“A Theory of Memory Retrieval.”</span> <em>Psychological Review</em> 85 (2): 59–108. <a href="https://doi.org/10.1037/0033-295X.85.2.59">https://doi.org/10.1037/0033-295X.85.2.59</a>.
</div>
<div id="ref-robinsonHowPeopleBuild2023" class="csl-entry" role="listitem">
Robinson, Maria M., Isabella C. DeStefano, Edward Vul, and Timothy F. Brady. 2023. <span>“How Do People Build up Visual Memory Representations from Sensory Evidence? <span>Revisiting</span> Two Classic Models of Choice.”</span> <em>Journal of Mathematical Psychology</em> 117 (December): 102805. <a href="https://doi.org/10.1016/j.jmp.2023.102805">https://doi.org/10.1016/j.jmp.2023.102805</a>.
</div>
<div id="ref-thurstoneLawComparativeJudgment1927" class="csl-entry" role="listitem">
Thurstone, L. L. 1927. <span>“A Law of Comparative Judgment.”</span> <em>Psychological Review</em> 34 (4): 273–86. <a href="https://doi.org/10.1037/h0070288">https://doi.org/10.1037/h0070288</a>.
</div>
<div id="ref-yellottRelationshipLucesChoice1977" class="csl-entry" role="listitem">
Yellott, John. 1977. <span>“The Relationship Between <span>Luce</span>’s <span>Choice Axiom</span>, <span>Thurstone</span>’s <span>Theory</span> of <span>Comparative Judgment</span>, and the Double Exponential Distribution.”</span> <em>Journal of Mathematical Psychology</em> 15 (2): 109–44. <a href="https://doi.org/10.1016/0022-2496(77)90026-8">https://doi.org/10.1016/0022-2496(77)90026-8</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>