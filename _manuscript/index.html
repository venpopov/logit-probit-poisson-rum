<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a74871fe4945b66d259aafc266475145.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice">
<meta name="citation_author" content="Vencislav Popov">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Literate programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
</head>

<body class="quarto-light">

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A Poisson Count Race as a Generative Bridge Between Logit and Probit Choice</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Vencislav Popov </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Department of Psychology, University of Zurich
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>



    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract"><span class="header-section-number">1</span> Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">2</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#a-stochastic-accumulation-perspective" id="toc-a-stochastic-accumulation-perspective" class="nav-link" data-scroll-target="#a-stochastic-accumulation-perspective"><span class="header-section-number">2.1</span> A stochastic accumulation perspective</a></li>
  <li><a href="#noise-scale-noise-shape-and-identification" id="toc-noise-scale-noise-shape-and-identification" class="nav-link" data-scroll-target="#noise-scale-noise-shape-and-identification"><span class="header-section-number">2.2</span> Noise scale, noise shape, and identification</a></li>
  <li><a href="#logit-and-probit-as-endpoint-regimes" id="toc-logit-and-probit-as-endpoint-regimes" class="nav-link" data-scroll-target="#logit-and-probit-as-endpoint-regimes"><span class="header-section-number">2.3</span> Logit and probit as endpoint regimes</a></li>
  <li><a href="#contribution-and-overview" id="toc-contribution-and-overview" class="nav-link" data-scroll-target="#contribution-and-overview"><span class="header-section-number">2.4</span> Contribution and overview</a></li>
  </ul></li>
  <li><a href="#formal-statement-of-random-utility-models" id="toc-formal-statement-of-random-utility-models" class="nav-link" data-scroll-target="#formal-statement-of-random-utility-models"><span class="header-section-number">3</span> Formal statement of random utility models</a></li>
  <li><a href="#the-generative-model-a-poisson-count-race" id="toc-the-generative-model-a-poisson-count-race" class="nav-link" data-scroll-target="#the-generative-model-a-poisson-count-race"><span class="header-section-number">4</span> The Generative Model: A Poisson Count Race</a>
  <ul class="collapse">
  <li><a href="#transformation-to-waiting-times" id="toc-transformation-to-waiting-times" class="nav-link" data-scroll-target="#transformation-to-waiting-times"><span class="header-section-number">4.1</span> Transformation to Waiting Times</a></li>
  <li><a href="#the-random-utility-representation" id="toc-the-random-utility-representation" class="nav-link" data-scroll-target="#the-random-utility-representation"><span class="header-section-number">4.2</span> The Random Utility Representation</a></li>
  </ul></li>
  <li><a href="#the-logit-boundary-theta-1" id="toc-the-logit-boundary-theta-1" class="nav-link" data-scroll-target="#the-logit-boundary-theta-1"><span class="header-section-number">5</span> The Logit Boundary (<span class="math inline">\(\theta = 1\)</span>)</a></li>
  <li><a href="#temperature-identification" id="toc-temperature-identification" class="nav-link" data-scroll-target="#temperature-identification"><span class="header-section-number">6</span> Temperature Identification</a></li>
  <li><a href="#the-probit-limit-theta-to-infty" id="toc-the-probit-limit-theta-to-infty" class="nav-link" data-scroll-target="#the-probit-limit-theta-to-infty"><span class="header-section-number">7</span> The Probit Limit (<span class="math inline">\(\theta \to \infty\)</span>)</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">8</span> Summary</a></li>
  <li><a href="#simulation-studies-binary-choice" id="toc-simulation-studies-binary-choice" class="nav-link" data-scroll-target="#simulation-studies-binary-choice"><span class="header-section-number">9</span> Simulation Studies: Binary Choice</a></li>
  <li><a href="#simulation-studies-multinomial-k-alternative-choice" id="toc-simulation-studies-multinomial-k-alternative-choice" class="nav-link" data-scroll-target="#simulation-studies-multinomial-k-alternative-choice"><span class="header-section-number">10</span> Simulation Studies: Multinomial K-alternative Choice</a>
  <ul class="collapse">
  <li><a href="#study-1-convergence-to-probit" id="toc-study-1-convergence-to-probit" class="nav-link" data-scroll-target="#study-1-convergence-to-probit"><span class="header-section-number">10.1</span> Study 1: Convergence to Probit</a></li>
  <li><a href="#study-2-choice-probability-vectors" id="toc-study-2-choice-probability-vectors" class="nav-link" data-scroll-target="#study-2-choice-probability-vectors"><span class="header-section-number">10.2</span> Study 2: Choice Probability Vectors</a></li>
  <li><a href="#study-3-set-size-scaling" id="toc-study-3-set-size-scaling" class="nav-link" data-scroll-target="#study-3-set-size-scaling"><span class="header-section-number">10.3</span> Study 3: Set-Size Scaling</a></li>
  <li><a href="#study-4-independence-of-irrelevant-alternatives" id="toc-study-4-independence-of-irrelevant-alternatives" class="nav-link" data-scroll-target="#study-4-independence-of-irrelevant-alternatives"><span class="header-section-number">10.4</span> Study 4: Independence of Irrelevant Alternatives</a></li>
  <li><a href="#study-5-parameter-invariance-across-set-size" id="toc-study-5-parameter-invariance-across-set-size" class="nav-link" data-scroll-target="#study-5-parameter-invariance-across-set-size"><span class="header-section-number">10.5</span> Study 5: Parameter Invariance Across Set Size</a></li>
  <li><a href="#study-6-distributional-shape-noise-skewness-and-kurtosis" id="toc-study-6-distributional-shape-noise-skewness-and-kurtosis" class="nav-link" data-scroll-target="#study-6-distributional-shape-noise-skewness-and-kurtosis"><span class="header-section-number">10.6</span> Study 6: Distributional Shape — Noise Skewness and Kurtosis</a></li>
  <li><a href="#study-7-robustness-across-utility-structures" id="toc-study-7-robustness-across-utility-structures" class="nav-link" data-scroll-target="#study-7-robustness-across-utility-structures"><span class="header-section-number">10.7</span> Study 7: Robustness Across Utility Structures</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1"><span class="header-section-number">10.8</span> Summary</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">11</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#relation-to-existing-choice-models" id="toc-relation-to-existing-choice-models" class="nav-link" data-scroll-target="#relation-to-existing-choice-models"><span class="header-section-number">11.1</span> Relation to existing choice models</a></li>
  <li><a href="#temperature-identification-and-model-comparison" id="toc-temperature-identification-and-model-comparison" class="nav-link" data-scroll-target="#temperature-identification-and-model-comparison"><span class="header-section-number">11.2</span> Temperature identification and model comparison</a></li>
  <li><a href="#intermediate-regimes-and-log-gamma-utility-noise" id="toc-intermediate-regimes-and-log-gamma-utility-noise" class="nav-link" data-scroll-target="#intermediate-regimes-and-log-gamma-utility-noise"><span class="header-section-number">11.3</span> Intermediate regimes and log-Gamma utility noise</a></li>
  <li><a href="#implications-for-empirical-modeling" id="toc-implications-for-empirical-modeling" class="nav-link" data-scroll-target="#implications-for-empirical-modeling"><span class="header-section-number">11.4</span> Implications for empirical modeling</a></li>
  <li><a href="#limitations-and-extensions" id="toc-limitations-and-extensions" class="nav-link" data-scroll-target="#limitations-and-extensions"><span class="header-section-number">11.5</span> Limitations and extensions</a></li>
  <li><a href="#concluding-remarks" id="toc-concluding-remarks" class="nav-link" data-scroll-target="#concluding-remarks"><span class="header-section-number">11.6</span> Concluding remarks</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="index-preview.html"><i class="bi bi-journal-code"></i>Article Notebook</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="abstract" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Abstract</h1>
<p>This study unifies the two canonical discrete choice specifications–Multinomial Logit and Multinomial Probit–within a single generative framework. We introduce a <strong>Poisson Count Race</strong>, wherein <span class="math inline">\(K\)</span> alternatives generate events according to independent Poisson processes. A choice is determined when an alternative reaches a specific cumulative count threshold, <span class="math inline">\(\theta\)</span>. We demonstrate that at the threshold <span class="math inline">\(\theta=1\)</span>, the model yields the Multinomial Logit (Luce choice rule). By normalizing the utility noise to maintain a constant variance–a process termed temperature identification–we show that as <span class="math inline">\(\theta \to \infty\)</span>, the model converges to the Multinomial Probit. This formulation provides a parametric bridge in which <span class="math inline">\(\theta\)</span> governs the shape of the error distribution and a separate parameter, <span class="math inline">\(\beta\)</span>, modulates the temperature. Crucially, this bridge arises from a single stochastic accumulation mechanism rather than from post hoc variance matching between distinct models.</p>
</section>
<section id="introduction" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Introduction</h1>
<p>Discrete choice models based on Random Utility Theory (RUT) form a central pillar of mathematical psychology, econometrics, and cognitive science. Across these fields, it is common to assume that each alternative in a choice set elicits a latent scalar quantity–often interpreted as strength, utility, or evidence–and that observed choices arise from a comparison of these latent quantities under stochastic variability. Despite the diversity of substantive applications, two modeling traditions dominate this landscape: logit-based models, derived from Luce’s choice axiom and extreme-value theory, and probit-based models, derived from Gaussian signal detection theory.</p>
<p>The central question addressed in this paper is this: Can the two canonical random-utility discrete-choice specifications—multinomial logit and multinomial probit—be derived as limit cases of a single stochastic accumulation/race mechanism, so that the logitprobit relationship is explained by a unified generative process rather than by informal distributional similarity or variance matching?</p>
<p>The Multinomial Logit (MNL) model arises when stochastic variability is modeled via independent Gumbel (Type I Extreme Value) disturbances added to deterministic utilities. This specification is tightly linked to Luce’s ratio-of-strengths axiom (Luce, 1959) and admits a well-known generative interpretation: if each alternative generates stochastic events according to an independent exponential clock, the probability that an alternative is chosen first is proportional to its rate. This exponential race interpretation has played a foundational role in mathematical psychology and economics, and it underlies a broad class of models including the Plackett–Luce ranking model and the “Gumbel–max trick” widely used in machine learning (McFadden, 1974; Yellott, 1977).</p>
<p>The Multinomial Probit (MNP) model, by contrast, assumes Gaussian noise on latent utilities and traces its lineage to Thurstone’s theory of comparative judgment (Thurstone, 1927) and later developments in psychometrics and signal detection theory (Bock &amp; Jones, 1968). Probit models are often motivated by appeal to aggregation, pooling, or measurement noise, and they offer greater flexibility in representing correlated disturbances across alternatives. Unlike logit, however, probit does not admit a simple closed-form choice rule, nor does it enjoy an equally canonical process-level interpretation.</p>
<p>Although logit and probit are often treated as interchangeable in practice, their conceptual relationship remains surprisingly underdeveloped. It is well known that logistic and Gaussian cumulative distribution functions closely approximate one another under variance matching in binary choice tasks, and this observation is frequently cited to justify pragmatic model choice. Yet this numerical similarity is rarely given a deeper theoretical explanation. Instead, logit and probit are typically presented as competing assumptions about the distribution of unobserved utility noise, reflecting different historical traditions rather than different instantiations of a common mechanism.</p>
<p>Recent work has renewed attention to this issue. Robinson et al.&nbsp;(2023) revisited the tension between Luce-based choice models and signal detection theory, applying a critical test of parameter invariance across changes in the number of alternatives (<span class="math inline">\(m\)</span>). Their empirical analysis demonstrated that Gaussian signal detection parameters (<span class="math inline">\(d'\)</span>) remained stable as the choice set size increased, whereas softmax parameters (<span class="math inline">\(\beta\)</span>) exhibited systematic variance. While this diagnostic work highlights the superior generalization of Gaussian assumptions in certain memory tasks, it leaves open the generative question: is there a single mechanism that can account for both regimes?</p>
<p>The present work addresses this by shifting perspective. Rather than beginning with static assumptions about utility noise, we begin with a stochastic accumulation process and ask what class of random utility models it induces.</p>
<section id="a-stochastic-accumulation-perspective" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="a-stochastic-accumulation-perspective"><span class="header-section-number">2.1</span> A stochastic accumulation perspective</h2>
<p>We introduce a <strong>Poisson count race</strong>, a simple generative model in which each alternative produces stochastic events over time according to an independent Poisson process. A decision is made when one alternative reaches a fixed cumulative count threshold, denoted by <span class="math inline">\(\theta\)</span>. The threshold <span class="math inline">\(\theta\)</span> thus controls how much stochastic evidence must be accumulated before commitment.</p>
<p>Psychologically, <span class="math inline">\(\theta\)</span> can be interpreted as a commitment criterion or decision caution parameter: higher thresholds require more accumulated events before choice.</p>
<p>This construction generalizes the classical exponential race in a minimal way. When <span class="math inline">\(\theta = 1\)</span>, the model reduces exactly to the familiar exponential race, recovering Luce’s choice rule and the Multinomial Logit model without approximation. For <span class="math inline">\(\theta &gt; 1\)</span>, choice depends not on the first event, but on the time required to accumulate multiple events, introducing a new degree of freedom into the stochastic mechanism.</p>
<p>Crucially, the Poisson count race admits an exact Random Utility representation. The waiting time for an alternative to reach <span class="math inline">\(\theta\)</span> events follows a Gamma (Erlang) distribution, and comparing these waiting times is equivalent to comparing latent utilities composed of a deterministic component and a stochastic component with a log-Gamma distribution. Varying the threshold <span class="math inline">\(\theta\)</span> therefore induces a one-parameter family of random utility models with a shared systematic utility but systematically varying noise distributions.</p>
<p>Although the Poisson count race is an accumulation process, the present work does not aim to model response times or trial-by-trial dynamics. Instead, accumulation is used here as a generative device for inducing a family of random utility models with interpretable noise structure.</p>
</section>
<section id="noise-scale-noise-shape-and-identification" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="noise-scale-noise-shape-and-identification"><span class="header-section-number">2.2</span> Noise scale, noise shape, and identification</h2>
<p>A central difficulty in comparing discrete choice models with different error distributions is the identification of scale. In standard practice, the scale of utility noise is absorbed into the coefficients, making it difficult to compare models with different distributional assumptions on equal footing. This issue is particularly salient when contrasting logit and probit models, whose error distributions differ not only in shape but also in variance and tail behavior.</p>
<p>To address this, we introduce a <strong>temperature identification</strong> that standardizes the stochastic component of utility to have fixed variance across all values of <span class="math inline">\(\theta\)</span>. This separates two conceptually distinct aspects of randomness in choice:</p>
<ol type="1">
<li><p><strong>Noise scale</strong>, controlled by a temperature parameter <span class="math inline">\(\beta\)</span>, which governs the overall magnitude of stochasticity relative to systematic utility.</p></li>
<li><p><strong>Noise shape</strong>, controlled by the accumulation threshold <span class="math inline">\(\theta\)</span>, which determines the distributional form of the stochastic component.</p></li>
</ol>
<p>Under this identification, increasing <span class="math inline">\(\theta\)</span> does not trivially eliminate noise by driving the model toward deterministic choice. Instead, it isolates the effect of changing the shape of stochasticity while holding its magnitude fixed.</p>
</section>
<section id="logit-and-probit-as-endpoint-regimes" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="logit-and-probit-as-endpoint-regimes"><span class="header-section-number">2.3</span> Logit and probit as endpoint regimes</h2>
<p>Within the temperature-identified Poisson count race, two classical models emerge as limiting cases of a single generative mechanism. When <span class="math inline">\(\theta = 1\)</span>, the stochastic utility component is exactly Gumbel distributed, yielding the <strong>Multinomial Logit</strong> model without approximation. As <span class="math inline">\(\theta \to \infty\)</span>, the standardized log-Gamma noise converges in distribution to a Gaussian by the Central Limit Theorem, and the resulting choice probabilities converge to those of the <strong>Multinomial Probit</strong> model.</p>
<p>From this perspective, the familiar distinction between logit and probit reflects differences in the stopping rule governing an evidence accumulation process, rather than fundamentally distinct assumptions about decision noise. Extreme-value behavior and Gaussian behavior arise naturally as endpoint regimes of the same stochastic system.</p>
</section>
<section id="contribution-and-overview" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="contribution-and-overview"><span class="header-section-number">2.4</span> Contribution and overview</h2>
<p>The contribution of this paper is not to introduce a new discrete choice specification, but to provide a unifying generative framework that clarifies the relationship between two foundational models. Specifically, we show that:</p>
<ol type="1">
<li><p>Multinomial Logit arises exactly as the single-event threshold case of a Poisson race.</p></li>
<li><p>Multinomial Probit emerges as a non-degenerate asymptotic limit under temperature identification.</p></li>
<li><p>Intermediate accumulation thresholds correspond to log-Gamma random utility models with a clear process interpretation.</p></li>
</ol>
<p>The remainder of the paper formalizes this framework. Section 2 gives the formal definition of generalized random utility models. Section 3 introduces the Poisson count race and derives its exact random utility representation. Section 4 establishes the logit boundary at <span class="math inline">\(\theta = 1\)</span>. Section 5 introduces temperature identification and analyzes the probit limit as <span class="math inline">\(\theta \to \infty\)</span>. Section 6 presents simulations illustrating the interpolation between regimes, and Section 7 discusses implications for discrete choice modeling and evidence accumulation theories.</p>
</section>
</section>
<section id="formal-statement-of-random-utility-models" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Formal statement of random utility models</h1>
<p>Consider a decision-maker facing a set of <span class="math inline">\(K \ge 2\)</span> mutually exclusive alternatives. The utility associated with alternative <span class="math inline">\(i\)</span> is decomposed into a systematic component, <span class="math inline">\(v_i\)</span>, and a stochastic component, <span class="math inline">\(\epsilon_i\)</span>:</p>
<p><span class="math display">\[U_{i} = v_{i} + \epsilon_{i}, \quad i=1, \dots, K\]</span></p>
<p>The decision-maker selects the alternative that maximizes utility:</p>
<p><span class="math display">\[C = \arg \max_{i} U_{i}\]</span></p>
<p>Classically, the distributional assumptions regarding <span class="math inline">\(\epsilon_i\)</span> dictate the structural form of the model:</p>
<ol type="1">
<li><p><strong>Gumbel Errors:</strong> If the <span class="math inline">\(\epsilon_i\)</span> terms are independent and identically distributed (i.i.d.) according to a Type I Extreme Value distribution, the choice probabilities follow the <strong>Multinomial Logit (MNL)</strong> or Softmax form.</p></li>
<li><p><strong>Gaussian Errors:</strong> If the <span class="math inline">\(\epsilon_i\)</span> terms follow a Multivariate Normal distribution, the choice probabilities are described by the <strong>Multinomial Probit (MNP)</strong> model.</p></li>
</ol>
<p>While the Logit model offers analytical tractability, the Probit model permits more flexible covariance structures. This paper derives a mechanism that naturally interpolates between these two regimes through a stochastic race process.</p>
</section>
<section id="the-generative-model-a-poisson-count-race" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> The Generative Model: A Poisson Count Race</h1>
<p>Let the accumulation of evidence or preference for each alternative <span class="math inline">\(i\)</span> be modeled by independent Poisson processes, denoted <span class="math inline">\(N_i(t)\)</span>, with rate parameters <span class="math inline">\(\lambda_i &gt; 0\)</span>.</p>
<p>We define a <strong>Count Race</strong> characterized by an integer threshold <span class="math inline">\(\theta \ge 1\)</span>. The process terminates when any single process accumulates <span class="math inline">\(\theta\)</span> events.</p>
<p><strong>Definition 1 (Stopping Time).</strong> The stopping time for the system is defined as:</p>
<p><span class="math display">\[\tau_{\theta} = \inf \{t \ge 0 : \max_{i} N_i(t) = \theta \}\]</span></p>
<p><strong>Definition 2 (Choice).</strong> The chosen alternative is the specific process that triggers the stopping time:</p>
<p><span class="math display">\[C = \arg \max_{i} N_i(\tau_{\theta})\]</span></p>
<section id="transformation-to-waiting-times" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="transformation-to-waiting-times"><span class="header-section-number">4.1</span> Transformation to Waiting Times</h2>
<p>To map this stochastic process to a utility framework, consider <span class="math inline">\(T_i^{(\theta)}\)</span>, the waiting time until the <span class="math inline">\(i\)</span>-th process records its <span class="math inline">\(\theta\)</span>-th event:</p>
<p><span class="math display">\[T_i^{(\theta)} := \inf \{t : N_i(t) = \theta \}\]</span></p>
<p>For a Poisson process with rate <span class="math inline">\(\lambda_i\)</span>, the waiting time to the <span class="math inline">\(\theta\)</span>-th jump follows a Gamma (Erlang) distribution:</p>
<p><span class="math display">\[T_i^{(\theta)} \sim \text{Gamma}(\text{shape}=\theta, \text{rate}=\lambda_i)\]</span></p>
<p>The condition that alternative <span class="math inline">\(i\)</span> “wins” the race (i.e., reaches <span class="math inline">\(\theta\)</span> events first) is equivalent to observing the minimum waiting time:</p>
<p><span class="math display">\[C = \arg \min_{i} T_i^{(\theta)}\]</span></p>
</section>
<section id="the-random-utility-representation" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="the-random-utility-representation"><span class="header-section-number">4.2</span> The Random Utility Representation</h2>
<p>The Gamma random variables may be standardized. Let <span class="math inline">\(G_i \sim \text{Gamma}(\theta, 1)\)</span> be i.i.d. random variables. Utilizing the scaling property of the Gamma distribution, the waiting times can be expressed as:</p>
<p><span class="math display">\[T_i^{(\theta)} \stackrel{d}{=} \frac{G_i}{\lambda_i}\]</span></p>
<p>Consequently, the choice problem is formulated as:</p>
<p><span class="math display">\[C = \arg \min_{i} \left( \frac{G_i}{\lambda_i} \right)\]</span></p>
<p>Applying the natural logarithm, a monotonic transformation, reverses the optimization direction from minimization to maximization:</p>
<p><span class="math display">\[\begin{aligned} C &amp;= \arg \min_{i} (\log G_i - \log \lambda_i) \\ &amp;= \arg \max_{i} (\log \lambda_i - \log G_i) \end{aligned}\]</span></p>
<p>This establishes an exact RUM structure:</p>
<p><span class="math display">\[U_i^{(\theta)} = v_i + \epsilon_i^{(\theta)}\]</span></p>
<p>where:</p>
<ul>
<li><strong>Systematic Utility:</strong> <span class="math inline">\(v_i = \log \lambda_i\)</span><br>
</li>
<li><strong>Stochastic Error:</strong> <span class="math inline">\(\epsilon_i^{(\theta)} = -\log G_i\)</span>, with <span class="math inline">\(G_i \sim \text{Gamma}(\theta, 1)\)</span>.</li>
</ul>
<p>Thus, the Poisson count race is isomorphic to a Random Utility Model characterized by Log-Gamma noise.</p>
</section>
</section>
<section id="the-logit-boundary-theta-1" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> The Logit Boundary (<span class="math inline">\(\theta = 1\)</span>)</h1>
<p>In the specific instance where the threshold is a single event (<span class="math inline">\(\theta = 1\)</span>), the waiting time distribution simplifies:</p>
<p><span class="math display">\[G_i \sim \text{Gamma}(1, 1) \equiv \text{Exponential}(1)\]</span></p>
<p>A fundamental property of the Gumbel distribution is its relationship to the Exponential distribution:</p>
<p><span class="math display">\[X \sim \text{Exp}(1) \implies -\log(X) \sim \text{Gumbel}(\text{Type I EV})\]</span></p>
<p>Therefore, when <span class="math inline">\(\theta=1\)</span>, the noise terms <span class="math inline">\(\epsilon_i^{(1)}\)</span> are i.i.d. Gumbel. This recovers the exact Multinomial Logit formula:</p>
<p><span class="math display">\[\Pr(C=i) = \frac{\exp(v_i)}{\sum_{j=1}^K \exp(v_j)} = \frac{\lambda_i}{\sum_{j=1}^K \lambda_j}\]</span></p>
<p><strong>Result 1:</strong> The Poisson race with threshold <span class="math inline">\(\theta=1\)</span> corresponds exactly to the Luce Choice Rule (Softmax). This is a classic, well-known derivation.</p>
</section>
<section id="temperature-identification" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Temperature Identification</h1>
<p>For thresholds <span class="math inline">\(\theta &gt; 1\)</span>, the error distribution deviates from the Gumbel form. Furthermore, as <span class="math inline">\(\theta\)</span> increases, the variance of the error term diminishes. Specifically, for <span class="math inline">\(\epsilon^{(\theta)} = -\log G\)</span> where <span class="math inline">\(G \sim \text{Gamma}(\theta, 1)\)</span>:</p>
<p><span class="math display">\[\begin{aligned} \mathbb{E}[\epsilon^{(\theta)}] &amp;= -\psi(\theta) \\ \text{Var}(\epsilon^{(\theta)}) &amp;= \psi_1(\theta) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\psi(\cdot)\)</span> denotes the digamma function and <span class="math inline">\(\psi_1(\cdot)\)</span> the trigamma function.</p>
<p>As <span class="math inline">\(\theta \to \infty\)</span>, <span class="math inline">\(\psi_1(\theta) \approx 1/\theta \to 0\)</span>. In the absence of standardization, the model becomes deterministic as the stochastic noise vanishes. To facilitate comparison of error “shapes” across varying <span class="math inline">\(\theta\)</span> values, a consistent scale must be enforced. This process is referred to as <strong>Temperature Identification</strong>. Unlike conventional scale normalization, this identification preserves non-degenerate choice behavior as <span class="math inline">\(\theta\)</span> increases, allowing the large-<span class="math inline">\(\theta\)</span> limit to be meaningfully interpreted.</p>
<p>We define the standardized noise term <span class="math inline">\(Z_i^{(\theta)}\)</span> such that it possesses zero mean and unit variance for all <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[Z_i^{(\theta)} := \frac{\epsilon_i^{(\theta)} - \mu_\theta}{\sigma_\theta} = \frac{-\log G_i + \psi(\theta)}{\sqrt{\psi_1(\theta)}}\]</span></p>
<p>We propose the <strong>Temperature-Identified Family</strong> of utility models:</p>
<p><span class="math display">\[U_i^{(\theta)} = v_i + \beta Z_i^{(\theta)}\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(v_i\)</span> represents the systematic utility.</li>
<li><span class="math inline">\(\theta\)</span> governs the <strong>shape</strong> of the noise distribution (tail behavior).</li>
<li><span class="math inline">\(\beta\)</span> represents the <strong>temperature</strong> (the scale of noise relative to utility).</li>
</ul>
</section>
<section id="the-probit-limit-theta-to-infty" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> The Probit Limit (<span class="math inline">\(\theta \to \infty\)</span>)</h1>
<p>This section investigates the asymptotic behavior of the temperature-identified model as the count threshold increases. We rely on the asymptotic normality of the log-transformed Gamma distribution.</p>
<p>Intuitively, as the count threshold increases, the log-Gamma noise reflects the aggregation of many small stochastic increments, and Gaussian behavior emerges by the Central Limit Theorem.</p>
<p>As <span class="math inline">\(\theta \to \infty\)</span>:</p>
<p><span class="math display">\[Z_i^{(\theta)} \xrightarrow{d} \mathcal{N}(0, 1)\]</span></p>
<p>Consequently, the random utilities converge in distribution:</p>
<p><span class="math display">\[(v_i + \beta Z_i^{(\theta)})_{i=1}^K \xrightarrow{d} (v_i + \beta Z_i)_{i=1}^K\]</span></p>
<p>where <span class="math inline">\(Z_i \stackrel{i.i.d.}{\sim} \mathcal{N}(0, 1)\)</span>.</p>
<p>Given that the error terms converge to additive Gaussian noise, the choice probabilities converge to those of the Multinomial Probit model.</p>
<p><strong>Result 2:</strong> As <span class="math inline">\(\theta \to \infty\)</span>, the Temperature-Identified Poisson race converges to a Probit model with noise scale <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="summary" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Summary</h1>
<p>The derivation presented herein establishes the Poisson Count Race as a flexible generative mechanism for discrete choice. By identifying the model via temperature standardization, we obtain a single-parameter family (indexed by <span class="math inline">\(\theta\)</span>) that continuously bridges the two classical extremes of choice modeling:</p>
<ol type="1">
<li><p><span class="math inline">\(\theta = 1\)</span><strong>:</strong> Exact <strong>Logit</strong> (Gumbel errors).</p></li>
<li><p><span class="math inline">\(1 &lt; \theta &lt; \infty\)</span><strong>:</strong> <strong>Log-Gamma</strong> RUM (an intermediate regime).</p></li>
<li><p><span class="math inline">\(\theta \to \infty\)</span><strong>:</strong> <strong>Probit</strong> (Gaussian errors).</p></li>
</ol>
<p>This result suggests that the distinction between Logit and Probit may be viewed not as a fundamental difference in error philosophy, but rather as a difference in the stopping rule governing an underlying stochastic accumulation process.</p>
</section>
<section id="simulation-studies-binary-choice" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Simulation Studies: Binary Choice</h1>
<p>To illustrate how the Poisson count race family interpolates between Logit and Probit, we conduct a simulation study in the binary choice setting (<span class="math inline">\(K=2\)</span>). This setting admits closed-form choice probabilities and allows direct visual comparison with both classical models.</p>
<p>Let <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> denote the Poisson rates of the two alternatives, and define the log-rate ratio <span class="math inline">\(x = \log(\lambda_1/\lambda_2)\)</span>. The probability that alternative 1 wins the race can be expressed in closed form using the regularized incomplete Beta function:</p>
<p><span class="math display">\[\Pr(C = 1 \mid x, \theta) = I_{\sigma(x)}(\theta, \theta)\]</span></p>
<p>where <span class="math inline">\(I_p(a, b)\)</span> is the regularized incomplete Beta function and <span class="math inline">\(\sigma(x) = (1 + e^{-x})^{-1}\)</span>. For <span class="math inline">\(\theta = 1\)</span>, this reduces exactly to <span class="math inline">\(\sigma(x)\)</span>, the logistic function.</p>
<p>When choice probabilities are plotted directly as a function of <span class="math inline">\(x\)</span> for increasing <span class="math inline">\(\theta\)</span>, the choice function becomes increasingly steep and converges to a step function at <span class="math inline">\(x = 0\)</span>, reflecting deterministic selection of the alternative with the larger rate. This confirms that without temperature identification, increasing the count threshold simply reduces stochasticity rather than inducing Gaussian behavior.</p>
<p>To compare noise <em>shape</em> independently of noise <em>scale</em>, we adopt the temperature identification introduced in Section 5. For binary choice, the variance of the utility noise difference is <span class="math inline">\(\text{Var}(\epsilon_1^{(\theta)} - \epsilon_2^{(\theta)}) = 2\psi_1(\theta)\)</span>. We therefore define the standardized signal <span class="math inline">\(s = x / \sqrt{2\psi_1(\theta)}\)</span>. On this variance-matched axis, the Logit reference (<span class="math inline">\(\theta = 1\)</span>) uses <span class="math inline">\(\text{sd}_{\text{diff}} = \pi/\sqrt{3}\)</span> (the standard deviation of the difference of two independent Gumbel variates), and the Probit reference is simply <span class="math inline">\(\Phi(s)\)</span>. Under this normalization, the <span class="math inline">\(\theta = 1\)</span> Poisson race coincides exactly with the variance-matched logit curve, while increasing <span class="math inline">\(\theta\)</span> yields choice functions that converge uniformly to the probit curve.</p>
<p>Because logit and probit are themselves numerically close under variance matching, the differences between models are small in absolute magnitude but systematic. To make these differences visible, <a href="#fig-residuals" class="quarto-xref">Figure&nbsp;1</a> plots residuals relative to the variance-matched Logit model. At <span class="math inline">\(\theta = 1\)</span>, the residual is identically zero (exact Logit). As <span class="math inline">\(\theta\)</span> increases, the residuals grow smoothly and converge toward the Probit<span class="math inline">\(-\)</span>Logit difference curve, with the maximum absolute deviation from probit decaying rapidly in <span class="math inline">\(\theta\)</span>. This confirms that the temperature-identified Poisson count race defines a continuous, parameterized family of choice rules that interpolates smoothly between logit-like and probit-like behavior.</p>
<div id="cell-fig-residuals" class="cell">
<div class="cell-output-display">
<div id="fig-residuals" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-residuals-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Residuals of Poisson count race choice probabilities relative to the Logit reference, plotted on a variance-matched axis. Each curve corresponds to a different accumulation threshold θ. At θ = 1 the model is exactly Logit (zero residual). As θ increases, the curves converge toward the Probit − Logit difference (black curve), confirming the theoretical bridge between the two models."><img src="index_files/figure-html/fig-residuals-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residuals-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Residuals of Poisson count race choice probabilities relative to the Logit reference, plotted on a variance-matched axis. Each curve corresponds to a different accumulation threshold θ. At θ = 1 the model is exactly Logit (zero residual). As θ increases, the curves converge toward the Probit − Logit difference (black curve), confirming the theoretical bridge between the two models.
</figcaption>
</figure>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-1" href="index-preview.html#cell-fig-residuals">Source: Article Notebook</a></div>
</section>
<section id="simulation-studies-multinomial-k-alternative-choice" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Simulation Studies: Multinomial K-alternative Choice</h1>
<p>The binary simulation demonstrates that the temperature-identified Poisson count race interpolates smoothly between Logit (<span class="math inline">\(\theta = 1\)</span>) and Probit (<span class="math inline">\(\theta \to \infty\)</span>) in the two-alternative case. Here we extend this analysis to the multinomial setting (<span class="math inline">\(K &gt; 2\)</span>), where the differences between logit and probit become richer and more consequential.</p>
<p>In the binary case, logit and probit choice functions differ only in the shape of the psychometric curve—a subtle quantitative distinction. With three or more alternatives, additional qualitative differences emerge. Most prominently, the Multinomial Logit model satisfies the <em>Independence of Irrelevant Alternatives</em> (IIA) property: the ratio of choice probabilities for any two alternatives is independent of the remaining alternatives in the choice set. The Multinomial Probit model, even with independent errors, does not share this property. The Poisson count race therefore provides a window into how IIA-like behavior gradually weakens as the noise distribution transitions from Gumbel to Gaussian.</p>
<p>We organise the multinomial simulations around five questions:</p>
<ol type="1">
<li><strong>Convergence</strong>: How quickly do Poisson count race choice probabilities converge to the Probit reference as <span class="math inline">\(\theta\)</span> increases, and does the rate of convergence depend on <span class="math inline">\(K\)</span>?</li>
<li><strong>Probability vectors</strong>: How does the full distribution over alternatives change as <span class="math inline">\(\theta\)</span> varies from 1 to large values?</li>
<li><strong>Set-size scaling</strong>: How does the probability of choosing a target alternative scale with the number of competitors, and how does this scaling differ between logit, probit, and intermediate regimes?</li>
<li><strong>Independence of Irrelevant Alternatives</strong>: How does the IIA property—exact under logit—erode as <span class="math inline">\(\theta\)</span> increases toward the probit regime?</li>
<li><strong>Parameter invariance</strong>: When misspecified logit or probit models are fit to Poisson count race data, which model yields parameters that are invariant to <span class="math inline">\(K\)</span>?</li>
</ol>
<p>All simulations use Monte Carlo sampling with <span class="math inline">\(10^6\)</span> replications per condition unless otherwise noted.</p>
<section id="study-1-convergence-to-probit" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="study-1-convergence-to-probit"><span class="header-section-number">10.1</span> Study 1: Convergence to Probit</h2>
<p>We first examine how the total variation (TV) distance between the Poisson count race choice probabilities and the Logit / Probit references changes as a function of <span class="math inline">\(\theta\)</span>, for different numbers of alternatives <span class="math inline">\(K\)</span>.</p>
<p>For each <span class="math inline">\(K\)</span>, we use linearly spaced utilities <span class="math inline">\(v_i = (K - i)/(K - 1)\)</span> for <span class="math inline">\(i = 1, \ldots, K\)</span>, ensuring that the best and worst alternatives always have utilities 1 and 0 regardless of <span class="math inline">\(K\)</span>. The temperature is fixed at <span class="math inline">\(\beta = 1\)</span>.</p>
<div id="cell-fig-tv-distance" class="cell">
<div class="cell-output-display">
<div id="fig-tv-distance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tv-distance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-tv-distance-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Total variation distance between Poisson count race choice probabilities and the Logit (blue) and Probit (red) references, as a function of threshold θ. Each panel corresponds to a different number of alternatives K. As θ increases, the race model moves away from logit and toward probit across all values of K. The convergence to probit is rapid: by θ ≈ 50–100, the TV distance from probit is negligible."><img src="index_files/figure-html/fig-tv-distance-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tv-distance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Total variation distance between Poisson count race choice probabilities and the Logit (blue) and Probit (red) references, as a function of threshold θ. Each panel corresponds to a different number of alternatives K. As θ increases, the race model moves away from logit and toward probit across all values of K. The convergence to probit is rapid: by θ ≈ 50–100, the TV distance from probit is negligible.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-2-choice-probability-vectors" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="study-2-choice-probability-vectors"><span class="header-section-number">10.2</span> Study 2: Choice Probability Vectors</h2>
<p>To visualise how the full distribution over alternatives evolves with <span class="math inline">\(\theta\)</span>, we fix <span class="math inline">\(K = 5\)</span> with utilities <span class="math inline">\(v = (2.0,\; 1.5,\; 1.0,\; 0.5,\; 0.0)\)</span> and plot the choice probability for each alternative across a range of thresholds.</p>
<div id="cell-fig-prob-vectors" class="cell">
<div class="cell-output-display">
<div id="fig-prob-vectors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-prob-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-prob-vectors-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Choice probabilities for each of five alternatives as a function of threshold θ, under the temperature-identified Poisson count race with β = 1. Horizontal dashed lines mark the Logit reference (θ = 1); horizontal dotted lines mark the Probit reference (θ → ∞). As θ increases, the race probabilities transition smoothly from logit to probit values. The probit model concentrates slightly more probability on the best alternative and less on the worst, relative to logit, reflecting the thinner tails of the Gaussian distribution."><img src="index_files/figure-html/fig-prob-vectors-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prob-vectors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Choice probabilities for each of five alternatives as a function of threshold θ, under the temperature-identified Poisson count race with β = 1. Horizontal dashed lines mark the Logit reference (θ = 1); horizontal dotted lines mark the Probit reference (θ → ∞). As θ increases, the race probabilities transition smoothly from logit to probit values. The probit model concentrates slightly more probability on the best alternative and less on the worst, relative to logit, reflecting the thinner tails of the Gaussian distribution.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-3-set-size-scaling" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="study-3-set-size-scaling"><span class="header-section-number">10.3</span> Study 3: Set-Size Scaling</h2>
<p>A critical diagnostic for discriminating between logit and probit models is the effect of adding alternatives to the choice set (Robinson et al., 2023). Under MNL, the probability of choosing a target alternative with fixed utility is strictly determined by the ratio of its strength to the total strength. Under MNP, the scaling with set size differs because the probability of “winning” the maximum comparison depends on the shape of the noise distribution.</p>
<p>We fix a target alternative with utility <span class="math inline">\(v_\text{target} = 1\)</span> and add <span class="math inline">\(K - 1\)</span> equal competitors, each with utility <span class="math inline">\(v_\text{comp} = 0\)</span>. As <span class="math inline">\(K\)</span> grows, we track the probability of choosing the target.</p>
<p>Under MNL, the choice probability is <span class="math inline">\(P(\text{target}) = e^a / (e^a + K - 1)\)</span>, where <span class="math inline">\(a = v_t \cdot \pi / (\beta \sqrt{6})\)</span> is the effective scaled utility.</p>
<p>Under MNP: <span class="math inline">\(P(\text{target}) = \int \phi(z) \,\Phi(v_t/\beta + z)^{K-1}\, dz\)</span> (by symmetry of the <span class="math inline">\(K - 1\)</span> equal competitors). This integral reveals that probit’s thinner tails give the target a larger advantage over many competitors than logit’s heavier tails.</p>
<div id="cell-fig-set-size" class="cell">
<div class="cell-output-display">
<div id="fig-set-size" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-set-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-set-size-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Probability of choosing a target alternative (v = 1) against K − 1 equal competitors (v = 0) as a function of set size K, for selected values of θ. MNL (dashed blue) and MNP (dashed red) references are shown. As θ increases, the race model transitions from logit-like to probit-like set-size scaling. The probit model assigns systematically higher probability to the target than logit when K is large, reflecting the thinner tails of Gaussian noise."><img src="index_files/figure-html/fig-set-size-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-set-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Probability of choosing a target alternative (v = 1) against K − 1 equal competitors (v = 0) as a function of set size K, for selected values of θ. MNL (dashed blue) and MNP (dashed red) references are shown. As θ increases, the race model transitions from logit-like to probit-like set-size scaling. The probit model assigns systematically higher probability to the target than logit when K is large, reflecting the thinner tails of Gaussian noise.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-4-independence-of-irrelevant-alternatives" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="study-4-independence-of-irrelevant-alternatives"><span class="header-section-number">10.4</span> Study 4: Independence of Irrelevant Alternatives</h2>
<p>The IIA property is a hallmark of the Multinomial Logit model: the ratio of choice probabilities for any two alternatives is invariant to the composition of the choice set. Formally, for alternatives <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[\frac{P(i \mid \mathcal{C})}{P(j \mid \mathcal{C})} = \frac{e^{v_i}}{e^{v_j}} \quad \text{for all choice sets } \mathcal{C} \ni i, j\]</span></p>
<p>This property does not hold for the Multinomial Probit model, even when errors are independent and identically distributed. The Poisson count race therefore provides a mechanism through which IIA holds exactly at <span class="math inline">\(\theta = 1\)</span> and is progressively violated as <span class="math inline">\(\theta\)</span> increases.</p>
<p>To quantify this, we consider three alternatives with utilities <span class="math inline">\(v = (2, 1, 0)\)</span>. We compute the ratio <span class="math inline">\(P(1)/P(2)\)</span> under two conditions:</p>
<ul>
<li><strong>Full set</strong>: all three alternatives present <span class="math inline">\(\{1, 2, 3\}\)</span></li>
<li><strong>Reduced set</strong>: only alternatives <span class="math inline">\(\{1, 2\}\)</span> present</li>
</ul>
<p>Under IIA, these ratios should be identical. We track the percentage change in the ratio as <span class="math inline">\(\theta\)</span> varies.</p>
<div id="cell-fig-iia" class="cell">
<div class="cell-output-display">
<div id="fig-iia" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-iia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-iia-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Test of Independence of Irrelevant Alternatives (IIA). Left: the ratio P(Alt 1)/P(Alt 2) computed in the full set {1, 2, 3} (solid) and the reduced set {1, 2} (dashed), as a function of θ. Under IIA (θ = 1), the two ratios are equal. As θ increases, they diverge. Right: percentage change in the ratio when alternative 3 is removed, quantifying the IIA violation. The violation increases with θ, confirming that IIA is a property of the Gumbel noise shape that erodes as the noise distribution shifts toward Gaussian."><img src="index_files/figure-html/fig-iia-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Test of Independence of Irrelevant Alternatives (IIA). Left: the ratio P(Alt 1)/P(Alt 2) computed in the full set {1, 2, 3} (solid) and the reduced set {1, 2} (dashed), as a function of θ. Under IIA (θ = 1), the two ratios are equal. As θ increases, they diverge. Right: percentage change in the ratio when alternative 3 is removed, quantifying the IIA violation. The violation increases with θ, confirming that IIA is a property of the Gumbel noise shape that erodes as the noise distribution shifts toward Gaussian.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-5-parameter-invariance-across-set-size" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="study-5-parameter-invariance-across-set-size"><span class="header-section-number">10.5</span> Study 5: Parameter Invariance Across Set Size</h2>
<p>A key empirical diagnostic for distinguishing between logit and probit is parameter invariance across changes in set size <span class="math inline">\(K\)</span> (Robinson et al., 2023). If choice data are generated by a logit model, the softmax inverse temperature <span class="math inline">\(\beta_{\text{logit}}\)</span> recovered from fitting a logit specification should remain constant as <span class="math inline">\(K\)</span> increases. Conversely, if the data follow a probit model, the Gaussian noise scale <span class="math inline">\(\beta_{\text{probit}}\)</span> should be invariant to <span class="math inline">\(K\)</span>.</p>
<p>We test this directly. For each value of <span class="math inline">\(\theta\)</span> and each set size <span class="math inline">\(K\)</span> (with a target at <span class="math inline">\(v = 1\)</span> vs.&nbsp;<span class="math inline">\(K-1\)</span> equal competitors at <span class="math inline">\(v = 0\)</span>), we compute the “true” choice probability <span class="math inline">\(P(\text{target})\)</span> from the race model and then recover the best-fitting logit and probit temperature parameters by inversion.</p>
<div id="cell-fig-parameter-recovery" class="cell">
<div class="cell-output-display">
<div id="fig-parameter-recovery" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parameter-recovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-parameter-recovery-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Recovered temperature parameters under logit (left) and probit (right) model assumptions, plotted as a function of set size K. Each line corresponds to a different accumulation threshold θ. At θ = 1 (exact logit), the recovered logit parameter is invariant to K, while the recovered probit parameter drifts. At large θ (approaching probit), the pattern reverses: the probit parameter is stable while the logit parameter varies. This directly parallels the empirical diagnostic of Robinson et al.&nbsp;(2023)."><img src="index_files/figure-html/fig-parameter-recovery-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parameter-recovery-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Recovered temperature parameters under logit (left) and probit (right) model assumptions, plotted as a function of set size K. Each line corresponds to a different accumulation threshold θ. At θ = 1 (exact logit), the recovered logit parameter is invariant to K, while the recovered probit parameter drifts. At large θ (approaching probit), the pattern reverses: the probit parameter is stable while the logit parameter varies. This directly parallels the empirical diagnostic of Robinson et al.&nbsp;(2023).
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-6-distributional-shape-noise-skewness-and-kurtosis" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="study-6-distributional-shape-noise-skewness-and-kurtosis"><span class="header-section-number">10.6</span> Study 6: Distributional Shape — Noise Skewness and Kurtosis</h2>
<p>The log-Gamma noise distribution transitions from highly skewed (Gumbel, <span class="math inline">\(\theta = 1\)</span>) to symmetric (Gaussian, <span class="math inline">\(\theta \to \infty\)</span>). This transition in distributional shape underlies all the choice-level phenomena documented above. To make this explicit, we plot the standardised noise density for several values of <span class="math inline">\(\theta\)</span> alongside the standard normal reference.</p>
<div id="cell-fig-noise-densities" class="cell">
<div class="cell-output-display">
<div id="fig-noise-densities" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noise-densities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-noise-densities-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Standardised noise densities Z^(θ) for selected values of θ. At θ = 1 (Gumbel), the distribution is right-skewed. As θ increases, the density converges to the standard normal (black dashed). The right tail — which governs the probability of ‘upset’ choices in multi-alternative settings — becomes thinner with increasing θ, explaining why probit concentrates more probability on the best alternative."><img src="index_files/figure-html/fig-noise-densities-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noise-densities-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Standardised noise densities Z^(θ) for selected values of θ. At θ = 1 (Gumbel), the distribution is right-skewed. As θ increases, the density converges to the standard normal (black dashed). The right tail — which governs the probability of ‘upset’ choices in multi-alternative settings — becomes thinner with increasing θ, explaining why probit concentrates more probability on the best alternative.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-skew-kurtosis" class="cell">
<div class="cell-output-display">
<div id="fig-skew-kurtosis" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-skew-kurtosis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-skew-kurtosis-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Skewness and excess kurtosis of the standardised log-Gamma noise as a function of θ. Both moments converge to zero (the Gaussian values) as θ increases, with skewness decaying as O(θ^{-1/2}) and excess kurtosis as O(θ^{-1}). These moment trajectories fully characterise the transition from Gumbel to Gaussian noise shape."><img src="index_files/figure-html/fig-skew-kurtosis-1.png" class="img-fluid figure-img" width="960"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-skew-kurtosis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Skewness and excess kurtosis of the standardised log-Gamma noise as a function of θ. Both moments converge to zero (the Gaussian values) as θ increases, with skewness decaying as O(θ^{-1/2}) and excess kurtosis as O(θ^{-1}). These moment trajectories fully characterise the transition from Gumbel to Gaussian noise shape.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="study-7-robustness-across-utility-structures" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="study-7-robustness-across-utility-structures"><span class="header-section-number">10.7</span> Study 7: Robustness Across Utility Structures</h2>
<p>The preceding studies used specific utility vectors. To assess robustness, we examine whether the convergence pattern holds across different utility configurations that are common in psychological experiments.</p>
<div id="cell-fig-robustness" class="cell">
<div class="cell-output-display">
<div id="fig-robustness" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robustness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-robustness-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Total variation distance from Probit as a function of θ for K = 5 alternatives under four qualitatively different utility structures. Uniform: all utilities equal (v = 0). Dominant: one strong alternative (v₁ = 3, others 0). Linear: linearly spaced utilities. Clustered: two groups of similar alternatives. Convergence to probit is robust across configurations, though the asymptotic TV distance (reflecting Monte Carlo noise) varies with K and utility spread."><img src="index_files/figure-html/fig-robustness-1.png" class="img-fluid figure-img" width="768"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robustness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Total variation distance from Probit as a function of θ for K = 5 alternatives under four qualitatively different utility structures. Uniform: all utilities equal (v = 0). Dominant: one strong alternative (v₁ = 3, others 0). Linear: linearly spaced utilities. Clustered: two groups of similar alternatives. Convergence to probit is robust across configurations, though the asymptotic TV distance (reflecting Monte Carlo noise) varies with K and utility spread.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="summary-1" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="summary-1"><span class="header-section-number">10.8</span> Summary</h2>
<p>These multinomial simulations confirm and extend the binary-case results:</p>
<ol type="1">
<li><p><strong>Convergence is gradual and universal</strong>: Across different values of <span class="math inline">\(K\)</span> and different utility structures, the Poisson count race converges to the Multinomial Probit reference within <span class="math inline">\(\theta \approx 50\)</span>–<span class="math inline">\(100\)</span> in total variation distance.</p></li>
<li><p><strong>Probability redistribution</strong>: As <span class="math inline">\(\theta\)</span> increases, the probit model concentrates more probability on the best alternative and less on inferior alternatives, reflecting the thinner tails of Gaussian noise relative to Gumbel.</p></li>
<li><p><strong>Set-size scaling</strong>: The logit and probit models predict systematically different scaling of target choice probability with the number of competitors. The Poisson count race interpolates between these two patterns, connecting to the empirical findings of Robinson et al.&nbsp;(2023).</p></li>
<li><p><strong>IIA erosion</strong>: The Independence of Irrelevant Alternatives property, which holds exactly at <span class="math inline">\(\theta = 1\)</span>, is progressively violated as <span class="math inline">\(\theta\)</span> increases. This provides a process-level account of why IIA holds for logit but not probit: it is a consequence of the Gumbel noise shape, and alternative noise shapes—induced by higher accumulation thresholds—do not preserve it.</p></li>
<li><p><strong>Parameter invariance</strong>: When choice data generated by the Poisson count race are fit under a logit assumption, the recovered inverse temperature drifts with set size <span class="math inline">\(K\)</span> for all <span class="math inline">\(\theta &gt; 1\)</span>. Conversely, when fit under a probit assumption, the recovered noise scale remains stable for large <span class="math inline">\(\theta\)</span> but drifts when <span class="math inline">\(\theta\)</span> is small. This cross-over in parameter invariance provides a process-level account of the empirical findings of Robinson et al.&nbsp;(2023): parameter stability across <span class="math inline">\(K\)</span> is diagnostic of whether the effective noise distribution is closer to Gumbel or Gaussian.</p></li>
<li><p><strong>Noise shape transition</strong>: The underlying mechanism is a smooth transition in the shape of the standardised noise distribution, from the skewed Gumbel (<span class="math inline">\(\theta = 1\)</span>) to the symmetric Gaussian (<span class="math inline">\(\theta \to \infty\)</span>). The skewness and kurtosis decay at known rates, providing analytic control over the approximation quality.</p></li>
</ol>
</section>
</section>
<section id="discussion" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Discussion</h1>
<p>The present work develops a generative framework in which two foundational discrete choice models—Multinomial Logit and Multinomial Probit—arise as endpoint regimes of a single stochastic accumulation process. By introducing a Poisson count race and a temperature identification that separates noise scale from noise shape, we provide a principled account of how extreme-value and Gaussian choice behavior can emerge from different stopping rules applied to the same underlying mechanism.</p>
<p>The goal of this paper is not to advocate replacing existing models, but to clarify their relationship. Logit and probit are often treated as competing assumptions about the distribution of unobserved utility noise. Our results suggest a different interpretation: these models correspond to distinct regimes of evidence accumulation, characterized by how much stochastic evidence is required before commitment.</p>
<p>While the binary case simulation provides a useful starting point for developing intuition and for establishing the endpoint results analytically, it is not where the distinction between logit and probit is most consequential. In binary choice, many distributional differences collapse to one-dimensional comparisons, and logit and probit are well known to yield closely aligned predictions under variance matching. For this reason, a central question is whether the proposed accumulation-based unification extends beyond binary choice and meaningfully differentiates behavior in multi-alternative settings, where independence assumptions and cross-option dependence play a decisive role. To address this question, we examined the behavior of the Poisson count race in the multinomial case through targeted simulation studies.</p>
<p>The multinomial setting further clarifies the contribution of the Poisson count race. While the logit–probit distinction is often discussed in binary choice, its practical significance lies in multi-alternative settings, where independence-of-irrelevant-alternatives and cross-option dependence become central. Our multinomial simulations show that thresholded accumulation induces systematic, graded violations of IIA as the accumulation threshold increases. Under temperature identification, these violations converge toward the dependence structure characteristic of multinomial probit. Importantly, this dependence is not imposed by construction but emerges endogenously from the accumulation and stopping rule. This provides a generative account of how probit-like behavior can arise from the same underlying mechanism that yields logit at a single-event threshold.</p>
<section id="relation-to-existing-choice-models" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="relation-to-existing-choice-models"><span class="header-section-number">11.1</span> Relation to existing choice models</h2>
<p>From a mathematical standpoint, all components of the present framework are classical. Exponential races yield Luce’s choice rule, Gamma waiting times arise from accumulated Poisson events, and asymptotic normality follows from the Central Limit Theorem. The contribution lies in assembling these elements into a single, coherent generative family and identifying the conditions under which its limiting behavior remains non-degenerate.</p>
<p>This synthesis clarifies why logit and probit models often behave similarly in practice, particularly under variance matching, while also explaining why they diverge systematically in multi-alternative settings. Rather than viewing these differences as artifacts of arbitrary distributional assumptions, the Poisson count race shows how they arise naturally from differences in stopping rules.</p>
<p>Importantly, the present model should not be conflated with full sequential sampling models such as the Diffusion Decision Model or Linear Ballistic Accumulator. Those models are designed to jointly account for response times and accuracy and typically involve continuous accumulation with explicit drift and boundary parameters. The Poisson count race, by contrast, is deliberately minimal: it uses accumulation as a generative device to induce a family of random utility models, without making claims about within-trial dynamics or response time distributions. In this sense, it occupies an intermediate position between static random utility formulations and full dynamical decision models.</p>
<p>A closely related work is the dependent Poisson race model of Ruan et al.&nbsp;(2008), who develop a thresholded accumulation process for binary conjoint choice. In their model, discrete Poisson “hits” accrue until a fixed integer threshold is reached, and dependence between alternatives is introduced via shared Poisson components tied to overlapping attributes. Their primary contribution is behavioral and statistical: modeling dominance and similarity effects in conjoint choice and improving predictive performance relative to independent race models and multinomial logit.</p>
<p>The present work overlaps with this literature in its use of Poisson accumulation and thresholded stopping rules. In particular, we do not claim novelty in the observation that the classical exponential race underlying Luce’s choice axiom corresponds to a single-event threshold, nor in the existence of thresholded Poisson race models more generally.</p>
<p>Our focus, however, is different. Whereas prior work emphasizes dependence structures and behavioral phenomena, we focus on the distributional consequences of thresholded accumulation and the resulting random utility representations. By restricting attention to independent races, we make explicit the induced log-Gamma utility noise and analyze how its shape varies systematically with the accumulation threshold.</p>
<p>Crucially, we introduce a variance-preserving (temperature) identification that separates noise scale from noise shape. Without this identification, increasing the threshold trivially drives the model toward deterministic choice. With it, the large-threshold regime remains non-degenerate and admits a well-defined asymptotic limit. Under this identification, multinomial logit arises exactly at threshold one, while multinomial probit emerges as an asymptotic regime as the threshold grows.</p>
<p>Finally, our results are stated for general <span class="math inline">\(K\)</span>-alternative choice. While thresholded Poisson races have been explored primarily in binary settings, the logit–probit distinction is most consequential in multinomial choice, where independence properties and correlation structures become central. By deriving both multinomial logit and multinomial probit within a single generative framework, we clarify their relationship in precisely the setting where their differences matter most.</p>
</section>
<section id="temperature-identification-and-model-comparison" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="temperature-identification-and-model-comparison"><span class="header-section-number">11.2</span> Temperature identification and model comparison</h2>
<p>A central technical step in the present work is the introduction of a temperature identification that standardizes the variance of the stochastic utility component across values of the accumulation threshold. Without this identification, increasing the threshold trivially drives the model toward deterministic choice, obscuring the effect of changing noise shape.</p>
<p>By fixing the scale of stochasticity and allowing only its shape to vary, the temperature-identified Poisson count race makes it possible to meaningfully compare models with different error distributions. This perspective reframes familiar variance-matching arguments in a generative context: rather than adjusting scale parameters post hoc to align predictions, scale normalization becomes an intrinsic part of the model definition.</p>
<p>Under this identification, the large-threshold limit yields a genuine Multinomial Probit model, not merely an approximation. This provides a process-level interpretation of probit that parallels the classical exponential race interpretation of logit and helps explain why Gaussian assumptions often exhibit greater parameter stability across changes in task structure.</p>
</section>
<section id="intermediate-regimes-and-log-gamma-utility-noise" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="intermediate-regimes-and-log-gamma-utility-noise"><span class="header-section-number">11.3</span> Intermediate regimes and log-Gamma utility noise</h2>
<p>Between the logit and probit endpoints lies a continuum of models characterized by log-Gamma utility noise. These intermediate regimes are not intended as new “default” choice models, but they may be useful in settings where the strict independence properties of logit are too restrictive, while the computational complexity of probit is undesirable.</p>
<p>From a theoretical perspective, the existence of these intermediate regimes underscores that logit and probit are not isolated modeling choices, but special cases of a broader family. From a practical perspective, they suggest that deviations from logit or probit behavior may sometimes reflect differences in accumulation thresholds rather than fundamentally different noise sources.</p>
</section>
<section id="implications-for-empirical-modeling" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="implications-for-empirical-modeling"><span class="header-section-number">11.4</span> Implications for empirical modeling</h2>
<p>Recent empirical work has emphasized differences in parameter invariance and generalization between logit-based and signal-detection-based models across task structures. The present framework complements this line of research by offering a theoretical account of why such differences may arise. In particular, models with larger effective accumulation thresholds naturally exhibit Gaussian-like behavior, which may confer greater stability across changes in the number of alternatives or decision format.</p>
<p>At the same time, the present results caution against interpreting superior empirical performance of one model class as evidence for a particular noise distribution in isolation. Differences between logit and probit may reflect differences in decision criteria or commitment thresholds rather than differences in representational noise per se.</p>
</section>
<section id="limitations-and-extensions" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="limitations-and-extensions"><span class="header-section-number">11.5</span> Limitations and extensions</h2>
<p>The Poisson count race is intentionally simple. It assumes independent accumulation processes and focuses exclusively on choice probabilities, abstracting away from response times and within-trial dynamics. Extensions that relax these assumptions—by allowing correlated accumulators, time-varying rates, or joint modeling of choice and response time—are natural directions for future work.</p>
<p>Another limitation is that the present analysis treats the accumulation threshold as fixed across trials and alternatives. Allowing threshold variability or adaptive stopping rules could further enrich the family of induced choice models and connect more directly to theories of decision caution and speed–accuracy trade-offs.</p>
<p>Finally, while the present paper focuses on the relationship between logit and probit, the framework naturally invites comparison with other random utility specifications. Exploring whether additional classical models arise as limiting regimes under alternative accumulation rules may provide further insight into the structure of discrete choice behavior.</p>
</section>
<section id="concluding-remarks" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="concluding-remarks"><span class="header-section-number">11.6</span> Concluding remarks</h2>
<p>By grounding discrete choice models in a common stochastic accumulation process, the Poisson count race reframes a long-standing modeling distinction in a new light. Logit and probit emerge not as competing assumptions about utility noise, but as endpoint regimes corresponding to different stopping rules applied to the same underlying mechanism.</p>
<p>This unification does not diminish the practical differences between these models, but it clarifies their conceptual relationship and provides a principled basis for comparison. More broadly, it illustrates how process-level reasoning can illuminate the structure of static choice models and suggests that some longstanding modeling dichotomies may reflect differences in perspective rather than fundamental incompatibilities.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>