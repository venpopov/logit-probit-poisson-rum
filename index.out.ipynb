{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Poisson Count Race as a Generative Bridge Between Logit and Probit\n",
    "\n",
    "Choice\n",
    "\n",
    "Vencislav Popov (Department of Psychology, University of Zurich)\n",
    "\n",
    "This study unifies the two canonical discrete choice specifications–Multinomial Logit and Multinomial Probit–within a single generative framework. I introduce a **Poisson Count Race**, wherein $K$ alternatives generate events according to independent Poisson processes. A choice is determined when an alternative reaches a cumulative count threshold $\\theta$. At the threshold $\\theta=1$, the model yields the Multinomial Logit (Luce choice rule). By normalizing the utility noise to maintain a constant variance–a process termed temperature identification–I show that as $\\theta \\to \\infty$, the model converges to the Multinomial Probit. This formulation provides a parametric bridge in which $\\theta$ governs the shape of the error distribution and a separate parameter, $\\beta$, modulates the temperature, while the systematic utilities $v_i$ are shared across all regimes. Crucially, this bridge arises from a single stochastic accumulation mechanism rather than from post hoc variance matching between distinct models.\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "Discrete choice models based on Random Utility (RU) theory ([McFadden 1974](#ref-mcfaddenConditionalLogitAnalysis1974)) form a central pillar of mathematical psychology, econometrics, and cognitive science. In these models, each alternative in a choice set elicits a *latent* scalar quantity - often interpreted as strength, utility, or evidence - and the observed choices arise from a comparison of these latent quantities under stochastic variability. The general setup is surprisingly simple. Consider a decision-maker facing a set of $K \\ge 2$ mutually exclusive alternatives. The utility associated with alternative $i$ can be decomposed into a systematic component, $v_i$, and a stochastic component, $\\epsilon_i$:\n",
    "\n",
    "$$\n",
    "U_{i} = v_{i} + \\epsilon_{i}, \\quad i=1, \\dots, K\n",
    "$$\n",
    "\n",
    "The decision-maker then selects the alternative with maximum utility:\n",
    "\n",
    "$$\n",
    "C = \\arg \\max_{i \\in K} U_{i}\n",
    "$$\n",
    "\n",
    "Within this general framework, different assumptions about the distribution of errors, $\\epsilon_i$, determine the structure of the choice model and its predictions. Two important classes dominate the field: logit-based models, derived from Luce’s choice axiom and extreme-value theory ([Luce 1959](#ref-luceIndividualChoiceBehavior1959); [Yellott 1977](#ref-yellottRelationshipLucesChoice1977); [McFadden 1974](#ref-mcfaddenConditionalLogitAnalysis1974)), and probit-based models, derived from Thurstone’s Theory of comparative judgements and gaussian Signal Detection Theory ([Robinson et al. 2023](#ref-robinsonHowPeopleBuild2023); [Thurstone 1927](#ref-thurstoneLawComparativeJudgment1927); [Hausman and Wise 1978](#ref-hausmanConditionalProbitModel1978a)). These two model families correspond to two types of error distributions:\n",
    "\n",
    "1.  **Gumbel Errors:** If the $\\epsilon_i$ terms are independent and identically distributed (i.i.d.) according to a Type I Extreme Value distribution, the choice probabilities follow the **Multinomial Logit (MNL)** or Softmax form.\n",
    "\n",
    "2.  **Gaussian Errors:** If the $\\epsilon_i$ terms follow a Multivariate Normal distribution (potentially allowing for correlated errors across alternatives) the choice probabilities are described by the **Multinomial Probit (MNP)** model.\n",
    "\n",
    "The random utility framework thus unifies axiomatic choice models ([Luce 1959](#ref-luceIndividualChoiceBehavior1959)) and measurement detection-based models ([Thurstone 1927](#ref-thurstoneLawComparativeJudgment1927)) under a single functional form. However, the unification currently stops there - the different error distributions reflect different assumptions about the generating process. As Robinson et al. ([2023](#ref-robinsonHowPeopleBuild2023)) recently put it, the two models “describe different ways of translating sensory evidence into decision variables.”.\n",
    "\n",
    "The central question addressed in this paper is this: Can the two canonical random-utility discrete-choice specifications — multinomial logit and multinomial probit — be derived from a shared generative mechanism? Can we find a deeper unifying principle? The surprising answer is yes - both models can be derived as limit cases of a single stochastic evidence accumulation mechanism. The key distinction between the models is not the form of the utility noise itself, but the stopping rule governing how stochastic evidence is accumulated before a choice is made.”\n",
    "\n",
    "## 1.1 Model overview\n",
    "\n",
    "I introduce a **Poisson count race**, wherein each alternative generates stochastic events via an independent Poisson process. A decision is made when one alternative reaches a cumulative count threshold $\\theta$. The threshold $\\theta$ thus controls how much stochastic evidence must be accumulated before commitment.\n",
    "\n",
    "To compare noise *shape* across $\\theta$ independently of noise *scale*, I introduce a **temperature identification** that standardizes the stochastic utility component to unit variance for all $\\theta$, with a separate parameter $\\beta$ controlling the overall magnitude of stochasticity. With this normalization, we can establish three main results:\n",
    "\n",
    "1.  At $\\theta = 1$, this reduces exactly to the Multinomial Logit\n",
    "2.  For any $\\theta \\ge 1$, the Poisson count race is isomorphic to a random utility model with log-Gamma noise, interpolating between Gumbel and Gaussian error distributions;\n",
    "3.  As $\\theta \\to \\infty$ under temperature identification, the model converges to the Multinomial Probit.\n",
    "\n",
    "From this perspective, the familiar distinction between logit and probit reflects differences in the stopping rule governing an evidence accumulation process, rather than fundamentally distinct assumptions about decision noise. Extreme-value behavior and Gaussian behavior arise naturally as endpoint regimes of the same stochastic system.\n",
    "\n",
    "The remainder of the paper formalizes this framework, presents simulations illustrating the interpolation between regimes, and discusses implications for discrete choice modeling.\n",
    "\n",
    "# 2. The Generative Model: A Poisson Count Race\n",
    "\n",
    "Let the accumulation of evidence or preference for each alternative $i$ be modeled by independent Poisson processes, denoted $N_i(t)$, with rate parameters $\\lambda_i > 0$. $N_i(t)$ represents the number of counts accumulated for alternative $i$ until time $t$.\n",
    "\n",
    "Then, define a **Count Race** characterized by an integer threshold $\\theta \\ge 1$. The process terminates when any single process accumulates $\\theta$ events.\n",
    "\n",
    "**Definition 1 (Stopping Time).** The stopping time for the system is defined as:\n",
    "\n",
    "$$\n",
    "\\tau_{\\theta} = \\inf \\{t \\ge 0 : \\max_{i} N_i(t) = \\theta \\}\n",
    "$$\n",
    "\n",
    "**Definition 2 (Choice).** The chosen alternative is the specific process that triggers the stopping time:\n",
    "\n",
    "$$\n",
    "C = \\arg \\max_{i} N_i(\\tau_{\\theta})\n",
    "$$\n",
    "\n",
    "## 2.1 Transformation to Waiting Times\n",
    "\n",
    "To map this stochastic process to a utility framework, consider $T_i^{(\\theta)}$, the waiting time until the $i$-th process records its $\\theta$-th event:\n",
    "\n",
    "$$\n",
    "T_i^{(\\theta)} := \\inf \\{t : N_i(t) = \\theta \\}\n",
    "$$\n",
    "\n",
    "For a Poisson process with rate $\\lambda_i$, the waiting time to the $\\theta$-th jump follows a Gamma (Erlang) distribution:\n",
    "\n",
    "$$\n",
    "T_i^{(\\theta)} \\sim \\text{Gamma}(\\text{shape}=\\theta, \\text{rate}=\\lambda_i)\n",
    "$$\n",
    "\n",
    "The condition that alternative $i$ “wins” the race (i.e., reaches $\\theta$ events first) is equivalent to observing the minimum waiting time:\n",
    "\n",
    "$$\n",
    "C = \\arg \\min_{i} T_i^{(\\theta)}\n",
    "$$\n",
    "\n",
    "## 2.2 The Random Utility Representation\n",
    "\n",
    "Because the Poisson processes $N_1(t), \\ldots, N_K(t)$ are mutually independent, the waiting times $T_1^{(\\theta)}, \\ldots, T_K^{(\\theta)}$ are also mutually independent. By the scaling property of the Gamma distribution, each waiting time can be written as\n",
    "\n",
    "$$\n",
    "T_i^{(\\theta)} \\stackrel{d}{=} \\frac{G_i}{\\lambda_i}\n",
    "$$\n",
    "\n",
    "where $G_1, \\ldots, G_K \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Gamma}(\\theta, 1)$ are mutually independent standard Gamma random variables. Consequently, the choice problem is formulated as:\n",
    "\n",
    "$$\n",
    "C = \\arg \\min_{i} \\left( \\frac{G_i}{\\lambda_i} \\right)\n",
    "$$\n",
    "\n",
    "Applying the natural logarithm, a monotonic transformation, reverses the optimization direction from minimization to maximization:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} C &= \\arg \\min_{i} (\\log G_i - \\log \\lambda_i) \\\\ &= \\arg \\max_{i} (\\log \\lambda_i - \\log G_i) \\end{aligned}\n",
    "$$\n",
    "\n",
    "This establishes an exact RUM structure:\n",
    "\n",
    "$$\n",
    "U_i^{(\\theta)} = v_i + \\epsilon_i^{(\\theta)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   **Systematic Utility:** $v_i = \\log \\lambda_i$  \n",
    "-   **Stochastic Error:** $\\epsilon_i^{(\\theta)} = -\\log G_i$, with $G_i \\sim \\text{Gamma}(\\theta, 1)$.\n",
    "\n",
    "Thus, the Poisson count race is isomorphic to a Random Utility Model characterized by Log-Gamma noise.\n",
    "\n",
    "> **Result 1 (Log-Gamma Random Utility Family)**\n",
    ">\n",
    "> For any integer threshold $\\theta \\ge 1$, the Poisson count race induces a random utility model $U_i^{(\\theta)} = \\log \\lambda_i - \\log G_i$ with i.i.d. log-Gamma($\\theta$) noise. This family interpolates between Gumbel noise ($\\theta = 1$) and, under temperature identification, Gaussian noise ($\\theta \\to \\infty$).\n",
    "\n",
    "# 3. The Logit Boundary ($\\theta = 1$)\n",
    "\n",
    "In the specific instance where the threshold is a single event ($\\theta = 1$), the waiting time distribution simplifies:\n",
    "\n",
    "$$\n",
    "G_i \\sim \\text{Gamma}(1, 1) \\equiv \\text{Exponential}(1)\n",
    "$$\n",
    "\n",
    "A fundamental property of the Gumbel distribution is its relationship to the Exponential distribution:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Exp}(1) \\implies -\\log(X) \\sim \\text{Gumbel}(\\text{Type I EV})\n",
    "$$\n",
    "\n",
    "Therefore, when $\\theta=1$, the noise terms $\\epsilon_i^{(1)}$ are i.i.d. Gumbel. This recovers the exact Multinomial Logit formula:\n",
    "\n",
    "$$\n",
    "\\Pr(C=i) = \\frac{\\exp(v_i)}{\\sum_{j=1}^K \\exp(v_j)} = \\frac{\\lambda_i}{\\sum_{j=1}^K \\lambda_j}\n",
    "$$\n",
    "\n",
    "> **Result 2 (Logit Boundary)**\n",
    ">\n",
    "> The Poisson count race with threshold $\\theta=1$ recovers the Multinomial Logit model exactly: the noise terms are i.i.d. standard Gumbel, and the choice probabilities follow Luce’s choice rule (Softmax). This is a classic, well-known derivation.\n",
    "\n",
    "# 4. Temperature Identification\n",
    "\n",
    "For thresholds $\\theta > 1$, the error distribution deviates from the Gumbel form. Furthermore, as $\\theta$ increases, the variance of the error term diminishes. Specifically, for $\\epsilon^{(\\theta)} = -\\log G$ where $G \\sim \\text{Gamma}(\\theta, 1)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\mathbb{E}[\\epsilon^{(\\theta)}] &= -\\psi(\\theta) \\\\ \\text{Var}(\\epsilon^{(\\theta)}) &= \\psi_1(\\theta) \\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\psi(\\cdot)$ denotes the digamma function and $\\psi_1(\\cdot)$ the trigamma function.\n",
    "\n",
    "As $\\theta \\to \\infty$, $\\psi_1(\\theta) \\approx 1/\\theta \\to 0$. In the absence of standardization, the model becomes deterministic as the stochastic noise vanishes. To facilitate comparison of error “shapes” across varying $\\theta$ values, a consistent scale must be enforced. This process is referred to as **Temperature Identification**. Unlike conventional scale normalization, this identification preserves non-degenerate choice behavior as $\\theta$ increases, allowing the large-$\\theta$ limit to be meaningfully interpreted.\n",
    "\n",
    "We can define the standardized noise term $Z_i^{(\\theta)}$ such that it possesses zero mean and unit variance for all $\\theta$:\n",
    "\n",
    "$$\n",
    "Z_i^{(\\theta)} := \\frac{\\epsilon_i^{(\\theta)} - \\mu_\\theta}{\\sigma_\\theta} = \\frac{-\\log G_i + \\psi(\\theta)}{\\sqrt{\\psi_1(\\theta)}}\n",
    "$$\n",
    "\n",
    "This leads to a **Temperature-Identified Family** of utility models:\n",
    "\n",
    "$$\n",
    "U_i^{(\\theta)} = v_i + \\beta Z_i^{(\\theta)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "-   $v_i$ represents the systematic utility.\n",
    "-   $\\theta$ governs the **shape** of the noise distribution (tail behavior).\n",
    "-   $\\beta$ represents the **temperature** (the scale of noise relative to utility).\n",
    "\n",
    "# 5. The Probit Limit ($\\theta \\to \\infty$)\n",
    "\n",
    "This section establishes the asymptotic behavior of the temperature-identified model as the count threshold increases.\n",
    "\n",
    "The key observation is that $G_i \\sim \\text{Gamma}(\\theta, 1)$ can be decomposed as a sum of $\\theta$ independent exponential random variables: $G_i = \\sum_{j=1}^{\\theta} E_{ij}$, where $E_{ij} \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Exp}(1)$. By the law of large numbers, $G_i / \\theta \\xrightarrow{p} 1$ as $\\theta \\to \\infty$. Applying the delta method with the function $g(x) = -\\log(x)$, we obtain\n",
    "\n",
    "$$\n",
    "\\sqrt{\\theta}\\,(-\\log G_i - (-\\log \\theta)) \\xrightarrow{d} \\mathcal{N}\\bigl(0,\\, [g'(1)]^2\\bigr) = \\mathcal{N}(0, 1).\n",
    "$$\n",
    "\n",
    "Since the standardization $Z_i^{(\\theta)} = (-\\log G_i + \\psi(\\theta)) / \\sqrt{\\psi_1(\\theta)}$ uses $\\psi(\\theta) \\sim \\log\\theta - 1/(2\\theta)$ and $\\psi_1(\\theta) \\sim 1/\\theta$, this is asymptotically equivalent. Therefore, as $\\theta \\to \\infty$:\n",
    "\n",
    "$$\n",
    "Z_i^{(\\theta)} \\xrightarrow{d} \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "Consequently, the random utilities converge in distribution:\n",
    "\n",
    "$$\n",
    "(v_i + \\beta Z_i^{(\\theta)})_{i=1}^K \\xrightarrow{d} (v_i + \\beta Z_i)_{i=1}^K\n",
    "$$\n",
    "\n",
    "where $Z_i \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0, 1)$.\n",
    "\n",
    "Given that the error terms converge to additive Gaussian noise, the choice probabilities converge to those of the Multinomial Probit model.\n",
    "\n",
    "> **Result 3 (Probit Limit)**\n",
    ">\n",
    "> As $\\theta \\to \\infty$, the temperature-identified Poisson count race converges to the Multinomial Probit model with noise scale $\\beta$. The standardized log-Gamma noise $Z_i^{(\\theta)} \\xrightarrow{d} \\mathcal{N}(0,1)$, and choice probabilities converge to their Gaussian counterparts.\n",
    "\n",
    "# 6. Simulation Studies: Binary Choice\n",
    "\n",
    "To illustrate how the Poisson count race family interpolates between Logit and Probit, I conducted a simulation study in the binary choice setting ($K=2$). This setting admits closed-form choice probabilities and allows direct visual comparison with both classical models.\n",
    "\n",
    "Let $\\lambda_1$ and $\\lambda_2$ denote the Poisson rates of the two alternatives, and define the log-rate ratio $x = \\log(\\lambda_1/\\lambda_2)$. The probability that alternative 1 wins the race can be expressed in closed form using the regularized incomplete Beta function:\n",
    "\n",
    "$$\n",
    "\\Pr(C = 1 \\mid x, \\theta) = I_{\\sigma(x)}(\\theta, \\theta)\n",
    "$$\n",
    "\n",
    "where $I_p(a, b)$ is the regularized incomplete Beta function and $\\sigma(x) = (1 + e^{-x})^{-1}$. To see this, note that alternative 1 wins the race if and only if $T_1^{(\\theta)} < T_2^{(\\theta)}$, where $T_i^{(\\theta)} \\sim \\text{Gamma}(\\theta, \\lambda_i)$. Equivalently, defining $W = T_1^{(\\theta)} / (T_1^{(\\theta)} + T_2^{(\\theta)})$, we have $W \\sim \\text{Beta}(\\theta, \\theta)$ when evaluated at $p = \\lambda_1 / (\\lambda_1 + \\lambda_2) = \\sigma(x)$, giving $\\Pr(T_1 < T_2) = I_{\\sigma(x)}(\\theta, \\theta)$. For $\\theta = 1$, this reduces exactly to $\\sigma(x)$, the logistic function.\n",
    "\n",
    "When choice probabilities are plotted directly as a function of $x$ for increasing $\\theta$, the choice function becomes increasingly steep and converges to a step function at $x = 0$, reflecting deterministic selection of the alternative with the larger rate. This confirms that without temperature identification, increasing the count threshold simply reduces stochasticity rather than inducing Gaussian behavior.\n",
    "\n",
    "To compare noise *shape* independently of noise *scale*, I adopted the temperature identification introduced in Section 5. For binary choice, the variance of the utility noise difference is $\\text{Var}(\\epsilon_1^{(\\theta)} - \\epsilon_2^{(\\theta)}) = 2\\psi_1(\\theta)$. I therefore define the standardized signal $s = x / \\sqrt{2\\psi_1(\\theta)}$. On this variance-matched axis, the Logit reference ($\\theta = 1$) uses $\\text{sd}_{\\text{diff}} = \\pi/\\sqrt{3}$ (the standard deviation of the difference of two independent Gumbel variates), and the Probit reference is simply $\\Phi(s)$.\n",
    "\n",
    "For the multinomial simulations, the same principle applies. The logit reference uses an effective inverse temperature of $\\pi / (\\beta\\sqrt{6})$, which ensures the Gumbel noise has standard deviation matching $\\beta$ under the unit-variance convention. The probit reference uses noise scale $\\beta$ directly. All simulations use $\\beta = 1$ unless otherwise noted. Under this normalization, the $\\theta = 1$ Poisson race coincides exactly with the variance-matched logit curve, while increasing $\\theta$ yields choice functions that converge uniformly to the probit curve.\n",
    "\n",
    "Because logit and probit are themselves numerically close under variance matching, the differences between models are small in absolute magnitude but systematic. To make these differences visible, <a href=\"#fig-residuals\" class=\"quarto-xref\">Figure 1</a> plots residuals relative to the variance-matched Logit model. At $\\theta = 1$, the residual is identically zero (exact Logit). As $\\theta$ increases, the residuals grow smoothly and converge toward the Probit$-$Logit difference curve, with the maximum absolute deviation from probit decaying rapidly in $\\theta$. This confirms that the temperature-identified Poisson count race defines a continuous, parameterized family of choice rules that interpolates smoothly between logit-like and probit-like behavior."
   ],
   "id": "48dc48ce-c8a6-40f2-bc28-7beef33e9ce7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "source(\"R/race_functions.R\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(patchwork)\n",
    "\n",
    "s <- seq(-4, 4, length.out = 1200)\n",
    "thetas <- c(1, 2, 3, 5, 10, 20, 50, 200)\n",
    "\n",
    "# Build data frame\n",
    "df <- data.frame(\n",
    "  s = s,\n",
    "  residual = probit_residual(s),\n",
    "  model = \"Probit\"\n",
    ")\n",
    "\n",
    "for (th in thetas) {\n",
    "  df <- rbind(df, data.frame(\n",
    "    s = s,\n",
    "    residual = race_residual(s, th),\n",
    "    model = paste0(\"θ = \", th)\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Order factor levels so Probit appears first, then thetas in order\n",
    "df$model <- factor(df$model,\n",
    "  levels = c(\"Probit\", paste0(\"θ = \", thetas))\n",
    ")\n",
    "\n",
    "ggplot(df, aes(x = s, y = residual, color = model, linewidth = model)) +\n",
    "  geom_line() +\n",
    "  geom_hline(yintercept = 0, linewidth = 0.5) +\n",
    "  scale_linewidth_manual(\n",
    "    values = c(\"Probit\" = 1.2, setNames(rep(0.6, length(thetas)),\n",
    "                                         paste0(\"θ = \", thetas)))\n",
    "  ) +\n",
    "  labs(\n",
    "    x = expression(\"Rescaled signal \" ~ s == log(lambda[1]/lambda[2]) / sd(epsilon[1] - epsilon[2])),\n",
    "    y = expression(\"Residual: \" ~ Pr(choice == 1) - Logit[ref](s)),\n",
    "    color = \"Model\",\n",
    "    linewidth = \"Model\"\n",
    "  ) +\n",
    "  theme_minimal(base_size = 13) +\n",
    "  theme(legend.position = \"right\")"
   ],
   "id": "cell-fig-residuals"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Simulation Studies: Multinomial K-alternative Choice\n",
    "\n",
    "The binary simulation demonstrates that the temperature-identified Poisson count race interpolates smoothly between Logit ($\\theta = 1$) and Probit ($\\theta \\to \\infty$) in the two-alternative case. Here I extend this analysis to the multinomial setting ($K > 2$), where the differences between logit and probit become richer and more consequential.\n",
    "\n",
    "In the binary case, logit and probit choice functions differ only in the shape of the psychometric curve—a subtle quantitative distinction. With three or more alternatives, additional qualitative differences emerge. Most prominently, the Multinomial Logit model satisfies the *Independence of Irrelevant Alternatives* (IIA) property: the ratio of choice probabilities for any two alternatives is independent of the remaining alternatives in the choice set ([**HausmanMcFadden1984?**](#ref-HausmanMcFadden1984)). The Multinomial Probit model, even with independent errors, does not share this property. The Poisson count race therefore provides a window into how IIA-like behavior gradually weakens as the noise distribution transitions from Gumbel to Gaussian.\n",
    "\n",
    "The multinomial simulations are organised around five questions:\n",
    "\n",
    "1.  **Convergence**: How quickly do Poisson count race choice probabilities converge to the Probit reference as $\\theta$ increases, and does the rate of convergence depend on $K$?\n",
    "2.  **Probability vectors**: How does the full distribution over alternatives change as $\\theta$ varies from 1 to large values?\n",
    "3.  **Set-size scaling**: How does the probability of choosing a target alternative scale with the number of competitors, and how does this scaling differ between logit, probit, and intermediate regimes?\n",
    "4.  **Independence of Irrelevant Alternatives**: How does the IIA property—exact under logit—erode as $\\theta$ increases toward the probit regime?\n",
    "5.  **Parameter invariance**: When misspecified logit or probit models are fit to Poisson count race data, which model yields parameters that are invariant to $K$?\n",
    "\n",
    "All simulations use Monte Carlo sampling with $10^6$ to $10^7$ replications per condition unless otherwise noted.\n",
    "\n",
    "## 7.1 Study 1: Convergence to Probit\n",
    "\n",
    "I first examine how the total variation (TV) distance between the Poisson count race choice probabilities and the Logit / Probit references changes as a function of $\\theta$, for different numbers of alternatives $K$.\n",
    "\n",
    "For each $K$, I use linearly spaced utilities $v_i = (K - i)/(K - 1)$ for $i = 1, \\ldots, K$, ensuring that the best and worst alternatives always have utilities 1 and 0 regardless of $K$. The temperature is fixed at $\\beta = 1$."
   ],
   "id": "b1aa1e57-5714-4f0d-8f0e-12a484006c78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(42)\n",
    "n_sim <- 1e7\n",
    "\n",
    "Ks <- c(3, 5, 8)\n",
    "thetas <- c(1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100, 200)\n",
    "\n",
    "results <- data.frame()\n",
    "\n",
    "for (K in Ks) {\n",
    "  v <- seq(1, 0, length.out = K)  # linearly spaced utilities\n",
    "  \n",
    "  # Reference models\n",
    "  p_logit <- mnl_probs(v, beta = 1)\n",
    "  p_probit <- mnp_probs(v, beta = 1, n_sim = 2e6)\n",
    "  \n",
    "  for (th in thetas) {\n",
    "    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n",
    "    \n",
    "    results <- rbind(results, data.frame(\n",
    "      K = K,\n",
    "      theta = th,\n",
    "      tv_logit = tv_dist(p_race, p_logit),\n",
    "      tv_probit = tv_dist(p_race, p_probit)\n",
    "    ))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Reshape for plotting\n",
    "df_tv <- rbind(\n",
    "  data.frame(K = results$K, theta = results$theta, \n",
    "             TV = results$tv_logit, Reference = \"vs. Logit\"),\n",
    "  data.frame(K = results$K, theta = results$theta, \n",
    "             TV = results$tv_probit, Reference = \"vs. Probit\")\n",
    ")\n",
    "df_tv$K_label <- paste0(\"K = \", df_tv$K)\n",
    "df_tv$K_label <- factor(df_tv$K_label, levels = paste0(\"K = \", Ks))\n",
    "\n",
    "ggplot(df_tv, aes(x = theta, y = TV, color = Reference)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 1.5) +\n",
    "  scale_x_log10() +\n",
    "  facet_wrap(~K_label) +\n",
    "  scale_color_manual(values = c(\"vs. Logit\" = \"steelblue\", \"vs. Probit\" = \"firebrick\")) +\n",
    "  labs(\n",
    "    x = expression(\"Threshold \" * theta),\n",
    "    y = \"Total variation distance\",\n",
    "    color = \"Reference\"\n",
    "  ) +\n",
    "  theme(legend.position = \"bottom\")"
   ],
   "id": "cell-fig-tv-distance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Study 2: Choice Probability Vectors\n",
    "\n",
    "To visualise how the full distribution over alternatives evolves with $\\theta$, I fix $K = 5$ with utilities $v = (2.0,\\; 1.5,\\; 1.0,\\; 0.5,\\; 0.0)$ and plot the choice probability for each alternative across a range of thresholds."
   ],
   "id": "9bbc3696-7340-4604-b78d-d8709a2fe30a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(123)\n",
    "K <- 5\n",
    "v <- seq(2, 0, length.out = K)\n",
    "thetas_fine <- c(1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100, 200)\n",
    "n_sim <- 2e6\n",
    "\n",
    "# References\n",
    "p_logit <- mnl_probs(v, beta = 1)\n",
    "p_probit <- mnp_probs(v, beta = 1, n_sim = 5e6)\n",
    "\n",
    "# Race probabilities across theta\n",
    "df_probs <- data.frame()\n",
    "for (th in thetas_fine) {\n",
    "  p <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n",
    "  for (i in 1:K) {\n",
    "    df_probs <- rbind(df_probs, data.frame(\n",
    "      theta = th,\n",
    "      alternative = paste0(\"Alt \", i, \" (v=\", v[i], \")\"),\n",
    "      prob = p[i]\n",
    "    ))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Reference data\n",
    "df_ref <- data.frame()\n",
    "for (i in 1:K) {\n",
    "  df_ref <- rbind(df_ref, data.frame(\n",
    "    alternative = paste0(\"Alt \", i, \" (v=\", v[i], \")\"),\n",
    "    logit = p_logit[i],\n",
    "    probit = p_probit[i]\n",
    "  ))\n",
    "}\n",
    "\n",
    "df_probs$alternative <- factor(df_probs$alternative, \n",
    "                                levels = paste0(\"Alt \", 1:K, \" (v=\", v, \")\"))\n",
    "df_ref$alternative <- factor(df_ref$alternative,\n",
    "                              levels = paste0(\"Alt \", 1:K, \" (v=\", v, \")\"))\n",
    "\n",
    "ggplot(df_probs, aes(x = theta, y = prob, color = alternative)) +\n",
    "  geom_line(linewidth = 0.9) +\n",
    "  geom_point(size = 1.2) +\n",
    "  geom_hline(data = df_ref, aes(yintercept = logit, color = alternative), \n",
    "             linetype = \"dashed\", linewidth = 0.5, alpha = 0.7) +\n",
    "  geom_hline(data = df_ref, aes(yintercept = probit, color = alternative), \n",
    "             linetype = \"dotted\", linewidth = 0.5, alpha = 0.7) +\n",
    "  scale_x_log10() +\n",
    "  labs(\n",
    "    x = expression(\"Threshold \" * theta),\n",
    "    y = \"Choice probability\",\n",
    "    color = \"Alternative\"\n",
    "  ) +\n",
    "  theme(legend.position = \"right\") +\n",
    "  annotate(\"text\", x = 250, y = max(p_logit) + 0.01, label = \"— Logit    ··· Probit\", \n",
    "           size = 3, hjust = 1)"
   ],
   "id": "cell-fig-prob-vectors"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Study 3: Set-Size Scaling\n",
    "\n",
    "A critical diagnostic for discriminating between logit and probit models is the effect of adding alternatives to the choice set ([**Robinson2023?**](#ref-Robinson2023)). Under MNL, the probability of choosing a target alternative with fixed utility is strictly determined by the ratio of its strength to the total strength. Under MNP, the scaling with set size differs because the probability of “winning” the maximum comparison depends on the shape of the noise distribution.\n",
    "\n",
    "I fix a target alternative with utility $v_\\text{target} = 1$ and add $K - 1$ equal competitors, each with utility $v_\\text{comp} = 0$. As $K$ grows, I track the probability of choosing the target.\n",
    "\n",
    "Under MNL, the choice probability is $P(\\text{target}) = e^a / (e^a + K - 1)$, where $a = v_t \\cdot \\pi / (\\beta \\sqrt{6})$ is the effective scaled utility.\n",
    "\n",
    "Under MNP: $P(\\text{target}) = \\int \\phi(z) \\,\\Phi(v_t/\\beta + z)^{K-1}\\, dz$ (by symmetry of the $K - 1$ equal competitors). This integral reveals that probit’s thinner tails give the target a larger advantage over many competitors than logit’s heavier tails."
   ],
   "id": "ea0304d8-d3d9-47da-bfbb-e1df39154f1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(456)\n",
    "n_sim <- 2e6\n",
    "\n",
    "Ks <- c(2, 3, 4, 5, 6, 8, 10, 15, 20)\n",
    "thetas_set <- c(2, 5, 10, 50, 200)\n",
    "\n",
    "v_target <- 1\n",
    "v_comp <- 0\n",
    "\n",
    "df_setsize <- data.frame()\n",
    "\n",
    "for (K in Ks) {\n",
    "  v <- c(v_target, rep(v_comp, K - 1))\n",
    "  \n",
    "  # References\n",
    "  p_logit_target <- mnl_probs(v, beta = 1)[1]\n",
    "  p_probit_target <- mnp_probs(v, beta = 1, n_sim = n_sim)[1]\n",
    "  \n",
    "  df_setsize <- rbind(df_setsize, data.frame(\n",
    "    K = K, theta = NA, prob = p_logit_target, model = \"Logit (MNL)\"\n",
    "  ))\n",
    "  df_setsize <- rbind(df_setsize, data.frame(\n",
    "    K = K, theta = NA, prob = p_probit_target, model = \"Probit (MNP)\"\n",
    "  ))\n",
    "  \n",
    "  for (th in thetas_set) {\n",
    "    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n",
    "    df_setsize <- rbind(df_setsize, data.frame(\n",
    "      K = K, theta = th, prob = p_race[1], model = paste0(\"θ = \", th)\n",
    "    ))\n",
    "  }\n",
    "}\n",
    "\n",
    "# Separate reference and race data\n",
    "df_ref_lines <- df_setsize[df_setsize$model %in% c(\"Logit (MNL)\", \"Probit (MNP)\"), ]\n",
    "df_race_lines <- df_setsize[!df_setsize$model %in% c(\"Logit (MNL)\", \"Probit (MNP)\"), ]\n",
    "\n",
    "df_race_lines$model <- factor(df_race_lines$model, \n",
    "                               levels = paste0(\"θ = \", thetas_set))\n",
    "\n",
    "ggplot() +\n",
    "  geom_line(data = df_ref_lines, aes(x = K, y = prob, color = model), \n",
    "            linetype = \"dashed\", linewidth = 1) +\n",
    "  geom_point(data = df_ref_lines, aes(x = K, y = prob, color = model), size = 2) +\n",
    "  geom_line(data = df_race_lines, aes(x = K, y = prob, group = model, color = model), \n",
    "            linewidth = 0.7) +\n",
    "  geom_point(data = df_race_lines, aes(x = K, y = prob, color = model), size = 1.3) +\n",
    "  scale_color_manual(\n",
    "    values = c(\n",
    "      \"Logit (MNL)\" = \"steelblue\",\n",
    "      \"Probit (MNP)\" = \"firebrick\",\n",
    "      setNames(scales::hue_pal()(length(thetas_set)), paste0(\"θ = \", thetas_set))\n",
    "    )\n",
    "  ) +\n",
    "  labs(\n",
    "    x = \"Number of alternatives K\",\n",
    "    y = expression(\"P(target chosen)\"),\n",
    "    color = \"Model\"\n",
    "  ) +\n",
    "  theme(legend.position = \"right\")"
   ],
   "id": "cell-fig-set-size"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Study 4: Independence of Irrelevant Alternatives\n",
    "\n",
    "The IIA property is a hallmark of the Multinomial Logit model: the ratio of choice probabilities for any two alternatives is invariant to the composition of the choice set. Formally, for alternatives $i$ and $j$:\n",
    "\n",
    "$$\\frac{P(i \\mid \\mathcal{C})}{P(j \\mid \\mathcal{C})} = \\frac{e^{v_i}}{e^{v_j}} \\quad \\text{for all choice sets } \\mathcal{C} \\ni i, j$$\n",
    "\n",
    "This property does not hold for the Multinomial Probit model, even when errors are independent and identically distributed. The Poisson count race therefore provides a mechanism through which IIA holds exactly at $\\theta = 1$ and is progressively violated as $\\theta$ increases.\n",
    "\n",
    "To quantify this, I consider three alternatives with utilities $v = (2, 1, 0)$. I compute the ratio $P(1)/P(2)$ under two conditions:\n",
    "\n",
    "-   **Full set**: all three alternatives present $\\{1, 2, 3\\}$\n",
    "-   **Reduced set**: only alternatives $\\{1, 2\\}$ present\n",
    "\n",
    "Under IIA, these ratios should be identical. I track the percentage change in the ratio as $\\theta$ varies."
   ],
   "id": "a3e76480-b921-42a8-b12a-243db493c78a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(789)\n",
    "n_sim <- 1e7\n",
    "\n",
    "v_full <- c(2, 1, 0)\n",
    "v_reduced <- c(2, 1)\n",
    "\n",
    "thetas_iia <- c(1, 2, 3, 5, 8, 10, 15, 20, 30, 50, 100, 200)\n",
    "\n",
    "df_iia <- data.frame()\n",
    "\n",
    "for (th in thetas_iia) {\n",
    "  # Full set {1, 2, 3}\n",
    "  p_full <- race_choice_probs(v_full, theta = th, beta = 1, n_sim = n_sim)\n",
    "  ratio_full <- p_full[1] / p_full[2]\n",
    "  \n",
    "  # Reduced set {1, 2}\n",
    "  p_reduced <- race_choice_probs(v_reduced, theta = th, beta = 1, n_sim = n_sim)\n",
    "  ratio_reduced <- p_reduced[1] / p_reduced[2]\n",
    "  \n",
    "  df_iia <- rbind(df_iia, data.frame(\n",
    "    theta = th,\n",
    "    ratio_full = ratio_full,\n",
    "    ratio_reduced = ratio_reduced,\n",
    "    pct_change = 100 * (ratio_reduced - ratio_full) / ratio_full\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Also compute reference values\n",
    "p_logit_full <- mnl_probs(v_full)[1] / mnl_probs(v_full)[2]\n",
    "p_logit_reduced <- mnl_probs(v_reduced)[1] / mnl_probs(v_reduced)[2]\n",
    "\n",
    "p_probit_full <- mnp_probs(v_full, n_sim = 5e6)\n",
    "r_probit_full <- p_probit_full[1] / p_probit_full[2]\n",
    "p_probit_reduced <- mnp_probs(v_reduced, n_sim = 5e6)\n",
    "r_probit_reduced <- p_probit_reduced[1] / p_probit_reduced[2]\n",
    "probit_pct <- 100 * (r_probit_reduced - r_probit_full) / r_probit_full\n",
    "\n",
    "# Left panel: ratios\n",
    "df_ratios <- rbind(\n",
    "  data.frame(theta = df_iia$theta, ratio = df_iia$ratio_full, \n",
    "             set = \"Full {1, 2, 3}\"),\n",
    "  data.frame(theta = df_iia$theta, ratio = df_iia$ratio_reduced, \n",
    "             set = \"Reduced {1, 2}\")\n",
    ")\n",
    "\n",
    "p1 <- ggplot(df_ratios, aes(x = theta, y = ratio, color = set, linetype = set)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 1.5) +\n",
    "  scale_x_log10() +\n",
    "  scale_linetype_manual(values = c(\"Full {1, 2, 3}\" = \"solid\", \"Reduced {1, 2}\" = \"dashed\")) +\n",
    "  scale_color_manual(values = c(\"Full {1, 2, 3}\" = \"steelblue\", \"Reduced {1, 2}\" = \"darkorange\")) +\n",
    "  labs(\n",
    "    x = expression(\"Threshold \" * theta),\n",
    "    y = \"P(Alt 1) / P(Alt 2)\",\n",
    "    color = \"Choice set\",\n",
    "    linetype = \"Choice set\"\n",
    "  ) +\n",
    "  theme(legend.position = \"bottom\")\n",
    "\n",
    "# Right panel: IIA violation\n",
    "p2 <- ggplot(df_iia, aes(x = theta, y = pct_change)) +\n",
    "  geom_line(linewidth = 0.8, color = \"grey30\") +\n",
    "  geom_point(size = 1.5) +\n",
    "  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"steelblue\", linewidth = 0.6) +\n",
    "  geom_hline(yintercept = probit_pct, linetype = \"dashed\", color = \"firebrick\", linewidth = 0.6) +\n",
    "  annotate(\"text\", x = 1.2, y = 0.5, label = \"Logit (IIA)\", color = \"steelblue\", \n",
    "           hjust = 0, size = 3.5) +\n",
    "  annotate(\"text\", x = 1.2, y = probit_pct - 0.5, label = \"Probit limit\", color = \"firebrick\", \n",
    "           hjust = 0, size = 3.5) +\n",
    "  scale_x_log10() +\n",
    "  labs(\n",
    "    x = expression(\"Threshold \" * theta),\n",
    "    y = \"% change in P(1)/P(2)\\nwhen Alt 3 removed\"\n",
    "  )\n",
    "\n",
    "p1 + p2 + plot_layout(widths = c(1, 1))"
   ],
   "id": "cell-fig-iia"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Study 5: Parameter Invariance Across Set Size\n",
    "\n",
    "A key empirical diagnostic for distinguishing between logit and probit is parameter invariance across changes in set size $K$ ([**Robinson2023?**](#ref-Robinson2023)). If choice data are generated by a logit model, the softmax inverse temperature $\\beta_{\\text{logit}}$ recovered from fitting a logit specification should remain constant as $K$ increases. Conversely, if the data follow a probit model, the Gaussian noise scale $\\beta_{\\text{probit}}$ should be invariant to $K$.\n",
    "\n",
    "I test this directly. For each value of $\\theta$ and each set size $K$ (with a target at $v = 1$ vs. $K-1$ equal competitors at $v = 0$), I compute the “true” choice probability $P(\\text{target})$ from the race model and then recover the best-fitting logit and probit temperature parameters by inversion."
   ],
   "id": "be39f450-cec4-40db-b10c-a535380054a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(303)\n",
    "n_sim <- 3e6\n",
    "\n",
    "Ks_param <- c(2, 3, 4, 5, 6, 8, 10, 15, 20)\n",
    "thetas_param <- c(1, 3, 10, 50, 200)\n",
    "v_target <- 1\n",
    "sigma_G <- pi / sqrt(6)\n",
    "\n",
    "df_param <- data.frame()\n",
    "\n",
    "for (th in thetas_param) {\n",
    "  for (K in Ks_param) {\n",
    "    v <- c(v_target, rep(0, K - 1))\n",
    "    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)[1]\n",
    "    \n",
    "    beta_logit <- recover_logit_beta(p_race, v_target, K)\n",
    "    beta_probit <- recover_probit_beta(p_race, v_target, K)\n",
    "    \n",
    "    df_param <- rbind(df_param, data.frame(\n",
    "      K = K, theta = th, \n",
    "      beta_logit = beta_logit, \n",
    "      beta_probit = beta_probit,\n",
    "      model_label = paste0(\"θ = \", th)\n",
    "    ))\n",
    "  }\n",
    "}\n",
    "\n",
    "df_param$model_label <- factor(df_param$model_label, \n",
    "                                levels = paste0(\"θ = \", thetas_param))\n",
    "\n",
    "# Reshape for faceting\n",
    "df_param_long <- rbind(\n",
    "  data.frame(K = df_param$K, theta_label = df_param$model_label,\n",
    "             beta = df_param$beta_logit, recovery = \"Logit recovery (β_L)\"),\n",
    "  data.frame(K = df_param$K, theta_label = df_param$model_label,\n",
    "             beta = df_param$beta_probit, recovery = \"Probit recovery (β_P)\")\n",
    ")\n",
    "\n",
    "ggplot(df_param_long, aes(x = K, y = beta, color = theta_label)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 1.5) +\n",
    "  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"grey50\", linewidth = 0.5) +\n",
    "  facet_wrap(~recovery, scales = \"free_y\") +\n",
    "  labs(\n",
    "    x = \"Number of alternatives K\",\n",
    "    y = expression(\"Recovered \" * beta),\n",
    "    color = \"Threshold\"\n",
    "  ) +\n",
    "  theme(legend.position = \"bottom\")"
   ],
   "id": "cell-fig-parameter-recovery"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Study 6: Distributional Shape — Noise Skewness and Kurtosis\n",
    "\n",
    "The log-Gamma noise distribution transitions from highly skewed (Gumbel, $\\theta = 1$) to symmetric (Gaussian, $\\theta \\to \\infty$). This transition in distributional shape underlies all the choice-level phenomena documented above. To make this explicit, I plot the standardised noise density for several values of $\\theta$ alongside the standard normal reference."
   ],
   "id": "9176ca01-0968-4058-acce-67e8ff1724a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(101)\n",
    "n_draw <- 1e6\n",
    "\n",
    "thetas_dens <- c(1, 2, 5, 10, 50)\n",
    "\n",
    "df_dens <- data.frame()\n",
    "\n",
    "for (th in thetas_dens) {\n",
    "  G <- rgamma(n_draw, shape = th, rate = 1)\n",
    "  eps <- -log(G)\n",
    "  Z <- (eps + digamma(th)) / sqrt(trigamma(th))  # note: +digamma because E[eps] = -digamma\n",
    "  # Use density estimation\n",
    "  d <- density(Z, from = -5, to = 5, n = 512)\n",
    "  df_dens <- rbind(df_dens, data.frame(\n",
    "    x = d$x, y = d$y, \n",
    "    model = paste0(\"θ = \", th)\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Normal reference\n",
    "x_norm <- seq(-5, 5, length.out = 512)\n",
    "df_dens <- rbind(df_dens, data.frame(\n",
    "  x = x_norm, y = dnorm(x_norm), model = \"N(0,1)\"\n",
    "))\n",
    "\n",
    "df_dens$model <- factor(df_dens$model, \n",
    "                         levels = c(paste0(\"θ = \", thetas_dens), \"N(0,1)\"))\n",
    "\n",
    "ggplot(df_dens, aes(x = x, y = y, color = model, linetype = model, linewidth = model)) +\n",
    "  geom_line() +\n",
    "  scale_linetype_manual(\n",
    "    values = c(setNames(rep(\"solid\", length(thetas_dens)), paste0(\"θ = \", thetas_dens)),\n",
    "               \"N(0,1)\" = \"dashed\")\n",
    "  ) +\n",
    "  scale_linewidth_manual(\n",
    "    values = c(setNames(rep(0.7, length(thetas_dens)), paste0(\"θ = \", thetas_dens)),\n",
    "               \"N(0,1)\" = 1.0)\n",
    "  ) +\n",
    "  labs(\n",
    "    x = expression(\"Standardised noise \" * Z^(theta)),\n",
    "    y = \"Density\",\n",
    "    color = \"Model\", linetype = \"Model\", linewidth = \"Model\"\n",
    "  ) +\n",
    "  coord_cartesian(xlim = c(-4.5, 4.5)) +\n",
    "  theme(legend.position = \"right\")"
   ],
   "id": "cell-fig-noise-densities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "# Exact skewness and kurtosis of -log(Gamma(theta,1)), standardised\n",
    "# Skewness = psi_2(theta) / psi_1(theta)^(3/2) where psi_2 is the tetragamma\n",
    "# Excess kurtosis = psi_3(theta) / psi_1(theta)^2 where psi_3 is the pentagamma\n",
    "# Note: psigamma(x, deriv=n) gives the n-th derivative of log Gamma\n",
    "# psi_1 = trigamma, psi_2 = psigamma(,2), psi_3 = psigamma(,3)\n",
    "\n",
    "thetas_moments <- seq(1, 200, by = 1)\n",
    "\n",
    "skewness <- -psigamma(thetas_moments, deriv = 2) / trigamma(thetas_moments)^(3/2)\n",
    "# Note: -log(G) has skewness = -psi_2 / psi_1^{3/2} (the negative sign because of -log)\n",
    "# Actually: let's be careful. For X = -log(G), E[(X-mu)^3] = -psi_2(theta)\n",
    "# and Var(X) = psi_1(theta), so skewness = -psi_2(theta) / psi_1(theta)^{3/2}\n",
    "\n",
    "# Excess kurtosis: E[(X-mu)^4]/Var^2 - 3 = psi_3(theta)/psi_1(theta)^2\n",
    "ex_kurtosis <- psigamma(thetas_moments, deriv = 3) / trigamma(thetas_moments)^2\n",
    "\n",
    "df_moments <- rbind(\n",
    "  data.frame(theta = thetas_moments, value = skewness, moment = \"Skewness\"),\n",
    "  data.frame(theta = thetas_moments, value = ex_kurtosis, moment = \"Excess kurtosis\")\n",
    ")\n",
    "\n",
    "ggplot(df_moments, aes(x = theta, y = value)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"firebrick\") +\n",
    "  facet_wrap(~moment, scales = \"free_y\") +\n",
    "  labs(\n",
    "    x = expression(\"Threshold \" * theta),\n",
    "    y = \"Value\"\n",
    "  )"
   ],
   "id": "cell-fig-skew-kurtosis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Study 7: Robustness Across Utility Structures\n",
    "\n",
    "The preceding studies used specific utility vectors. To assess robustness, I examine whether the convergence pattern holds across different utility configurations that are common in psychological experiments."
   ],
   "id": "c7b0a0ed-b5c4-4596-b0cf-f0560a52d73a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "set.seed(202)\n",
    "n_sim <- 2e6\n",
    "K <- 5\n",
    "\n",
    "configs <- list(\n",
    "  \"Uniform\" = rep(0, K),\n",
    "  \"Dominant\" = c(3, rep(0, K - 1)),\n",
    "  \"Linear\" = seq(2, 0, length.out = K),\n",
    "  \"Clustered\" = c(2.0, 1.9, 0.1, 0.0, 0.0)\n",
    ")\n",
    "\n",
    "thetas_robust <- c(1, 2, 3, 5, 10, 20, 50, 100, 200)\n",
    "\n",
    "df_robust <- data.frame()\n",
    "\n",
    "for (cfg_name in names(configs)) {\n",
    "  v <- configs[[cfg_name]]\n",
    "  p_probit <- mnp_probs(v, beta = 1, n_sim = 5e6)\n",
    "  \n",
    "  for (th in thetas_robust) {\n",
    "    p_race <- race_choice_probs(v, theta = th, beta = 1, n_sim = n_sim)\n",
    "    df_robust <- rbind(df_robust, data.frame(\n",
    "      theta = th,\n",
    "      tv_probit = tv_dist(p_race, p_probit),\n",
    "      config = cfg_name\n",
    "    ))\n",
    "  }\n",
    "}\n",
    "\n",
    "df_robust$config <- factor(df_robust$config, levels = names(configs))\n",
    "\n",
    "ggplot(df_robust, aes(x = theta, y = tv_probit, color = config)) +\n",
    "  geom_line(linewidth = 0.8) +\n",
    "  geom_point(size = 1.5) +\n",
    "  scale_x_log10() +\n",
    "  labs(\n",
    "    x = expression(\"Threshold \" * theta),\n",
    "    y = \"TV distance from Probit\",\n",
    "    color = \"Utility structure\"\n",
    "  ) +\n",
    "  theme(legend.position = \"right\")"
   ],
   "id": "cell-fig-robustness"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Summary\n",
    "\n",
    "These multinomial simulations confirm and extend the binary-case results:\n",
    "\n",
    "1.  **Convergence is gradual and universal**: Across different values of $K$ and different utility structures, the Poisson count race converges to the Multinomial Probit reference within $\\theta \\approx 50$–$100$ in total variation distance.\n",
    "\n",
    "2.  **Probability redistribution**: As $\\theta$ increases, the probit model concentrates more probability on the best alternative and less on inferior alternatives, reflecting the thinner tails of Gaussian noise relative to Gumbel.\n",
    "\n",
    "3.  **Set-size scaling**: The logit and probit models predict systematically different scaling of target choice probability with the number of competitors. The Poisson count race interpolates between these two patterns, connecting to the empirical findings of ([**Robinson2023?**](#ref-Robinson2023)).\n",
    "\n",
    "4.  **IIA erosion**: The Independence of Irrelevant Alternatives property, which holds exactly at $\\theta = 1$, is progressively violated as $\\theta$ increases. This provides a process-level account of why IIA holds for logit but not probit: it is a consequence of the Gumbel noise shape, and alternative noise shapes—induced by higher accumulation thresholds—do not preserve it.\n",
    "\n",
    "5.  **Parameter invariance**: When choice data generated by the Poisson count race are fit under a logit assumption, the recovered inverse temperature drifts with set size $K$ for all $\\theta > 1$. Conversely, when fit under a probit assumption, the recovered noise scale remains stable for large $\\theta$ but drifts when $\\theta$ is small. This cross-over in parameter invariance provides a process-level account of the empirical findings of ([**Robinson2023?**](#ref-Robinson2023)): parameter stability across $K$ is diagnostic of whether the effective noise distribution is closer to Gumbel or Gaussian.\n",
    "\n",
    "6.  **Noise shape transition**: The underlying mechanism is a smooth transition in the shape of the standardised noise distribution, from the skewed Gumbel ($\\theta = 1$) to the symmetric Gaussian ($\\theta \\to \\infty$). The skewness and kurtosis decay at known rates, providing analytic control over the approximation quality.\n",
    "\n",
    "# 8. Discussion\n",
    "\n",
    "The present work develops a generative framework in which Multinomial Logit and Multinomial Probit arise as endpoint regimes of a single stochastic accumulation process. By introducing a Poisson count race and a temperature identification that separates noise scale from noise shape, this paper clarifies how extreme-value and Gaussian choice behavior emerge from different stopping rules applied to the same underlying mechanism. The goal is not to advocate replacing existing models, but to clarify their relationship: logit and probit correspond to distinct regimes of evidence accumulation, characterized by how much stochastic evidence is required before commitment.\n",
    "\n",
    "The multinomial simulations demonstrate that this unification is not merely a theoretical curiosity. Thresholded accumulation induces systematic, graded violations of IIA that converge toward the dependence structure characteristic of multinomial probit. This dependence emerges endogenously from the accumulation and stopping rule, rather than being imposed by construction. The parameter invariance results further connect the framework to recent empirical findings ([**Robinson2023?**](#ref-Robinson2023)), providing a process-level account of why Gaussian-based parameters exhibit greater stability across changes in set size.\n",
    "\n",
    "## 8.1 Relation to existing models\n",
    "\n",
    "From a mathematical standpoint, all components of the present framework are classical: exponential races yield Luce’s choice rule, Gamma waiting times arise from accumulated Poisson events, and asymptotic normality follows from the Central Limit Theorem. The contribution lies in assembling these elements into a single generative family and identifying the conditions under which its limiting behavior remains non-degenerate.\n",
    "\n",
    "The present model should not be conflated with full sequential sampling models such as the Diffusion Decision Model ([**Ratcliff1978?**](#ref-Ratcliff1978)) or Linear Ballistic Accumulator ([**BrownHeathcote2008?**](#ref-BrownHeathcote2008)). Those models jointly account for response times and accuracy via continuous accumulation with explicit drift and boundary parameters (see also [**TownsendAshby1983?**](#ref-TownsendAshby1983); [**SmithVickers1988?**](#ref-SmithVickers1988)). The Poisson count race is deliberately minimal: it uses accumulation as a generative device to induce a family of random utility models, without making claims about within-trial dynamics or response time distributions.\n",
    "\n",
    "Between the logit and probit endpoints lies a continuum of log-Gamma random utility models. These intermediate regimes are not intended as new default specifications, but they underscore that logit and probit are special cases of a broader family. Deviations from logit or probit behavior may sometimes reflect differences in accumulation thresholds rather than fundamentally different noise sources.\n",
    "\n",
    "## 8.2 Implications for empirical modeling\n",
    "\n",
    "The present framework offers a theoretical account of why logit-based and probit-based models may differ in parameter invariance across task structures. Models with larger effective accumulation thresholds naturally exhibit Gaussian-like behavior, which may confer greater stability across changes in the number of alternatives. At the same time, the results caution against interpreting superior empirical performance of one model class as evidence for a particular noise distribution in isolation: differences between logit and probit may reflect differences in decision criteria or commitment thresholds rather than differences in representational noise per se.\n",
    "\n",
    "## 8.3 Limitations and extensions\n",
    "\n",
    "The Poisson count race is intentionally simple. It assumes independent accumulation processes and focuses exclusively on choice probabilities, abstracting away from response times and within-trial dynamics. Extensions that allow correlated accumulators, time-varying rates, or joint modeling of choice and response time are natural directions for future work.\n",
    "\n",
    "The present analysis treats the accumulation threshold as fixed across trials and alternatives. Allowing threshold variability or adaptive stopping rules could further enrich the family of induced choice models and connect more directly to theories of decision caution and speed–accuracy trade-offs.\n",
    "\n",
    "The model positions the Poisson count race as a parametric family indexed by $(\\theta, \\beta)$, but a formal identification analysis is beyond the present scope. In principle, $\\theta$ and $\\beta$ play distinct roles—shape versus scale of the noise distribution—and the shape of the psychometric function or the pattern of IIA violations could serve to identify $\\theta$ from choice data. Whether these parameters are jointly identifiable from aggregate choice frequencies alone, and under what experimental designs, remains an open question for future investigation.\n",
    "\n",
    "Finally, the framework naturally invites comparison with other random utility specifications. Exploring whether additional classical models arise as limiting regimes under alternative accumulation rules may provide further insight into the structure of discrete choice behavior.\n",
    "\n",
    "## 8.4 Concluding remarks\n",
    "\n",
    "By grounding discrete choice models in a common stochastic accumulation process, the Poisson count race reframes a long-standing modeling distinction. Logit and probit emerge not as competing assumptions about utility noise, but as endpoint regimes corresponding to different stopping rules applied to the same underlying mechanism. This unification clarifies their conceptual relationship and provides a principled basis for comparison, illustrating how process-level reasoning can illuminate the structure of static choice models.\n",
    "\n",
    "Hausman, Jerry A., and David A. Wise. 1978. “A Conditional Probit Model for Qualitative Choice: Discrete Decisions Recognizing Interdependence and Heterogeneous Preferences.” *Econometrica* 46 (2): 403–26. <https://doi.org/10.2307/1913909>.\n",
    "\n",
    "Luce, R. Duncan. 1959. *Individual Choice Behavior*. Individual Choice Behavior. Oxford, England: John Wiley.\n",
    "\n",
    "McFadden, Daniel. 1974. “Conditional Logit Analysis of Qualitative Choice Behavior.” In *Frontiers in Econometrics*, 105. Frontiers in Econometrics. - New York \\[u.a.\\] : Academic Press, ISBN 0-12-776150-0. - 1974, p. 105-142.\n",
    "\n",
    "Robinson, Maria M., Isabella C. DeStefano, Edward Vul, and Timothy F. Brady. 2023. “How Do People Build up Visual Memory Representations from Sensory Evidence? Revisiting Two Classic Models of Choice.” *Journal of Mathematical Psychology* 117 (December): 102805. <https://doi.org/10.1016/j.jmp.2023.102805>.\n",
    "\n",
    "Thurstone, L. L. 1927. “A Law of Comparative Judgment.” *Psychological Review* 34 (4): 273–86. <https://doi.org/10.1037/h0070288>.\n",
    "\n",
    "Yellott, John. 1977. “The Relationship Between Luce’s Choice Axiom, Thurstone’s Theory of Comparative Judgment, and the Double Exponential Distribution.” *Journal of Mathematical Psychology* 15 (2): 109–44. <https://doi.org/10.1016/0022-2496(77)90026-8>."
   ],
   "id": "c608c092-b3cf-4808-9c84-4635f9d0cdd8"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
